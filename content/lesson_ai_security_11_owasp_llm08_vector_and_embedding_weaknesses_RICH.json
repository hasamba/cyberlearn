{
  "lesson_id": "4860408e-4e36-481e-ba13-09d9661f176d",
  "domain": "ai_security",
  "title": "OWASP LLM08: Vector and Embedding Weaknesses",
  "subtitle": "Securing retrieval-augmented generation pipelines",
  "difficulty": 3,
  "estimated_time": 60,
  "order_index": 11,
  "prerequisites": [],
  "concepts": [
    "rag poisoning",
    "vector database security",
    "embedding manipulation",
    "semantic attacks",
    "tenant isolation",
    "monitoring"
  ],
  "learning_objectives": [
    "Explain how adversaries manipulate embeddings and vector stores to influence retrieval results.",
    "Design access controls and isolation strategies that protect multi-tenant vector databases.",
    "Implement validation, scoring, and filtering mechanisms for retrieved context.",
    "Detect and respond to anomalous retrieval patterns that indicate poisoning or exfiltration."
  ],
  "post_assessment": [
    {
      "question": "What is a key risk when untrusted content is embedded and stored in a vector database?",
      "options": [
        "Vectors automatically encrypt themselves.",
        "An attacker can insert adversarial embeddings that outrank legitimate documents and steer model responses.",
        "Embedding vectors cannot be modified after creation.",
        "Vector databases do not support metadata."
      ],
      "correct_answer": 1,
      "difficulty": 3,
      "type": "multiple_choice",
      "question_id": "98894730-e0f9-4f18-a479-56289b5e836e",
      "explanation": "Correct answer explained in lesson content."
    },
    {
      "question": "How can teams prevent cross-tenant data exposure in shared vector stores?",
      "options": [
        "Allow every tenant to query the global namespace.",
        "Enforce namespace isolation, authentication, and metadata-based filtering before retrieval.",
        "Disable TLS between services.",
        "Store all embeddings in a public S3 bucket for convenience."
      ],
      "correct_answer": 1,
      "difficulty": 3,
      "type": "multiple_choice",
      "question_id": "5297c202-529c-4f55-b726-842d03c828ac",
      "explanation": "Correct answer explained in lesson content."
    },
    {
      "question": "Which signal suggests retrieval poisoning may be underway?",
      "options": [
        "Stable query latency.",
        "Sudden spikes in nearly identical embeddings being inserted from new accounts.",
        "Scheduled backups completing successfully.",
        "A routine schema migration."
      ],
      "correct_answer": 1,
      "difficulty": 3,
      "type": "multiple_choice",
      "question_id": "1386d6a3-210b-414f-891e-47abcdee6c37",
      "explanation": "Correct answer explained in lesson content."
    }
  ],
  "jim_kwik_principles": [
    "active_learning",
    "minimum_effective_dose",
    "teach_like_im_10",
    "memory_hooks",
    "meta_learning",
    "connect_to_what_i_know",
    "reframe_limiting_beliefs",
    "gamify_it",
    "learning_sprint",
    "multiple_memory_pathways"
  ],
  "content_blocks": [
    {
      "type": "explanation",
      "content": {
        "text": "\nVector databases power retrieval-augmented generation (RAG) by storing embeddings of documents, FAQs, and knowledge bases. When a user submits a query, the system retrieves semantically similar vectors to provide context to the LLM. Attackers exploit this pipeline by inserting malicious or poisoned embeddings. Because similarity search is based on mathematical distance rather than explicit keywords, an adversarial vector crafted to sit near many queries can dominate results even if the underlying text is malicious. Poisoned entries might include false instructions, hidden prompt injections, or confidential data intended for exfiltration. Multi-tenant deployments add further risk: without strict partitioning, one customer's embeddings could leak into another's results.\n\nManipulation can also occur during embedding generation. If the embedding model is compromised or receives crafted input, it might produce vectors that misrepresent the content, causing the retrieval layer to surface irrelevant or harmful documents. Attackers may exploit metadata filters, injecting tags that bypass security policies or escalate trust levels. Because RAG pipelines often fetch context automatically, poisoned vectors can silently influence LLM outputs without obvious prompts, making detection challenging.\n"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "\nDefending RAG systems requires control at every layer. First, authenticate all write operations to the vector store and assign per-tenant namespaces. Enforce metadata constraints that restrict which documents can satisfy a query based on user role, classification, or document state. Validate content before embedding—scan for prompt injections, secrets, or policy violations. Embed only sanitized text, and store hashes of source documents so tampering can be detected. Consider using separate embedding models for trusted vs. untrusted sources to prevent contamination.\n\nAt retrieval time, apply scoring ensembles that combine vector similarity with metadata weights, lexical matching, or trust scores. Cap the number of results pulled from each data source and run retrieved snippets through sanitizers before they reach the LLM. Monitoring is essential: track insert rates, vector similarity distributions, and query patterns. Alert when new embeddings closely match sensitive documents or when a tenant suddenly inserts thousands of vectors. Incident response should include quarantining suspect namespaces, re-embedding clean data, and notifying affected users. By layering controls, teams can enjoy the power of RAG while minimizing the attack surface.\n"
      }
    },
    {
      "type": "code_exercise",
      "content": {
        "text": "\n### Lab: Enforcing Namespace Isolation in a Vector Store\n\n1. **Wrap vector operations** with authentication checks.\n\n```python\nclass VectorStoreClient:\ndef __init__(self, backend):\nself.backend = backend\n\ndef upsert(self, tenant: str, vectors: list[dict]):\nif not self._is_authorized(tenant, \"write\"):\nraise PermissionError(\"Unauthorized tenant\")\nnamespaced = [dict(v, namespace=tenant) for v in vectors]\nself.backend.upsert(vectors=namespaced)\n\ndef query(self, tenant: str, embedding: list[float], filters: dict):\nif not self._is_authorized(tenant, \"read\"):\nraise PermissionError(\"Unauthorized tenant\")\nfilters = dict(filters or {})\nfilters[\"namespace\"] = tenant\nreturn self.backend.query(vector=embedding, filters=filters)\n\ndef _is_authorized(self, tenant: str, action: str) -> bool:\n# Replace with real policy engine\nreturn tenant in {\"tenant-a\", \"tenant-b\"}\n```\n\n2. **Attach metadata filters** for document classification (e.g., public, confidential) and enforce them per user role.\n3. **Log insertion and query metrics** to detect anomalies such as unusual vector similarity spikes.\n4. **Run cleanup scripts** that remove orphaned or stale embeddings regularly.\n"
      }
    },
    {
      "type": "real_world",
      "content": {
        "text": "\nSeveral open-source RAG demos have been compromised by malicious pull requests that inserted poisoned knowledge base entries. When unsuspecting users deployed the demos, the default vector store returned the attacker's content, causing the chatbot to leak secrets or direct users to phishing sites. Enterprises have reported similar incidents when employees bulk-uploaded documents without scanning them, accidentally embedding sensitive data that later surfaced in other users' responses.\n"
      }
    },
    {
      "type": "memory_aid",
      "content": {
        "text": "\nUse **VECTOR SAFE**:\n\n- **V**alidate content before embedding.\n- **E**nforce namespaces and tenant isolation.\n- **C**ombine similarity with trust scoring.\n- **T**rack insertion and query anomalies.\n- **O**bserve metadata for suspicious tags.\n- **R**espond quickly by quarantining contaminated data.\n- **S**anitize retrieved snippets before prompting.\n- **A**udit embedding models and pipeline code.\n- **F**reshen indexes with re-embedding when sources change.\n- **E**ducate teams on safe document ingestion.\n"
      }
    },
    {
      "type": "quiz",
      "content": {
        "text": "\n1. Why should retrieval pipelines limit the number of documents per source?  \n**Answer:** It prevents a single poisoned source from dominating the context and encourages diverse, trustworthy evidence.\n\n2. How can metadata enhance vector security?  \n**Answer:** Metadata enables role-based filtering, classification enforcement, and detection of unusual tags that may signal manipulation.\n\n3. What telemetry helps spot embedding abuse?  \n**Answer:** Monitoring insert volume, cosine similarity distributions, new tenant activity, and retrieval success rates reveals anomalies associated with poisoning.\n"
      }
    },
    {
      "type": "reflection",
      "content": {
        "text": "\n- Which teams are allowed to upload content to your knowledge bases, and how do you vet their submissions?  \n- Do you maintain an audit trail linking each embedding to its source document and uploader?  \n- How quickly can you rebuild a clean index if contamination is discovered?\n"
      }
    },
    {
      "type": "mindset_coach",
      "content": {
        "text": "\nRAG pipelines are powerful but delicate. Treat embeddings and vector stores as critical infrastructure—regular maintenance, access reviews, and proactive monitoring keep the knowledge graph trustworthy.\n"
      }
    }
  ],
  "tags": [
    "Course: OWASP LLM Top 10"
  ]
}