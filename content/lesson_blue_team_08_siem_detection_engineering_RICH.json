{
  "lesson_id": "4a3b2c1d-0e9f-8a7b-6c5d-4e3f2a1b0c9d",
  "domain": "blue_team",
  "title": "SIEM Detection Engineering and Rule Development",
  "difficulty": 3,
  "order_index": 8,
  "prerequisites": [
    "bt000000-0000-0000-0000-000000000001"
  ],
  "concepts": [
    "SIEM Architecture and Data Flow",
    "Log Collection and Normalization",
    "Detection Rule Engineering",
    "Splunk SPL (Search Processing Language)",
    "Microsoft Sentinel KQL (Kusto Query Language)",
    "Sigma Rules (Universal Detection Format)",
    "False Positive Reduction",
    "Alert Fatigue Management",
    "Threat Hunting with SIEM",
    "MITRE ATT&CK Mapping",
    "Detection-as-Code"
  ],
  "estimated_time": 60,
  "learning_objectives": [
    "Understand SIEM architecture and how log data flows from sources to analytics",
    "Write effective detection rules in Splunk SPL and Microsoft Sentinel KQL",
    "Create Sigma rules for platform-agnostic detection logic",
    "Tune detection rules to reduce false positives while maintaining coverage",
    "Map detections to MITRE ATT&CK framework for gap analysis",
    "Implement detection-as-code workflows for version control and testing",
    "Design detection strategies based on threat intelligence and incident patterns",
    "Measure and improve detection effectiveness with metrics"
  ],
  "content_blocks": [
    {
      "type": "explanation",
      "title": "Introduction: SIEM and the Detection Engineering Discipline",
      "content": {
        "text": "**Detection Engineering** is the discipline of creating, testing, and maintaining detection logic that identifies security threats in log data. **SIEM** (Security Information and Event Management) is the platform where this happens.\n\n## The Evolution of SIEM\n\n**Generation 1: Log Management (2000s)**\n```plaintext\nPurpose: Compliance (HIPAA, PCI-DSS)\nFunctionality: Collect logs, store them, search when needed\nDetection: Manual searches by analysts\nProblem: Reactive, no real-time detection\n```\n\n**Generation 2: Correlation Engines (2010s)**\n```plaintext\nPurpose: Real-time threat detection\nFunctionality: Correlation rules (if X then Y)\nDetection: Automated alerts based on predefined rules\nProblem: Rules brittle, high false positive rates\n```\n\n**Generation 3: Modern SIEM (2020s)**\n```plaintext\nPurpose: Threat detection + investigation + response\nFunctionality: ML/AI, behavioral analytics, UEBA, SOAR integration\nDetection: Sophisticated rules + machine learning\nProblem: Requires skilled detection engineers\n```\n\n## What is Detection Engineering?\n\n**Definition**: The practice of designing, implementing, and maintaining detection capabilities to identify adversary behaviors.\n\n**NOT just writing rules**:\n\n```plaintext\nTraditional Approach:\n1. Threat intel says \"bad.com is malicious\"\n2. Write rule: if (domain = \"bad.com\") then alert\n3. Deploy rule\n4. Hope it catches something\n\nDetection Engineering Approach:\n1. Understand attacker TTP (Tactics, Techniques, Procedures)\n2. Map to MITRE ATT&CK (T1071.001 - Web Service - Application Layer Protocol)\n3. Identify data sources (proxy logs, DNS logs, firewall logs)\n4. Design detection logic (behavioral pattern, not just IOC)\n5. Write rule with multiple conditions\n6. Test against benign and malicious samples\n7. Tune to reduce false positives\n8. Document detection rationale\n9. Version control (Git)\n10. Measure effectiveness\n11. Iterate based on results\n```\n\n## Why Detection Engineering Matters\n\n**The Detection Gap**:\n\n```plaintext\nSecurity Controls Stack:\n\n[Preventive Controls]\n- Firewall (blocks known-bad IPs)\n- Antivirus (blocks known malware)\n- Email gateway (blocks phishing)\n\nProblem: Preventive controls fail ~30-40% of the time\n\n[Detection Controls] ← YOU ARE HERE\n- SIEM detections\n- EDR behavioral rules\n- Network traffic analysis\n\nGoal: Catch what preventive controls missed\n\n[Response Controls]\n- Incident response\n- Containment\n- Eradication\n```\n\n**Real-World Impact**:\n\n```plaintext\nWithout Detection Engineering:\n- Generic SIEM rules (vendor defaults)\n- High false positive rate (98%)\n- Alert fatigue\n- Real attacks buried in noise\n- Mean Time to Detect (MTTD): 200+ days\n\nWith Detection Engineering:\n- Custom rules tuned to environment\n- Low false positive rate (<10%)\n- Actionable alerts\n- Real attacks detected quickly\n- Mean Time to Detect (MTTD): Hours to days\n```\n\n**Case Study**: Target Breach (2013)\n\n```plaintext\nWhat happened:\n- Target had FireEye SIEM deployed\n- SIEM DID detect malware (alerted)\n- SOC dismissed alert as false positive\n- Attacker stole 40 million credit cards\n\nRoot cause:\n- Poor detection rule tuning\n- Alert fatigue (too many false positives)\n- No confidence in SIEM alerts\n\nLesson: Detection engineering prevents \"alert fatigue blindness\"\n```\n\n## SIEM Architecture Overview\n\n```plaintext\n┌─────────────────────────────────────────────────────────────┐\n│                     LOG SOURCES                              │\n│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐   │\n│  │ Windows  │  │  Linux   │  │ Firewall │  │   EDR    │   │\n│  │  Event   │  │  Syslog  │  │   Logs   │  │   Logs   │   │\n│  │   Logs   │  │          │  │          │  │          │   │\n│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘   │\n└───────┼─────────────┼─────────────┼─────────────┼──────────┘\n        │             │             │             │\n        └─────────────┴─────────────┴─────────────┘\n                          │\n                    (Forwarders)\n                          │\n                          ▼\n┌─────────────────────────────────────────────────────────────┐\n│                    SIEM PLATFORM                             │\n│                                                              │\n│  ┌────────────────────────────────────────────────────┐    │\n│  │  1. INGESTION                                       │    │\n│  │     - Receive logs from forwarders                  │    │\n│  │     - Parse and normalize                           │    │\n│  │     - Extract fields (src_ip, user, action)         │    │\n│  └────────────────────────────────────────────────────┘    │\n│                                                              │\n│  ┌────────────────────────────────────────────────────┐    │\n│  │  2. STORAGE                                         │    │\n│  │     - Index logs for fast searching                 │    │\n│  │     - Hot storage (recent 30 days)                  │    │\n│  │     - Warm/cold storage (archive)                   │    │\n│  └────────────────────────────────────────────────────┘    │\n│                                                              │\n│  ┌────────────────────────────────────────────────────┐    │\n│  │  3. DETECTION RULES (YOU BUILD THESE)              │    │\n│  │     - Correlation rules                             │    │\n│  │     - Scheduled searches                            │    │\n│  │     - Real-time alerts                              │    │\n│  │     - Machine learning models                       │    │\n│  └────────────────────────────────────────────────────┘    │\n│                                                              │\n│  ┌────────────────────────────────────────────────────┐    │\n│  │  4. ALERTING                                        │    │\n│  │     - Trigger alerts when rules match               │    │\n│  │     - Enrich with context                           │    │\n│  │     - Send to SOC queue                             │    │\n│  └────────────────────────────────────────────────────┘    │\n│                                                              │\n│  ┌────────────────────────────────────────────────────┐    │\n│  │  5. INVESTIGATION                                   │    │\n│  │     - Analyst reviews alert                         │    │\n│  │     - Pivots to related events                      │    │\n│  │     - Builds timeline                               │    │\n│  └────────────────────────────────────────────────────┘    │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Major SIEM Platforms\n\n### 1. Splunk Enterprise Security\n\n```plaintext\nMarket Leader: ~30% market share\n\nPros:\n✓ Powerful search language (SPL)\n✓ Mature platform (20+ years)\n✓ Extensive app ecosystem\n✓ Great for large datasets\n\nCons:\n✗ Expensive (licensing per GB/day)\n✗ Steep learning curve\n✗ Resource-intensive\n\nQuery Language: SPL (Search Processing Language)\nPricing: ~$150K-$500K/year for enterprise\n```\n\n### 2. Microsoft Sentinel\n\n```plaintext\nCloud-Native: Azure-based SIEM\n\nPros:\n✓ Cloud-native (no infrastructure)\n✓ Integrates with Microsoft 365, Azure, Windows\n✓ Pay-as-you-go pricing\n✓ Built-in threat intelligence\n\nCons:\n✗ Less mature than Splunk\n✗ Azure dependency\n✗ KQL learning curve\n\nQuery Language: KQL (Kusto Query Language)\nPricing: ~$2-$5 per GB ingested\n```\n\n### 3. Elastic Security (ELK Stack)\n\n```plaintext\nOpen-Source Foundation: Elasticsearch + Kibana\n\nPros:\n✓ Open-source option available\n✓ Flexible (logs + metrics + traces)\n✓ Good for DevOps/SecOps convergence\n\nCons:\n✗ Security features less mature\n✗ Requires more DIY setup\n\nQuery Language: Lucene / KQL\nPricing: Free (open-source) or $95/month per node (commercial)\n```\n\n### 4. IBM QRadar\n\n```plaintext\nEnterprise Focus: Common in large orgs\n\nPros:\n✓ Strong correlation engine\n✓ Good for compliance\n✓ On-prem or cloud\n\nCons:\n✗ Complex to configure\n✗ Expensive\n✗ Dated UI\n\nQuery Language: AQL (Ariel Query Language)\nPricing: ~$200K-$800K for enterprise\n```\n\n## The Detection Engineering Mindset\n\n**Think Like an Attacker**:\n\n```plaintext\nAttacker's Kill Chain:\n1. Reconnaissance → Can we detect scanning?\n2. Initial Access → Can we detect phishing/exploit?\n3. Execution → Can we detect code execution?\n4. Persistence → Can we detect new services/tasks?\n5. Privilege Escalation → Can we detect elevation attempts?\n6. Defense Evasion → Can we detect log clearing?\n7. Credential Access → Can we detect credential dumping?\n8. Discovery → Can we detect enumeration?\n9. Lateral Movement → Can we detect pivoting?\n10. Collection → Can we detect data staging?\n11. Exfiltration → Can we detect large transfers?\n12. Impact → Can we detect encryption/destruction?\n\nGoal: Detect at MULTIPLE stages (defense in depth)\n```\n\n**Detection Pyramid** (prioritize what to detect):\n\n```plaintext\n         ┌──────────┐\n         │   IOCs   │  ← Easy to change (IP, domain, hash)\n         │ (Fragile)│     Not reliable for detection\n         ├──────────┤\n         │  Tools   │  ← Medium (Mimikatz, Cobalt Strike)\n         │          │     Detectable but can be customized\n         ├──────────┤\n         │   TTPs   │  ← Hard to change (behavior patterns)\n         │ (Robust) │     Best for detection\n         └──────────┘\n\nFocus on TTPs: Behavioral patterns that are consistent across attacks\n```\n\n**Example**:\n\n```plaintext\nBad Detection (IOC-based):\n  \"Alert if connection to 185.220.101.5\"\n  \nProblem: Attacker changes IP, detection breaks\n\nGood Detection (TTP-based):\n  \"Alert if process injects into lsass.exe AND \n   creates network connection AND\n   connection uses non-standard port\"\n   \nReason: This behavior pattern (credential dumping + exfiltration) \n        is consistent even if tools/IPs change\n```\n\n## Learning Mindset\n\nDetection engineering is **part science, part art**:\n\n**Science**:\n- Understanding log formats\n- Writing queries\n- Statistical analysis\n- Measuring effectiveness\n\n**Art**:\n- Intuiting attacker behavior\n- Balancing sensitivity vs specificity\n- Crafting readable, maintainable rules\n- Knowing what NOT to detect\n\nYou're about to learn:\n- How to write Splunk and Sentinel detection rules\n- How to create universal Sigma rules\n- How to tune rules to reduce false positives\n- How to measure detection effectiveness\n- How to implement detection-as-code\n\nLet's start with Splunk SPL."
      }
    },
    {
      "type": "code_exercise",
      "title": "Splunk Detection Rules: From Basics to Advanced",
      "content": {
        "text": "**Splunk SPL** (Search Processing Language) is powerful but has a learning curve. Let's build detection rules progressively.\n\n## SPL Fundamentals\n\n### Basic Search Structure\n\n```splunk\nindex=<where> sourcetype=<what> <filters>\n| <transformation commands>\n```\n\n**Example**:\n\n```splunk\nindex=windows sourcetype=WinEventLog:Security EventCode=4624\n| stats count by user\n```\n\nReads as: \"In Windows Security logs, find successful logons (4624), count by user\"\n\n### Common SPL Commands\n\n```splunk\n# Search\nindex=windows user=admin\n\n# Stats (aggregation)\n| stats count, sum(bytes), avg(duration) by src_ip\n\n# Table (display fields)\n| table _time, user, action, result\n\n# Where (filtering after aggregation)\n| where count > 10\n\n# Eval (create calculated fields)\n| eval risk_score = if(failed_logins > 5, \"high\", \"low\")\n\n# Join (combine datasets)\n| join user [search index=hr_data]\n\n# Transaction (group related events)\n| transaction user maxspan=1h\n\n# Lookup (enrich with external data)\n| lookup threat_intel.csv ip as src_ip OUTPUT threat_level\n```\n\n## Detection 1: Brute Force Authentication\n\n**Threat**: Attacker attempts multiple failed logins\n\n**Detection Logic**:\n```plaintext\nIF failed_logins > 10 within 5 minutes\nTHEN alert\n```\n\n**Splunk Rule**:\n\n```splunk\nindex=windows sourcetype=WinEventLog:Security EventCode=4625\n| bin _time span=5m\n| stats count as failed_attempts by _time, src_ip, user\n| where failed_attempts > 10\n| eval severity=\"medium\"\n| eval description=\"Potential brute force attack: \" + tostring(failed_attempts) + \" failed logins from \" + src_ip\n```\n\n**Explanation**:\n1. `EventCode=4625`: Failed logon attempts\n2. `bin _time span=5m`: Group events into 5-minute buckets\n3. `stats count`: Count failures per time bucket, IP, user\n4. `where failed_attempts > 10`: Threshold\n5. `eval`: Create severity and description fields\n\n**Save as Alert**:\n\n```plaintext\nSplunk UI:\n1. Save search as \"Alert: Brute Force Authentication\"\n2. Schedule: Real-time\n3. Trigger: Per-result (alert for each IP exceeding threshold)\n4. Action: Send email to SOC, create notable event\n```\n\n## Detection 2: Credential Dumping (Mimikatz)\n\n**Threat**: Attacker uses Mimikatz to dump credentials from LSASS\n\n**Detection Logic** (Sysmon Event ID 10 - Process Access):\n\n```plaintext\nIF process accesses lsass.exe \nAND process is not whitelisted (not LSASS itself, not security tools)\nTHEN alert\n```\n\n**Splunk Rule**:\n\n```splunk\nindex=windows sourcetype=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" EventCode=10\n| where TargetImage=\"C:\\\\Windows\\\\System32\\\\lsass.exe\"\n| where NOT (SourceImage IN (\n    \"C:\\\\Windows\\\\System32\\\\lsass.exe\",\n    \"C:\\\\Program Files\\\\Microsoft\\\\*\",\n    \"C:\\\\Program Files\\\\CrowdStrike\\\\*\"\n  ))\n| table _time, Computer, SourceImage, SourceProcessId, SourceUser, GrantedAccess\n| eval severity=\"high\"\n| eval description=\"LSASS memory access by \" + SourceImage + \" (possible credential dumping)\"\n```\n\n**Explanation**:\n1. `EventCode=10`: Sysmon process access event\n2. `TargetImage=\"lsass.exe\"`: Targeting LSASS process\n3. `NOT SourceImage IN (...)`: Exclude legitimate tools\n4. `GrantedAccess`: Can add filter for specific access rights (0x1010 = read memory)\n\n**Tuning**:\n\n```splunk\n# Add more exclusions as you find false positives\n| where NOT (SourceImage IN (\n    \"C:\\\\Windows\\\\System32\\\\lsass.exe\",\n    \"C:\\\\Program Files\\\\Microsoft\\\\*\",\n    \"C:\\\\Program Files\\\\CrowdStrike\\\\*\",\n    \"C:\\\\Program Files\\\\1Password\\\\*\",  # Password manager\n    \"C:\\\\Windows\\\\System32\\\\csrss.exe\"  # Windows process\n  ))\n```\n\n## Detection 3: Lateral Movement via PsExec\n\n**Threat**: Attacker uses PsExec to execute commands on remote systems\n\n**Detection Logic**:\n\n```plaintext\nIF service created remotely (Event 7045)\nAND service name contains \"PSEXESVC\"\nOR service name is random 8-16 characters\nTHEN alert\n```\n\n**Splunk Rule**:\n\n```splunk\nindex=windows sourcetype=WinEventLog:System EventCode=7045\n| rex field=ServiceName \"^(?<random_name>[a-zA-Z0-9]{8,16})$\"\n| where ServiceName=\"PSEXESVC\" OR isnotnull(random_name)\n| lookup asset_inventory.csv ComputerName as Computer OUTPUT asset_type, owner\n| eval severity=\"high\"\n| eval description=\"Potential lateral movement: Service '\" + ServiceName + \"' created on \" + Computer\n| table _time, Computer, ServiceName, ServiceFileName, asset_type, owner\n```\n\n**Explanation**:\n1. `EventCode=7045`: Service installation\n2. `rex`: Extract random service names (PsExec creates random names)\n3. Check for \"PSEXESVC\" (default PsExec service) or random names\n4. `lookup`: Enrich with asset inventory (is this a server or workstation?)\n\n**Advanced Version** (correlated detection):\n\n```splunk\n# Step 1: Find PsExec service creation\nindex=windows sourcetype=WinEventLog:System EventCode=7045 ServiceName=\"PSEXESVC\"\n| rename Computer as dest_host\n| table _time, dest_host, ServiceFileName\n\n# Step 2: Correlate with network connection (within 5 minutes)\n| join type=inner dest_host [\n    search index=windows sourcetype=WinEventLog:Security EventCode=5156\n    | rename DestAddress as dest_host\n    | where DestPort=445\n    | table _time, SourceAddress, dest_host, DestPort\n  ]\n  \n# Step 3: Show attacker source and destination\n| eval severity=\"critical\"\n| eval description=\"Lateral movement: \" + SourceAddress + \" → \" + dest_host + \" via PsExec\"\n| table _time, SourceAddress, dest_host, ServiceFileName\n```\n\nThis correlates service creation with SMB network connection for high-confidence detection.\n\n## Detection 4: Data Exfiltration (Large Uploads)\n\n**Threat**: Attacker exfiltrates data via cloud storage\n\n**Detection Logic**:\n\n```plaintext\nIF upload to cloud domain (dropbox, mega, etc.)\nAND bytes_out > 100 MB\nAND user not in whitelist (IT, marketing)\nTHEN alert\n```\n\n**Splunk Rule** (using proxy logs):\n\n```splunk\nindex=proxy action=allowed method=POST\n| where bytes_out > 104857600  # 100 MB in bytes\n| lookup cloud_domains.csv domain as dest_domain OUTPUT cloud_service\n| where isnotnull(cloud_service)\n| lookup employee_roles.csv user OUTPUT department, manager\n| where department NOT IN (\"IT\", \"Marketing\", \"Engineering\")\n| stats sum(bytes_out) as total_bytes, count as uploads by user, dest_domain, cloud_service\n| eval total_mb = round(total_bytes / 1048576, 2)\n| eval severity=\"medium\"\n| eval description=user + \" uploaded \" + tostring(total_mb) + \" MB to \" + cloud_service\n| table _time, user, cloud_service, dest_domain, total_mb, department, manager\n```\n\n**Explanation**:\n1. Filter for HTTP POST (uploads)\n2. Filter for >100 MB\n3. Lookup known cloud storage domains\n4. Enrich with employee data\n5. Exclude departments with legitimate cloud use\n\n**Lookup Table** (cloud_domains.csv):\n\n```csv\ndomain,cloud_service\ndropbox.com,Dropbox\napi.dropboxapi.com,Dropbox\nmega.nz,Mega\nbox.com,Box\nonedrive.live.com,OneDrive\ndrive.google.com,Google Drive\n```\n\n## Detection 5: PowerShell Suspicious Commands\n\n**Threat**: Attacker uses PowerShell for malicious activity\n\n**Detection Logic** (PowerShell Script Block Logging - Event 4104):\n\n```plaintext\nIF PowerShell script contains suspicious keywords:\n  - DownloadString, DownloadFile (download malware)\n  - Invoke-Expression, IEX (execute code)\n  - -EncodedCommand, -enc (obfuscation)\n  - mimikatz, sekurlsa (credential theft)\n  - Invoke-Mimikatz (known attack tool)\nTHEN alert\n```\n\n**Splunk Rule**:\n\n```splunk\nindex=windows sourcetype=\"WinEventLog:Microsoft-Windows-PowerShell/Operational\" EventCode=4104\n| eval suspicious_score=0\n\n# Scoring system (accumulate points for suspicious indicators)\n| eval suspicious_score=suspicious_score + if(like(ScriptBlockText, \"%DownloadString%\"), 10, 0)\n| eval suspicious_score=suspicious_score + if(like(ScriptBlockText, \"%Invoke-Expression%\") OR like(ScriptBlockText, \"%IEX%\"), 10, 0)\n| eval suspicious_score=suspicious_score + if(like(ScriptBlockText, \"%-EncodedCommand%\") OR like(ScriptBlockText, \"%-enc%\"), 15, 0)\n| eval suspicious_score=suspicious_score + if(like(ScriptBlockText, \"%mimikatz%\") OR like(ScriptBlockText, \"%sekurlsa%\"), 30, 0)\n| eval suspicious_score=suspicious_score + if(like(ScriptBlockText, \"%Bypass%\"), 5, 0)\n| eval suspicious_score=suspicious_score + if(like(ScriptBlockText, \"%-NoP%\") OR like(ScriptBlockText, \"%-NonI%\"), 5, 0)\n\n# Alert if score exceeds threshold\n| where suspicious_score >= 20\n\n# Extract first 500 characters of script for context\n| eval script_preview=substr(ScriptBlockText, 1, 500)\n\n| eval severity=case(\n    suspicious_score >= 40, \"critical\",\n    suspicious_score >= 30, \"high\",\n    suspicious_score >= 20, \"medium\",\n    1=1, \"low\"\n  )\n\n| eval description=\"Suspicious PowerShell detected (score: \" + tostring(suspicious_score) + \"): \" + script_preview\n\n| table _time, ComputerName, User, suspicious_score, severity, script_preview\n```\n\n**Explanation**:\n- **Scoring system**: More sophisticated than binary yes/no\n- **Thresholds**: Adjust based on environment (start high, lower gradually)\n- **Context**: Include script preview for analyst investigation\n\n**Tuning** (whitelist legitimate scripts):\n\n```splunk\n# After scoring, exclude known-good scripts\n| eval script_hash=sha256(ScriptBlockText)\n| lookup whitelist_scripts.csv script_hash OUTPUT is_whitelisted\n| where isnull(is_whitelisted)\n```\n\n## Detection 6: Account Creation (Persistence)\n\n**Threat**: Attacker creates new account for persistence\n\n**Detection Logic**:\n\n```plaintext\nIF new user account created (Event 4720)\nAND creator is not HR/IT\nAND account added to privileged group (Event 4728, 4732)\nTHEN alert\n```\n\n**Splunk Rule**:\n\n```splunk\n# Step 1: Find account creations\nindex=windows sourcetype=WinEventLog:Security EventCode=4720\n| eval created_user=TargetUserName\n| eval creator=SubjectUserName\n| eval creator_domain=SubjectDomainName\n\n# Step 2: Check if creator is authorized\n| lookup authorized_admins.csv username as creator OUTPUT is_authorized\n| where isnull(is_authorized) OR is_authorized=\"false\"\n\n# Step 3: Check if account added to admin groups (within 5 minutes)\n| join type=left created_user [\n    search index=windows sourcetype=WinEventLog:Security (EventCode=4728 OR EventCode=4732)\n    | eval created_user=MemberName\n    | eval added_to_group=TargetUserName\n    | where added_to_group IN (\"Domain Admins\", \"Enterprise Admins\", \"Administrators\")\n    | table created_user, added_to_group\n  ]\n\n| eval severity=if(isnotnull(added_to_group), \"critical\", \"high\")\n\n| eval description=creator + \" created account '\" + created_user + \"'\" + if(isnotnull(added_to_group), \" and added to \" + added_to_group, \"\")\n\n| table _time, creator, creator_domain, created_user, added_to_group, severity\n```\n\n## Advanced: MITRE ATT&CK Mapping\n\n**Map each detection to ATT&CK**:\n\n```splunk\n# Add ATT&CK metadata to alert\n| eval mitre_tactic=\"Credential Access\"\n| eval mitre_technique=\"T1003.001 - LSASS Memory\"\n| eval mitre_url=\"https://attack.mitre.org/techniques/T1003/001/\"\n```\n\n**Benefits**:\n- Gap analysis (which techniques have no detection?)\n- Communication with management (\"We detect 78% of ATT&CK techniques\")\n- Threat intel integration (\"This APT uses T1003, T1055, T1071\")\n\n## Detection Validation (Testing)\n\n**Red Team Purple Team Exercise**:\n\n```splunk\n# Tag events from red team exercises\nindex=windows tag=redteam\n\n# Check: Did our detections fire?\n| join type=left _time, Computer [\n    search index=notable_events\n    | table _time, Computer, alert_name\n  ]\n\n# Show gaps\n| where isnull(alert_name)\n| stats count by EventCode, action\n| eval detection_gap=\"No alert triggered for this activity\"\n```\n\nSplunk SPL is powerful once you master the pipeline concept. Practice by translating detection hypotheses into queries."
      }
    },
    {
      "type": "explanation",
      "title": "Microsoft Sentinel KQL and Sigma Rules",
      "content": {
        "text": "## Microsoft Sentinel KQL (Kusto Query Language)\n\n**KQL** is the query language for Microsoft Sentinel (and Azure in general). Similar to SPL but with differences.\n\n### KQL Fundamentals\n\n**Basic Structure**:\n\n```kql\nTableName\n| where <filter>\n| extend <new_field> = <calculation>\n| summarize <aggregation> by <field>\n| project <fields_to_display>\n```\n\n### Common KQL Operators\n\n```kql\n// Filter\n| where TimeGenerated > ago(1h)\n| where AccountType == \"User\"\n\n// Create calculated field\n| extend RiskScore = iff(FailedLogons > 5, 100, 0)\n\n// Aggregation\n| summarize count(), sum(BytesSent) by SourceIP\n\n// Select columns\n| project TimeGenerated, UserName, Activity, Result\n\n// Join\n| join kind=inner (OtherTable) on $left.UserName == $right.UserName\n\n// Sorting\n| sort by TimeGenerated desc\n\n// Limit results\n| take 100\n```\n\n## KQL Detection 1: Brute Force (Sentinel)\n\n```kql\nSecurityEvent\n| where TimeGenerated > ago(5m)\n| where EventID == 4625  // Failed logon\n| summarize FailedAttempts = count() by SourceIP = IpAddress, TargetAccount = Account\n| where FailedAttempts > 10\n| extend Severity = \"Medium\"\n| extend Description = strcat(\"Brute force: \", tostring(FailedAttempts), \" failed logins from \", SourceIP)\n| project TimeGenerated, SourceIP, TargetAccount, FailedAttempts, Severity, Description\n```\n\n## KQL Detection 2: Impossible Travel\n\n**Threat**: User logs in from two geographically distant locations in short time (compromised account)\n\n```kql\nlet timeframe = 1h;\nlet distance_threshold = 500; // kilometers\n\nSigninLogs\n| where TimeGenerated > ago(timeframe)\n| where ResultType == \"0\"  // Successful logins\n| extend Country = LocationDetails.countryOrRegion\n| extend City = LocationDetails.city\n| extend Latitude = LocationDetails.geoCoordinates.latitude\n| extend Longitude = LocationDetails.geoCoordinates.longitude\n| summarize Locations = make_set(City), \n            Countries = make_set(Country),\n            LoginTimes = make_list(TimeGenerated),\n            IPs = make_set(IPAddress),\n            Coordinates = make_list(pack(\"lat\", Latitude, \"lon\", Longitude))\n    by UserPrincipalName\n| where array_length(Countries) > 1  // Logged in from multiple countries\n| mv-expand LoginTime = LoginTimes, Coordinate = Coordinates\n| extend Latitude1 = todouble(Coordinate.lat), Longitude1 = todouble(Coordinate.lon)\n// Calculate distance between locations (Haversine formula)\n// If distance > threshold AND time < 1 hour = impossible travel\n| extend Severity = \"High\"\n| extend Description = strcat(\"Impossible travel detected for user \", UserPrincipalName)\n| project UserPrincipalName, Locations, Countries, LoginTimes, IPs, Severity\n```\n\n## KQL Detection 3: Azure Resource Deletion\n\n**Threat**: Attacker deletes resources for disruption\n\n```kql\nAzureActivity\n| where TimeGenerated > ago(1h)\n| where OperationNameValue endswith \"delete\" and ActivityStatusValue == \"Success\"\n| where ResourceProviderValue in (\"Microsoft.Compute\", \"Microsoft.Storage\", \"Microsoft.Sql\")\n| summarize DeletedResources = make_set(Resource), \n            Operations = make_set(OperationNameValue),\n            Count = count()\n    by Caller, CallerIpAddress\n| where Count > 5  // More than 5 deletions in 1 hour\n| extend Severity = \"Critical\"\n| extend Description = strcat(Caller, \" deleted \", tostring(Count), \" resources from \", CallerIpAddress)\n| project TimeGenerated, Caller, CallerIpAddress, DeletedResources, Count, Severity\n```\n\n## Sigma Rules: Universal Detection Format\n\n**Sigma** is a generic signature format for SIEM systems. Write once, deploy anywhere.\n\n**Philosophy**: Detection logic shouldn't be tied to specific SIEM platform.\n\n### Sigma Rule Structure (YAML)\n\n```yaml\ntitle: Credential Dumping via Mimikatz\nid: 0ed1b4b2-3f8a-4e5b-9c7d-6e4f5a3b2c1d\nstatus: stable\ndescription: Detects LSASS memory access by non-authorized processes (Mimikatz-style credential dumping)\nauthor: SOC Team\ndate: 2024/01/15\nmodified: 2024/01/15\n\ntags:\n  - attack.credential_access\n  - attack.t1003.001\n\nlogsource:\n  product: windows\n  service: sysmon\n  \ndetection:\n  selection:\n    EventID: 10  # ProcessAccess\n    TargetImage|endswith: '\\lsass.exe'\n  \n  filter:\n    SourceImage|startswith:\n      - 'C:\\Windows\\System32\\'\n      - 'C:\\Program Files\\'\n    SourceImage|endswith:\n      - '\\csrss.exe'\n      - '\\wininit.exe'\n  \n  condition: selection and not filter\n  \nfalsepositives:\n  - Legitimate security tools (EDR, password managers)\n  - Windows diagnostic tools\n  \nlevel: high\n```\n\n### Converting Sigma to Splunk/Sentinel\n\n**Using sigmac (Sigma Converter)**:\n\n```bash\n# Install sigma tools\npip install sigma-cli\n\n# Convert to Splunk SPL\nsigmac -t splunk -c /path/to/config.yml mimikatz.yml\n\n# Output:\nindex=windows sourcetype=XmlWinEventLog:Sysmon EventCode=10 TargetImage=\"*\\\\lsass.exe\" NOT (SourceImage=\"C:\\\\Windows\\\\System32\\\\*\" OR SourceImage=\"C:\\\\Program Files\\\\*\")\n\n# Convert to Microsoft Sentinel KQL\nsigmac -t ata -c /path/to/config.yml mimikatz.yml\n\n# Output:\nDeviceProcessEvents\n| where ActionType == \"ProcessAccess\"\n| where TargetImage endswith \"\\\\lsass.exe\"\n| where not (SourceImage startswith \"C:\\\\Windows\\\\System32\\\\\")\n```\n\n### Sigma Rule: Suspicious PowerShell\n\n```yaml\ntitle: Suspicious PowerShell Commands\nid: 7e3f2a1b-4c5d-6e7f-8a9b-0c1d2e3f4a5b\nstatus: experimental\ndescription: Detects PowerShell commands commonly used in attacks\nauthor: Detection Team\ndate: 2024/01/15\n\ntags:\n  - attack.execution\n  - attack.t1059.001\n\nlogsource:\n  product: windows\n  service: powershell\n  definition: 'Requirements: Script Block Logging (Event ID 4104)'\n  \ndetection:\n  selection_download:\n    EventID: 4104\n    ScriptBlockText|contains:\n      - 'DownloadString'\n      - 'DownloadFile'\n      - 'WebClient'\n  \n  selection_execution:\n    EventID: 4104\n    ScriptBlockText|contains:\n      - 'Invoke-Expression'\n      - 'IEX'\n      - 'Invoke-Command'\n  \n  selection_obfuscation:\n    EventID: 4104\n    ScriptBlockText|contains:\n      - '-EncodedCommand'\n      - '-enc'\n      - 'FromBase64String'\n  \n  selection_bypass:\n    EventID: 4104\n    ScriptBlockText|contains:\n      - '-ExecutionPolicy Bypass'\n      - '-ep bypass'\n  \n  condition: (selection_download and selection_execution) or (selection_obfuscation and selection_bypass)\n  \nfalsepositives:\n  - Legitimate administrative scripts\n  - Deployment tools\n  - Configuration management (Ansible, SCCM)\n  \nlevel: medium\n```\n\n### Sigma Rule: PsExec Lateral Movement\n\n```yaml\ntitle: PsExec Service Installation\nid: 9a8b7c6d-5e4f-3a2b-1c0d-9e8f7a6b5c4d\nstatus: stable\ndescription: Detects service installation of PsExec (lateral movement indicator)\nauthor: Detection Team\ndate: 2024/01/15\n\ntags:\n  - attack.lateral_movement\n  - attack.t1021.002\n  - attack.execution\n  - attack.t1569.002\n\nlogsource:\n  product: windows\n  service: system\n  \ndetection:\n  selection:\n    EventID: 7045  # Service installation\n    ServiceName:\n      - 'PSEXESVC'\n      - 'PAExec'  # Alternative PsExec-like tool\n  \n  selection_random:  # PsExec can use random service names\n    EventID: 7045\n    ImagePath|contains: '\\ADMIN$\\'\n  \n  condition: selection or selection_random\n  \nfalsepositives:\n  - Legitimate IT administration using PsExec\n  - Software deployment tools\n  \nlevel: medium\n\nfields:\n  - ComputerName\n  - ServiceName\n  - ImagePath\n  - AccountName\n```\n\n## Sigma Rule Repository\n\n**Public Sigma Rules**: https://github.com/SigmaHQ/sigma\n\n```bash\n# Clone Sigma rule repository\ngit clone https://github.com/SigmaHQ/sigma.git\n\n# Browse rules\ncd sigma/rules\nls -la\n\n# Categories:\nwindows/\n  builtin/         # Windows Event Logs\n  sysmon/          # Sysmon logs\n  powershell/      # PowerShell logs\n  defender/        # Windows Defender logs\n  \nlinux/\n  auditd/          # Linux auditd logs\n  auth/            # Authentication logs\n  \ncloud/\n  aws/             # AWS CloudTrail\n  azure/           # Azure Activity Logs\n  gcp/             # GCP Audit Logs\n\nnetwork/\n  zeek/            # Zeek (Bro) logs\n  dns/             # DNS logs\n  firewall/        # Firewall logs\n```\n\n**Convert entire directory**:\n\n```bash\n# Convert all Windows rules to Splunk\nsigmac -t splunk -r sigma/rules/windows/ -o splunk_rules/\n\n# Now you have 500+ Splunk rules ready to deploy\n```\n\n## Detection-as-Code\n\n**Modern Approach**: Treat detection rules like software code\n\n### Git Workflow\n\n```bash\n# Create detection repository\nmkdir detection-rules\ncd detection-rules\ngit init\n\n# Directory structure\nmkdir -p rules/splunk rules/sentinel rules/sigma\nmkdir -p tests/ docs/\n\n# Create rule\ncat > rules/sigma/credential_dumping_lsass.yml <<EOF\ntitle: LSASS Credential Dumping\nid: abc123...\n# ... (Sigma rule content)\nEOF\n\n# Commit\ngit add rules/sigma/credential_dumping_lsass.yml\ngit commit -m \"Add LSASS credential dumping detection\"\n\n# Push to GitHub\ngit remote add origin https://github.com/company/detection-rules.git\ngit push origin main\n```\n\n### CI/CD Pipeline\n\n```yaml\n# .github/workflows/validate-rules.yml\n\nname: Validate Detection Rules\n\non:\n  pull_request:\n    paths:\n      - 'rules/**'\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v2\n      \n      - name: Install Sigma Tools\n        run: pip install sigma-cli\n      \n      - name: Validate Sigma Syntax\n        run: |\n          for rule in rules/sigma/*.yml; do\n            sigmac --target splunk --config sigma-config.yml \"$rule\" || exit 1\n          done\n      \n      - name: Run Unit Tests\n        run: pytest tests/\n      \n      - name: Check MITRE ATT&CK Mapping\n        run: python scripts/check_attack_mapping.py\n      \n      - name: Deploy to Dev SIEM\n        if: success()\n        run: python scripts/deploy_to_dev.py\n```\n\n### Testing Detection Rules\n\n```python\n# tests/test_lsass_detection.py\n\nimport pytest\nfrom detection_testing import SplunkTester\n\ndef test_lsass_credential_dumping():\n    tester = SplunkTester()\n    \n    # Benign event (should NOT alert)\n    benign_event = {\n        \"EventCode\": 10,\n        \"TargetImage\": \"C:\\\\Windows\\\\System32\\\\lsass.exe\",\n        \"SourceImage\": \"C:\\\\Windows\\\\System32\\\\csrss.exe\"\n    }\n    \n    result = tester.test_rule(\"lsass_detection.spl\", benign_event)\n    assert result.alerted == False, \"Should not alert on csrss.exe\"\n    \n    # Malicious event (SHOULD alert)\n    malicious_event = {\n        \"EventCode\": 10,\n        \"TargetImage\": \"C:\\\\Windows\\\\System32\\\\lsass.exe\",\n        \"SourceImage\": \"C:\\\\Temp\\\\mimikatz.exe\"\n    }\n    \n    result = tester.test_rule(\"lsass_detection.spl\", malicious_event)\n    assert result.alerted == True, \"Should alert on mimikatz.exe\"\n    assert result.severity == \"high\"\n```\n\n## Detection Engineering Metrics\n\n**Measure effectiveness**:\n\n```python\n# Detection effectiveness dashboard\n\nmetrics = {\n    \"total_rules\": 150,\n    \"rules_triggered_30d\": 42,\n    \"coverage\": 0.28,  # 28% of rules triggered (others may be dormant)\n    \n    \"alerts_generated_30d\": 1250,\n    \"true_positives\": 125,\n    \"false_positives\": 1125,\n    \"precision\": 0.10,  # 10% precision (90% FP rate - needs tuning!)\n    \n    \"incidents_detected\": 12,\n    \"incidents_missed\": 3,\n    \"recall\": 0.80,  # 80% recall (detected 12 of 15 incidents)\n    \n    \"mean_time_to_detect\": 45,  # minutes\n    \"mean_time_to_respond\": 120,  # minutes\n}\n```\n\n**Goals**:\n- **Precision**: >80% (low false positive rate)\n- **Recall**: >90% (catch most attacks)\n- **Coverage**: 100% of critical ATT&CK techniques\n- **MTTD**: <1 hour\n\nDetection engineering is iterative - measure, tune, improve, repeat."
      }
    },
    {
      "type": "real_world",
      "title": "False Positive Reduction and Rule Tuning",
      "content": {
        "text": "**Alert fatigue** kills detection programs. Let's fix it systematically.\n\n## The False Positive Problem\n\n**Typical Scenario**:\n\n```plaintext\nWeek 1 after rule deployment:\n- 500 alerts/day\n- Analyst investigates each\n- 490 false positives\n- 10 true positives\n- FP rate: 98%\n\nWeek 4:\n- Still 500 alerts/day\n- Analyst stops investigating\n- Assumes all are FP\n- Misses real attack buried in noise\n- BREACH OCCURS\n```\n\n**Goal**: Reduce to <50 alerts/day with <20% FP rate\n\n## Tuning Methodology: FILTER\n\n**F** - **Find** top false positive generators\n**I** - **Investigate** samples\n**L** - **Label** root cause\n**T** - **Tune** detection logic\n**E** - **Evaluate** effectiveness\n**R** - **Repeat** until acceptable\n\n### Step 1: Identify Top Offenders\n\n```splunk\n# Splunk query to find highest-volume alerts\nindex=notable_events\n| stats count by alert_name, severity\n| sort -count\n| head 20\n```\n\n**Output**:\n\n```plaintext\nalert_name                          | count | severity\n------------------------------------|-------|----------\nBrute Force Authentication          | 2450  | medium\nSuspicious PowerShell Commands      | 1820  | high\nPsExec Lateral Movement             | 980   | high\nLarge Data Upload                   | 750   | medium\nAccount Creation                    | 420   | high\n```\n\n### Step 2: Investigate Samples\n\n**Pull 20 samples** of \"Brute Force Authentication\":\n\n```splunk\nindex=notable_events alert_name=\"Brute Force Authentication\"\n| head 20\n| table _time, src_ip, user, failed_attempts, description\n```\n\n**Analysis**:\n\n```plaintext\nSample 1:\nTime: 2024-01-15 08:05:22\nIP: 10.0.5.50\nUser: admin\nAttempts: 12\nDescription: Potential brute force\n\nInvestigation:\n- IP 10.0.5.50 = IT admin workstation\n- User admin = legitimate admin account\n- Time 08:05 = start of business day\n- Context: Admin trying to log into server, forgot password, tried several times\nVerdict: FALSE POSITIVE (legitimate admin error)\n\nSample 2:\nTime: 2024-01-15 14:22:33\nIP: 185.220.101.5\nUser: admin\nAttempts: 50\nDescription: Potential brute force\n\nInvestigation:\n- IP 185.220.101.5 = External (Tor exit node)\n- User admin = common target\n- Time 14:22 = middle of day\n- Context: No legitimate admin at this IP\nVerdict: TRUE POSITIVE (actual attack)\n```\n\n**Pattern**: False positives are internal IPs, true positives are external IPs\n\n### Step 3: Tune Detection\n\n**Original Rule**:\n\n```splunk\nindex=windows EventCode=4625\n| stats count by src_ip, user\n| where count > 10\n```\n\n**Problem**: Doesn't distinguish internal vs external\n\n**Tuned Rule**:\n\n```splunk\nindex=windows EventCode=4625\n| eval is_internal=if(cidrmatch(\"10.0.0.0/8\", src_ip) OR cidrmatch(\"192.168.0.0/16\", src_ip), \"true\", \"false\")\n| stats count by src_ip, user, is_internal\n| eval threshold=if(is_internal=\"true\", 20, 10)  # Higher threshold for internal\n| where count > threshold\n| eval severity=if(is_internal=\"true\", \"low\", \"high\")  # Lower severity for internal\n```\n\n**Improvements**:\n1. Distinguishes internal vs external\n2. Higher threshold for internal (admins make mistakes)\n3. Lower severity for internal\n4. Maintains sensitivity for external\n\n### Step 4: Add Whitelisting\n\n**Create Lookup Table** (authorized_admins.csv):\n\n```csv\nip_address,hostname,user,reason\n10.0.5.50,IT-ADMIN-01,admin,Legitimate IT admin workstation\n10.0.5.51,IT-ADMIN-02,admin,Legitimate IT admin workstation\n10.0.8.10,HELPDESK-01,support,Help desk password resets\n```\n\n**Updated Rule**:\n\n```splunk\nindex=windows EventCode=4625\n| eval is_internal=if(cidrmatch(\"10.0.0.0/8\", src_ip), \"true\", \"false\")\n| stats count by src_ip, user, is_internal\n| eval threshold=if(is_internal=\"true\", 20, 10)\n| where count > threshold\n\n# Lookup whitelist\n| lookup authorized_admins.csv ip_address as src_ip, user as user OUTPUT reason\n\n# Suppress if whitelisted\n| where isnull(reason)\n\n| eval severity=if(is_internal=\"true\", \"low\", \"high\")\n```\n\n### Step 5: Time-Based Tuning\n\n**Observation**: More false positives during business hours (legitimate activity)\n\n**Time-Aware Rule**:\n\n```splunk\nindex=windows EventCode=4625\n| eval hour=strftime(_time, \"%H\")\n| eval is_business_hours=if(hour >= 8 AND hour <= 18, \"true\", \"false\")\n| eval is_internal=if(cidrmatch(\"10.0.0.0/8\", src_ip), \"true\", \"false\")\n| stats count by src_ip, user, is_internal, is_business_hours\n\n# Adjust threshold based on context\n| eval threshold=case(\n    is_internal=\"true\" AND is_business_hours=\"true\", 30,  # Internal + business hours = very high threshold\n    is_internal=\"true\" AND is_business_hours=\"false\", 15, # Internal + off-hours = medium threshold\n    is_internal=\"false\" AND is_business_hours=\"true\", 10, # External + business hours = low threshold\n    is_internal=\"false\" AND is_business_hours=\"false\", 5, # External + off-hours = very low threshold\n    1=1, 10\n  )\n\n| where count > threshold\n```\n\n**Logic**: \n- Internal + business hours = likely legitimate, high threshold\n- External + off-hours = very suspicious, low threshold\n\n### Step 6: User Behavior Baseline\n\n**Advanced**: Learn normal behavior per user\n\n```splunk\n# Build baseline (run weekly)\nindex=windows EventCode=4624  # Successful logins\n| stats avg(count) as avg_logins, stdev(count) as stddev_logins by user\n| outputlookup user_baseline.csv\n\n# Detection rule\nindex=windows EventCode=4625  # Failed logins\n| stats count as failed_logins by user\n| lookup user_baseline.csv user OUTPUT avg_logins, stddev_logins\n| eval z_score=(failed_logins - avg_logins) / stddev_logins\n| where z_score > 3  # More than 3 standard deviations = anomalous\n| eval severity=\"medium\"\n| eval description=user + \" has abnormal failed login count (z-score: \" + tostring(round(z_score, 2)) + \")\"\n```\n\n**Benefit**: Detects deviation from normal, not absolute threshold\n\n## Common False Positive Patterns\n\n### FP Pattern 1: Legitimate Admin Tools\n\n**Problem**: PsExec alert triggers on IT using PsExec\n\n**Solution**: Whitelist known IT admin IPs + require ticket\n\n```splunk\nindex=windows EventCode=7045 ServiceName=\"PSEXESVC\"\n\n# Lookup IT admin IPs\n| lookup it_admin_hosts.csv src_ip OUTPUT is_it_admin\n\n# Lookup change control tickets\n| lookup change_tickets.csv _time OUTPUT ticket_number\n\n# Alert only if NOT IT admin OR no ticket\n| where isnull(is_it_admin) OR isnull(ticket_number)\n```\n\n### FP Pattern 2: Scheduled Tasks\n\n**Problem**: Backup script triggers \"suspicious PowerShell\" alert nightly\n\n**Solution**: Hash-based whitelist\n\n```splunk\nindex=windows EventCode=4104\n| eval script_hash=sha256(ScriptBlockText)\n\n# Lookup known-good script hashes\n| lookup approved_scripts.csv script_hash OUTPUT script_name, owner\n\n# Alert only if NOT approved\n| where isnull(script_name)\n```\n\n**Maintain approved_scripts.csv**:\n\n```csv\nscript_hash,script_name,owner,approved_date\na3b5c7d9...,Nightly_Backup.ps1,IT Operations,2024-01-01\n1f2e3d4c...,User_Provisioning.ps1,HR Systems,2024-01-05\n```\n\n### FP Pattern 3: Application Behavior\n\n**Problem**: Web server makes large uploads (legitimate CDN sync) triggering \"data exfiltration\" alert\n\n**Solution**: Exclude known-good application flows\n\n```splunk\nindex=proxy bytes_out > 104857600\n\n# Lookup known-good application flows\n| lookup app_flows.csv src_ip, dest_domain OUTPUT app_name, is_approved\n\n# Alert only if NOT approved application\n| where isnull(is_approved) OR is_approved=\"false\"\n```\n\n**app_flows.csv**:\n\n```csv\nsrc_ip,dest_domain,app_name,is_approved,notes\n10.0.2.50,cdn.cloudflare.com,WebApp CDN Sync,true,Nightly sync at 2 AM\n10.0.3.10,s3.amazonaws.com,Backup Service,true,Database backups\n```\n\n## Measuring Tuning Effectiveness\n\n**Before Tuning**:\n\n```plaintext\nAlert: Brute Force Authentication\nVolume: 2,450 alerts/month\nFalse Positives: 2,400 (98%)\nTrue Positives: 50 (2%)\nPrecision: 2%\nAnalyst Time: 2,450 * 5 min = 204 hours\n```\n\n**After Tuning**:\n\n```plaintext\nAlert: Brute Force Authentication\nVolume: 120 alerts/month\nFalse Positives: 20 (17%)\nTrue Positives: 100 (83%)\nPrecision: 83%\nAnalyst Time: 120 * 5 min = 10 hours\n\nTime Saved: 194 hours/month\nImprovement: 95% reduction in volume, 80% precision gain\n```\n\n## Tuning Best Practices\n\n**DO**:\n- ✓ Start with high thresholds, lower gradually\n- ✓ Document all tuning decisions\n- ✓ Review whitelists quarterly (remove stale entries)\n- ✓ Test tuning changes in dev before production\n- ✓ Measure FP rate before/after tuning\n\n**DON'T**:\n- ✗ Suppress alerts completely (always investigate first)\n- ✗ Whitelist too broadly (\"suppress all from 10.0.0.0/8\")\n- ✗ Tune based on single sample (need statistical significance)\n- ✗ Forget to document why you tuned\n- ✗ Never review tuning (environment changes, tuning becomes stale)\n\n## Tuning Documentation Template\n\n```yaml\nalert_name: Brute Force Authentication\ntuning_date: 2024-01-15\ntuned_by: Jane Analyst\n\nproblem:\n  - 2,450 alerts/month\n  - 98% false positive rate\n  - Alert fatigue impacting SOC\n\nroot_cause_analysis:\n  - 80% FPs from internal IT admin IPs\n  - 15% FPs during business hours (legitimate password errors)\n  - 5% true positives (external attacks)\n\ntuning_actions:\n  1. Added IP-based thresholds (internal=20, external=10)\n  2. Created authorized_admins.csv whitelist\n  3. Implemented time-based thresholds\n  4. Lowered severity for internal IPs\n\nresults:\n  - Volume: 2,450 → 120 alerts/month (95% reduction)\n  - FP rate: 98% → 17% (81% improvement)\n  - Precision: 2% → 83%\n  - Analyst time saved: 194 hours/month\n\nnext_review: 2024-04-15\n```\n\nTuning is ongoing. Set quarterly reviews, measure effectiveness, iterate continuously."
      }
    },
    {
      "type": "reflection",
      "title": "Reflect on Detection Engineering Practice",
      "content": {
        "text": "Take a moment to reflect on the discipline of detection engineering:\n\n## The Art and Science Balance\n\n**Detection engineering requires both**:\n\n**Science (Quantitative)**:\n```plaintext\n- Measuring FP/TP rates\n- Statistical analysis (z-scores, baselines)\n- Performance metrics (MTTD, MTTR)\n- Coverage gaps (ATT&CK heatmaps)\n```\n\n**Art (Qualitative)**:\n```plaintext\n- Intuiting attacker behavior\n- Crafting readable, maintainable rules\n- Balancing sensitivity vs specificity\n- Knowing what NOT to detect\n```\n\n**Question**: Which comes more naturally to you? Are you more data-driven or intuition-driven?\n\n**Both are needed**. The best detection engineers blend data analysis with security intuition.\n\n## The False Positive Dilemma\n\n**Fundamental Tension**:\n\n```plaintext\nHigh Sensitivity (catch everything):\n- Detects all attacks ✓\n- BUT: Tons of false positives ✗\n- Result: Alert fatigue, missed real attacks\n\nHigh Specificity (only high-confidence):\n- Few false positives ✓\n- BUT: Misses subtle attacks ✗\n- Result: Attackers slip through\n```\n\n**Question**: Which is worse?\n1. Missing a real attack (false negative)\n2. Flooding SOC with false positives (false positives)\n\n**There's no right answer**. It depends on:\n- Organization risk tolerance\n- SOC capacity\n- Asset criticality\n\n**Most organizations**: Start with high sensitivity (catch everything), tune down false positives gradually.\n\n## The Baseline Problem\n\n**To detect anomalies, you need to know \"normal\"**:\n\n```plaintext\nChallenge: What is \"normal\"?\n\n- Users log in at different times\n- Applications behave differently \n- Network traffic varies by time/day\n- \"Normal\" changes over time\n```\n\n**Question**: How do you baseline \"normal\" in a dynamic environment?\n\n**Approaches**:\n1. **Statistical** (use averages, standard deviations)\n2. **Machine Learning** (let ML learn normal)\n3. **Manual** (document expected behavior)\n4. **Hybrid** (combine all three)\n\n**No perfect solution**. Baselining is ongoing work.\n\n## The Coverage vs Maintenance Trade-off\n\n**More rules = Better coverage, but...**:\n\n```plaintext\n50 Rules:\n- Cover 50 techniques\n- Maintenance: 2 hours/week\n- Manageable\n\n500 Rules:\n- Cover 200 techniques\n- Maintenance: 20 hours/week\n- Requires dedicated team\n\n5000 Rules:\n- Cover 250 techniques (diminishing returns)\n- Maintenance: 80+ hours/week\n- Unsustainable\n```\n\n**Question**: What's the right number of detection rules?\n\n**Answer**: Depends on:\n- Team size\n- Threat landscape\n- Risk tolerance\n\n**Rule of thumb**: 100-300 well-tuned rules for most organizations.\n\n**Focus on quality over quantity**.\n\n## The Attribution Question\n\n**Detection tells you WHAT happened, not WHO did it**:\n\n```plaintext\nDetection: \"LSASS memory access detected\"\n\nAnswers:\n✓ What: Credential dumping attempt\n✓ Where: LAPTOP-HR-023\n✓ When: 2024-01-15 14:22:33\n\nDoesn't answer:\n✗ Who: Employee? External attacker? Both?\n✗ Why: Curiosity? Malice? Accident?\n✗ What next: More attacks coming? One-off?\n```\n\n**Question**: Should detection rules attempt attribution?\n\n**Answer**: Generally no. Detection focuses on BEHAVIOR (what happened). Investigation determines attribution (who did it).\n\n**Why**: Attribution is complex, requires context beyond single event.\n\n## The Sigma Philosophy\n\n**Sigma proposes**: Detection logic should be platform-agnostic\n\n**Benefits**:\n```plaintext\n✓ Write once, deploy anywhere\n✓ Share rules across organizations\n✓ No vendor lock-in\n✓ Community contributions\n```\n\n**Challenges**:\n```plaintext\n✗ Conversion isn't perfect (some features don't translate)\n✗ Platform-specific optimizations lost\n✗ Tuning still needed per environment\n```\n\n**Question**: Should you use Sigma or native SIEM language?\n\n**Answer**: Both have value:\n- **Sigma**: For shareable, generic rules\n- **Native**: For optimized, environment-specific rules\n\n**Hybrid approach**: Start with Sigma, tune in native language.\n\n## Career Path Considerations\n\n**Detection Engineering is a specialized role**:\n\n**Skills Required**:\n```plaintext\n- Query languages (SPL, KQL)\n- Security knowledge (attack TTPs)\n- Data analysis\n- Scripting (Python, PowerShell)\n- Statistics (baselines, anomaly detection)\n- Communication (document decisions)\n```\n\n**Career Progression**:\n```plaintext\n1. SOC Analyst (use detections others wrote)\n2. Senior SOC Analyst (tune existing detections)\n3. Detection Engineer (write new detections)\n4. Senior Detection Engineer (strategy, mentoring)\n5. Detection Engineering Manager (lead team)\n```\n\n**Salary Range**:\n- Detection Engineer: $100K-$140K\n- Senior Detection Engineer: $130K-$170K\n- Manager: $150K-$200K\n\n**Demand**: Growing. More organizations building dedicated detection engineering teams.\n\n## The Measurement Challenge\n\n**How do you measure detection effectiveness?**\n\n**Easy to Measure**:\n```plaintext\n✓ Alert volume\n✓ False positive rate\n✓ Time to detect (for known incidents)\n```\n\n**Hard to Measure**:\n```plaintext\n✗ Attacks you DIDN'T detect (unknown unknowns)\n✗ Deterrent effect (attacks not attempted)\n✗ Coverage gaps (what you CAN'T detect)\n```\n\n**Question**: If you don't know what you're missing, how do you improve?\n\n**Approaches**:\n1. **Purple Team**: Red team attacks, measure detection\n2. **Threat Intel**: \"This APT uses T1003, do we detect it?\"\n3. **ATT&CK Coverage**: Map rules to framework, identify gaps\n4. **Incident Post-Mortems**: \"Why didn't we detect this?\"\n\n**Key Insight**: You can't manage what you don't measure, but some things are hard to measure.\n\n## Final Thoughts\n\n**Detection Engineering is critical** because:\n1. Preventive controls fail (always)\n2. EDR/antivirus miss things\n3. Attackers evolve constantly\n4. You need visibility to respond\n\n**Detection Engineering is challenging** because:\n1. Attackers adapt (cat and mouse game)\n2. Environments change (rules become stale)\n3. Data quality varies (garbage in, garbage out)\n4. False positives burn out analysts\n\n**Detection Engineering is rewarding** because:\n1. You directly impact security posture\n2. You catch attackers others miss\n3. You save organizations from breaches\n4. You constantly learn (threat landscape evolves)\n\n**Question**: After this lesson, do you want to specialize in detection engineering?\n\n**If yes**: Start writing detection rules in a lab. Practice with Splunk/Sentinel free trials. Study MITRE ATT&CK. Contribute to Sigma rule repository.\n\n**If no**: That's okay. Detection engineering isn't for everyone. But you now understand how detections work, making you better at whatever security role you pursue.\n\nWelcome to the world of detection engineering. You now speak the language of SIEM and can build the sensors that protect organizations from cyber threats."
      }
    },
    {
      "type": "memory_aid",
      "title": "SIEM Detection Engineering Quick Reference",
      "content": {
        "text": "## Detection Engineering Workflow: DESIGN\n\n**D** - **Define** threat (what are we detecting?)\n**E** - **Enumerate** data sources (where is evidence?)\n**S** - **Specify** logic (how do we detect it?)\n**I** - **Implement** rule (write SPL/KQL/Sigma)\n**G** - **Generate** test cases (benign + malicious)\n**N** - **Notice** false positives (tune, iterate)\n\n## SPL Query Structure: SWEAT\n\n**S** - **Search**: `index=windows sourcetype=WinEventLog`\n**W** - **Where**: `| where EventCode=4625`\n**E** - **Eval**: `| eval is_external=if(...)`\n**A** - **Aggregate**: `| stats count by src_ip, user`\n**T** - **Threshold**: `| where count > 10`\n\n## Common SPL Commands\n\n```splunk\n# Search\nindex=<index> sourcetype=<sourcetype> <filters>\n\n# Stats (aggregation)\n| stats count, sum(bytes), avg(duration) by field\n\n# Eval (calculated fields)\n| eval new_field=if(condition, value_if_true, value_if_false)\n\n# Where (filter after aggregation)\n| where count > threshold\n\n# Table (display)\n| table _time, field1, field2\n\n# Lookup (enrich)\n| lookup lookup_table.csv input_field OUTPUT output_field\n\n# Join\n| join field [search ...]\n```\n\n## Common KQL Operators\n\n```kql\n// Filter\n| where TimeGenerated > ago(1h)\n| where Field == \"Value\"\n\n// Extend (calculated field)\n| extend NewField = iff(Condition, TrueValue, FalseValue)\n\n// Summarize (aggregation)\n| summarize count(), sum(Field) by GroupField\n\n// Project (select columns)\n| project TimeGenerated, Field1, Field2\n\n// Join\n| join kind=inner (OtherTable) on $left.Field == $right.Field\n\n// Sort\n| sort by TimeGenerated desc\n```\n\n## Sigma Rule Template\n\n```yaml\ntitle: <Detection Name>\nid: <UUID>\nstatus: stable | experimental | deprecated\ndescription: <What does this detect>\nauthor: <Your Name>\ndate: YYYY/MM/DD\n\ntags:\n  - attack.<tactic>\n  - attack.t<technique_id>\n\nlogsource:\n  product: windows | linux | aws | azure\n  service: <log_type>\n  \ndetection:\n  selection:\n    Field: value\n    Field|contains: substring\n  \n  filter:\n    Field: excluded_value\n  \n  condition: selection and not filter\n  \nfalsepositives:\n  - <Known FP scenarios>\n  \nlevel: critical | high | medium | low | informational\n\nfields:\n  - <key_fields_for_analyst>\n```\n\n## Detection Examples Quick Reference\n\n### Brute Force (SPL)\n\n```splunk\nindex=windows EventCode=4625\n| bin _time span=5m\n| stats count by _time, src_ip, user\n| where count > 10\n```\n\n### Brute Force (KQL)\n\n```kql\nSecurityEvent\n| where EventID == 4625\n| summarize Count = count() by bin(TimeGenerated, 5m), SourceIP = IpAddress, Account\n| where Count > 10\n```\n\n### LSASS Access (SPL)\n\n```splunk\nindex=windows sourcetype=Sysmon EventCode=10\n| where TargetImage=\"*\\\\lsass.exe\"\n| where NOT SourceImage=\"*\\\\System32\\\\*\"\n```\n\n### LSASS Access (KQL)\n\n```kql\nDeviceProcessEvents\n| where ActionType == \"ProcessAccess\"\n| where TargetImage endswith \"\\\\lsass.exe\"\n| where not(SourceImage startswith \"C:\\\\Windows\\\\System32\\\\\")\n```\n\n### Suspicious PowerShell (SPL)\n\n```splunk\nindex=windows EventCode=4104\n| where like(ScriptBlockText, \"%DownloadString%\") OR like(ScriptBlockText, \"%IEX%\")\n```\n\n### PsExec Lateral Movement (SPL)\n\n```splunk\nindex=windows EventCode=7045\n| where ServiceName=\"PSEXESVC\" OR match(ServiceName, \"^[a-zA-Z0-9]{8,16}$\")\n```\n\n## False Positive Tuning: FILTER\n\n**F** - **Find** top FP generators (`| stats count by alert_name`)\n**I** - **Investigate** samples (pull 20 alerts, analyze)\n**L** - **Label** root cause (internal IP? scheduled task?)\n**T** - **Tune** rule (add exclusions, adjust thresholds)\n**E** - **Evaluate** results (measure FP rate before/after)\n**R** - **Repeat** until acceptable FP rate\n\n## Tuning Techniques\n\n### 1. IP-Based Thresholds\n\n```splunk\n| eval threshold=if(cidrmatch(\"10.0.0.0/8\", src_ip), 20, 10)\n| where count > threshold\n```\n\n### 2. Time-Based Thresholds\n\n```splunk\n| eval hour=strftime(_time, \"%H\")\n| eval threshold=if(hour >= 8 AND hour <= 18, 30, 10)\n| where count > threshold\n```\n\n### 3. Whitelist Lookup\n\n```splunk\n| lookup whitelist.csv field OUTPUT is_whitelisted\n| where isnull(is_whitelisted)\n```\n\n### 4. Hash-Based Exclusion\n\n```splunk\n| eval script_hash=sha256(ScriptBlockText)\n| lookup approved_scripts.csv script_hash OUTPUT script_name\n| where isnull(script_name)\n```\n\n### 5. Statistical Baseline\n\n```splunk\n| lookup baseline.csv user OUTPUT avg_value, stddev_value\n| eval z_score=(current_value - avg_value) / stddev_value\n| where z_score > 3\n```\n\n## MITRE ATT&CK Coverage Mapping\n\n```splunk\n# Add to every detection rule\n| eval mitre_tactic=\"<Tactic Name>\"\n| eval mitre_technique=\"T<ID> - <Technique Name>\"\n| eval mitre_sub_technique=\"T<ID>.<SUB_ID>\"\n```\n\n**Example**:\n\n```splunk\n| eval mitre_tactic=\"Credential Access\"\n| eval mitre_technique=\"T1003 - OS Credential Dumping\"\n| eval mitre_sub_technique=\"T1003.001 - LSASS Memory\"\n```\n\n## Detection Metrics\n\n**Measure Effectiveness**:\n\n```python\nmetrics = {\n    # Coverage\n    \"total_rules\": 150,\n    \"attack_techniques_covered\": 85,\n    \"coverage_percentage\": 0.42,  # 42% of ATT&CK\n    \n    # Quality\n    \"alerts_per_day\": 45,\n    \"false_positive_rate\": 0.15,  # 15%\n    \"true_positive_rate\": 0.85,   # 85%\n    \"precision\": 0.85,\n    \n    # Performance\n    \"mean_time_to_detect\": 35,      # minutes\n    \"mean_time_to_investigate\": 25, # minutes\n    \"mean_time_to_respond\": 60,     # minutes\n}\n```\n\n**Goals**:\n- Coverage: >80% of critical techniques\n- FP Rate: <20%\n- Precision: >80%\n- MTTD: <1 hour\n\n## Detection-as-Code Checklist\n\n```\n[ ] Store rules in Git repository\n[ ] Version control all changes\n[ ] Document rationale for each rule\n[ ] Map to MITRE ATT&CK\n[ ] Write unit tests (benign + malicious samples)\n[ ] CI/CD pipeline validates before deploy\n[ ] Peer review for new rules\n[ ] Quarterly review of existing rules\n[ ] Deprecate unused rules\n```\n\n## Common Data Sources\n\n**Windows**:\n- Security Event Log (EventCode 4624, 4625, 4688, 4720, etc.)\n- Sysmon (EventCode 1, 3, 7, 8, 10, 11, etc.)\n- PowerShell Operational (EventCode 4104)\n- System Log (EventCode 7045)\n\n**Network**:\n- Firewall logs (connections allowed/blocked)\n- Proxy logs (HTTP/HTTPS requests)\n- DNS logs (domain queries)\n- NetFlow (traffic metadata)\n\n**Cloud**:\n- AWS CloudTrail (API calls)\n- Azure Activity Logs (resource changes)\n- GCP Audit Logs (admin activity)\n- O365 Audit Logs (email, SharePoint, OneDrive)\n\n**Endpoint**:\n- EDR telemetry (CrowdStrike, SentinelOne, Defender)\n- Process execution\n- Network connections\n- File modifications\n\n## Resources\n\n**Splunk**:\n- Splunk Docs: https://docs.splunk.com/\n- SPL Reference: https://docs.splunk.com/Documentation/Splunk/latest/SearchReference\n- Splunk Boss of the SOC (practice): https://www.splunk.com/en_us/blog/conf-splunklive/bots.html\n\n**Microsoft Sentinel**:\n- Sentinel Docs: https://learn.microsoft.com/en-us/azure/sentinel/\n- KQL Reference: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\n- Sample queries: https://github.com/Azure/Azure-Sentinel\n\n**Sigma**:\n- Sigma Rules: https://github.com/SigmaHQ/sigma\n- Sigma Docs: https://github.com/SigmaHQ/sigma/wiki\n- Sigmac (converter): https://github.com/SigmaHQ/sigma/tree/master/tools/sigmac\n\n**MITRE ATT&CK**:\n- ATT&CK Matrix: https://attack.mitre.org/\n- ATT&CK Navigator: https://mitre-attack.github.io/attack-navigator/\n\n## Key Takeaways\n\n**Detection Engineering**:\n- Is both science (metrics) and art (intuition)\n- Requires understanding attacker TTPs\n- Focuses on behavior, not just IOCs\n- Is iterative (measure, tune, improve)\n\n**Effective Detections**:\n- Have low false positive rates (<20%)\n- Are well-documented (why does this detect X?)\n- Are mapped to MITRE ATT&CK\n- Are tested against benign and malicious samples\n- Are version controlled\n\n**Career Path**:\n- Growing demand for detection engineers\n- Requires query language skills (SPL/KQL)\n- Requires security knowledge (attack patterns)\n- Salary range: $100K-$170K\n\nYou now have the foundation to write, tune, and maintain SIEM detection rules that protect organizations from cyber threats."
      }
    },
    {
      "type": "video",
      "title": "Video Tutorial: SIEM Detection Engineering",
      "content": {
        "resources": "Watch this comprehensive video tutorial on writing and tuning SIEM detection rules.\n\n**Video**: [SIEM Detection Engineering with Splunk by John Hammond](https://www.youtube.com/watch?v=YKW6rMRM_kk)\n\n**What you'll see:**\n- Introduction to SIEM architecture and detection engineering\n- Splunk SPL basics and advanced queries\n- Writing detection rules for common attack patterns\n- Tuning rules to reduce false positives\n- Integration with MITRE ATT&CK framework\n- Detection-as-code workflow demonstration\n- Real-world case studies of detection engineering\n\n**Duration**: ~50 minutes\n\n**Additional Resources:**\n- [Splunk Documentation](https://docs.splunk.com/)\n- [Microsoft Sentinel Documentation](https://learn.microsoft.com/en-us/azure/sentinel/)\n- [Sigma Rules Repository](https://github.com/SigmaHQ/sigma)\n- [MITRE ATT&CK](https://attack.mitre.org/)\n- [Splunk Boss of the SOC](https://www.splunk.com/en_us/blog/conf-splunklive/bots.html) - Practice dataset\n- [Detection Lab](https://github.com/clong/DetectionLab) - Local SIEM lab environment\n\n**Hands-On Practice**:\n\n**Splunk Practice**:\n- Download Splunk Free: https://www.splunk.com/en_us/download/splunk-enterprise.html\n- Install on VM or local machine\n- Download Boss of the SOC dataset\n- Practice writing detection rules\n\n**Microsoft Sentinel Practice**:\n- Azure Free Tier: https://azure.microsoft.com/en-us/free/\n- Deploy Sentinel (free for 30 days)\n- Connect data sources (Windows VMs)\n- Write KQL detection rules\n\n**Detection Lab**:\n```bash\ngit clone https://github.com/clong/DetectionLab.git\ncd DetectionLab\nvagrant up  # Deploys full lab (Splunk, Windows, AD)\n```\n\n**Sigma Practice**:\n```bash\ngit clone https://github.com/SigmaHQ/sigma.git\ncd sigma/rules/windows/\n# Browse rules, learn patterns\n# Write your own Sigma rules\n# Convert to SPL/KQL with sigmac\n```\n\n**Community**:\n- Splunk Community: https://community.splunk.com/\n- Detection Engineering Slack/Discord groups\n- Sigma contributors on GitHub\n\nThis video provides practical demonstrations of detection engineering and reinforces the concepts covered in this lesson."
      }
    }
  ],
  "post_assessment": [
    {
      "question": "Your 'Brute Force Authentication' detection rule generates 500 alerts/day with a 95% false positive rate. Investigation shows most FPs are from internal IT admin IPs during business hours. What is the BEST tuning approach?",
      "options": [
        "Disable the rule - it's generating too many false positives",
        "Whitelist all internal IPs (10.0.0.0/8) to eliminate internal false positives",
        "Implement context-aware thresholds: higher threshold for internal IPs during business hours, lower for external IPs",
        "Increase the threshold globally from 10 to 50 failed logins"
      ],
      "correct_answer": 2,
      "explanation": "**Correct: Context-aware thresholds.** This balances sensitivity with specificity. The tuned rule would use different thresholds based on context: (1) Internal IP + business hours = 30 attempts (IT admins make mistakes), (2) Internal IP + off-hours = 15 attempts (suspicious), (3) External IP + business hours = 10 attempts, (4) External IP + off-hours = 5 attempts (very suspicious). Option A (disable) defeats the purpose - you're blind to attacks. Option B (whitelist all internal) is dangerous - insider threats and compromised internal systems won't be detected. Option D (global threshold increase) reduces false positives BUT also reduces sensitivity to real attacks - an external attacker with 40 attempts would be missed. Context-aware tuning maintains high sensitivity where needed (external, off-hours) while reducing FPs where expected (internal, business hours).",
      "question_id": "bd976cee-d073-44ab-bd3d-f8f44fb59357",
      "type": "multiple_choice",
      "difficulty": 3
    },
    {
      "question": "You write a Sigma rule to detect LSASS credential dumping. You convert it to Splunk SPL and deploy. Testing shows it works perfectly. Three months later, it stops alerting. What is the MOST LIKELY cause?",
      "options": [
        "Attackers learned to evade your detection",
        "The Sigma-to-SPL conversion was flawed and had a delayed bug",
        "Log source changed (Sysmon upgraded, field names changed)",
        "Your SIEM stopped ingesting logs"
      ],
      "correct_answer": 2,
      "explanation": "**Correct: Log source changed.** This is the most common reason for 'detection drift.' Field names, log formats, or parsers change when you upgrade log sources (Sysmon, Windows, EDR). Your rule expects `TargetImage` but new version uses `TargetFileName`. Suddenly rule stops matching. This is why detection-as-code is important - you need monitoring to detect when rules stop firing. Option A is possible but less likely than infrastructure changes. Option B is unlikely - if conversion worked initially, it won't spontaneously break. Option D would be obvious (ALL detections stop, not just one). Prevention: (1) Test rules after any log source upgrade, (2) Monitor 'rule effectiveness' - alert if a rule hasn't fired in X days (might mean it's broken OR attacker changed tactics), (3) Version control rules and log schemas together. This demonstrates why detection engineering is ONGOING, not one-time.",
      "question_id": "5dff7844-0fde-45c4-a3cb-a44bbe12e35f",
      "type": "multiple_choice",
      "difficulty": 3
    },
    {
      "question": "You're building a detection rule for PowerShell-based attacks. Which approach provides the MOST ROBUST detection?",
      "options": [
        "Alert on PowerShell.exe execution with specific command-line flags (-EncodedCommand, -NoProfile, -WindowStyle Hidden)",
        "Alert on PowerShell downloading and executing content (DownloadString + Invoke-Expression pattern)",
        "Create a scoring system: accumulate points for multiple suspicious indicators, alert if score exceeds threshold",
        "Whitelist approved PowerShell scripts by hash, alert on any script not in whitelist"
      ],
      "correct_answer": 2,
      "explanation": "**Correct: Scoring system.** This is the most robust because attackers can evade single-indicator rules. Here's why: Option A detects specific flags, but attacker can use different flags or obfuscate (e.g., `-No` instead of `-NoProfile`). Option B detects specific pattern, but attacker can use `Invoke-RestMethod` instead of `DownloadString`, or `&` instead of `IEX`. Option D (whitelist) is brittle - every new legitimate script needs whitelisting, and attackers can use approved scripts maliciously. **Scoring system** (option C) is resilient: Award points for MULTIPLE suspicious behaviors: +10 for DownloadString, +10 for IEX, +15 for EncodedCommand, +5 for Bypass, +5 for Hidden window. Alert if score ≥20. Attacker must avoid ALL indicators to evade (harder). Even if attacker avoids one indicator, others still accumulate. Example: Attacker uses `Invoke-RestMethod` (avoids DownloadString) but still uses IEX, EncodedCommand, Bypass = 30 points = detected. This multi-indicator approach mirrors how EDR works and is more resistant to evasion.",
      "question_id": "b0601409-54ca-490a-bb9a-8dbdc1439f3d",
      "type": "multiple_choice",
      "difficulty": 3
    },
    {
      "question": "You have 150 detection rules mapped to MITRE ATT&CK. Coverage analysis shows you detect 65% of techniques. Management asks: 'Why not 100%?' What is the BEST response?",
      "options": [
        "We need more budget to deploy additional detection rules for the missing 35%",
        "Some techniques are not detectable with current log sources; we need to add EDR/NDR for full coverage",
        "100% coverage is unrealistic - some techniques are undetectable, others are low-risk, and perfect coverage creates maintenance burden",
        "We're working on it - full coverage will be achieved next quarter"
      ],
      "correct_answer": 2,
      "explanation": "**Correct: 100% coverage is unrealistic and may not be optimal.** Here's the nuanced reality: (1) **Some techniques are undetectable** with reasonable logging (e.g., T1600.001 - Firmware Corruption requires specialized monitoring). (2) **Not all techniques are equally relevant** to your environment (e.g., cloud techniques if you're purely on-prem). (3) **Diminishing returns** - the last 35% of coverage requires 80% of the effort. (4) **Maintenance burden** - 500 rules (100% coverage) requires 40 hrs/week maintenance vs 150 rules (65% coverage) requiring 8 hrs/week. **Better approach**: Risk-based coverage. Cover 100% of HIGH-risk techniques for YOUR environment, 80% of medium-risk, 50% of low-risk. Option A (more budget) doesn't address whether those techniques are worth detecting. Option B (add log sources) is partially true but doesn't address relevance. Option D (full coverage next quarter) sets unrealistic expectation. Educate management: detection engineering prioritizes HIGH-VALUE detections, not checklist completion. 65% coverage is actually GOOD if it covers the right 65%. Show ATT&CK heatmap colored by risk to your organization.",
      "question_id": "c6f96fa3-e990-4518-a136-ab6fd02e3544",
      "type": "multiple_choice",
      "difficulty": 3
    }
  ],
  "jim_kwik_principles": [
    "active_learning",
    "teach_like_im_10",
    "connect_to_what_i_know",
    "memory_hooks",
    "minimum_effective_dose",
    "meta_learning",
    "reframe_limiting_beliefs",
    "gamify_it",
    "learning_sprint",
    "multiple_memory_pathways"
  ]
}