{
  "lesson_id": "e8f7d6c5-b9a8-4e7d-9c8b-6e5d4c3b2a1f",
  "domain": "osint",
  "title": "OSINT Automation & Tool Integration",
  "difficulty": 3,
  "order_index": 10,
  "prerequisites": [
    "84542765-253e-4b68-b230-3abfb6d16f54"
  ],
  "concepts": [
    "Recon-ng framework mastery",
    "SpiderFoot automation platform",
    "TheHarvester for enumeration",
    "OSINT Framework overview",
    "Custom Python OSINT scripts",
    "API integration (Shodan, Hunter.io, VirusTotal, Censys)",
    "CI/CD for continuous OSINT monitoring",
    "Building OSINT dashboards",
    "Workflow automation and orchestration"
  ],
  "estimated_time": 60,
  "learning_objectives": [
    "Master Recon-ng framework for automated reconnaissance and intelligence gathering",
    "Deploy SpiderFoot for comprehensive automated OSINT collection across 200+ data sources",
    "Build custom Python OSINT scripts integrating multiple APIs (Shodan, VirusTotal, Hunter.io)",
    "Design and implement continuous OSINT monitoring pipelines with alerting",
    "Create OSINT dashboards using ELK Stack or similar visualization platforms",
    "Understand workflow orchestration and CI/CD principles for intelligence automation"
  ],
  "content_blocks": [
    {
      "type": "mindset_coach",
      "content": {
        "text": "## Welcome to OSINT Automation Mastery\n\nYou've made it to the culmination of your OSINT journey! This lesson represents the transition from manual investigator to automation architect. You're about to learn how professional threat intelligence teams, investigative journalists, and security operations centers scale their OSINT capabilities from individual investigations to continuous, organization-wide intelligence programs.\n\n### Why Automation Matters\n\nThink about the OSINT techniques you've learned so far\u2014social media reconnaissance, domain enumeration, image geolocation, dark web monitoring. Now imagine performing these tasks not once, but continuously, across hundreds of targets, 24/7. That's the power of automation.\n\n**Real-world impact**: Recorded Future processes over 1 billion intelligence events daily through automated OSINT pipelines. Mandiant's threat intelligence platform correlates OSINT from 300+ sources automatically. These aren't magic\u2014they're well-designed automation workflows you're about to learn.\n\n### The Mindset Shift\n\nAutomation isn't about replacing human intelligence\u2014it's about amplifying it. Your role evolves from \"finder\" to \"architect\":\n\n- **Manual OSINT**: You find one email address for a target\n- **Automated OSINT**: Your scripts find 500 email addresses, validate them, check breach databases, correlate with social profiles, and alert you only to high-value findings\n\nThis lesson will challenge you to think in **systems** rather than **searches**. You'll learn to ask: \"How can I make this repeatable, scalable, and maintainable?\"\n\n### What You'll Build\n\nBy the end of this lesson, you'll have built:\n1. A Recon-ng workspace that automates subdomain enumeration and port scanning\n2. A SpiderFoot scan correlating 50+ OSINT data points for a target\n3. A custom Python script integrating Shodan, VirusTotal, and Hunter.io APIs\n4. A continuous monitoring pipeline that alerts you to new findings\n5. An OSINT dashboard visualizing your intelligence data\n\n**Jim Kwik principle**: \"The fastest way to learn is to teach.\" As you build these automations, imagine explaining each component to a junior analyst. This mental model will deepen your understanding.\n\nLet's transform you from OSINT practitioner to OSINT automation engineer!"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "## The OSINT Automation Landscape\n\n### Why Automation Is Critical\n\nModern OSINT faces three fundamental challenges:\n\n1. **Volume**: The surface area of digital intelligence is exponential (4.9 billion internet users, 1.13 billion websites, 500 million tweets daily)\n2. **Velocity**: Intelligence becomes stale rapidly (average breach detection time: 207 days, but most infrastructure changes within 48 hours)\n3. **Variety**: OSINT sources span 200+ platforms (social media, DNS, certificates, code repositories, paste sites, dark web forums)\n\nManual OSINT cannot scale to meet these challenges. Automation is mandatory for professional-grade intelligence operations.\n\n### The OSINT Automation Stack\n\nProfessional OSINT automation involves four layers:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 4: Visualization & Reporting         \u2502\n\u2502  (Kibana, Grafana, Custom Dashboards)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 3: Orchestration & Workflow          \u2502\n\u2502  (Airflow, n8n, Custom Schedulers)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 2: Data Collection & Processing      \u2502\n\u2502  (Recon-ng, SpiderFoot, TheHarvester)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 1: Data Sources & APIs               \u2502\n\u2502  (Shodan, VirusTotal, Hunter.io, Censys)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nThis lesson will teach you to build systems spanning all four layers.\n\n### Framework #1: Recon-ng\n\n**Recon-ng** is a full-featured reconnaissance framework written in Python. Think of it as \"Metasploit for OSINT\"\u2014a modular platform for automated intelligence gathering.\n\n**Key architecture concepts**:\n\n- **Workspaces**: Isolated environments for different investigations (like Metasploit workspaces)\n- **Modules**: 90+ pre-built reconnaissance modules organized by category\n- **Marketplace**: Centralized repository for discovering and installing modules\n- **API Keys**: Integrated key management for 30+ APIs (Shodan, VirusTotal, etc.)\n- **Database**: Built-in SQLite database automatically correlating findings\n- **Reporting**: Export results to HTML, CSV, JSON, XML\n\n**Module categories**:\n```\nrecon/\n\u251c\u2500\u2500 domains-companies/      # Company attribution\n\u251c\u2500\u2500 domains-contacts/        # Email/contact discovery\n\u251c\u2500\u2500 domains-hosts/           # Subdomain enumeration\n\u251c\u2500\u2500 hosts-domains/           # Reverse DNS\n\u251c\u2500\u2500 hosts-hosts/             # IP correlation\n\u251c\u2500\u2500 contacts-profiles/       # Social media discovery\n\u2514\u2500\u2500 profiles-contacts/       # Contact extraction from profiles\n\nreporting/\n\u251c\u2500\u2500 html                     # HTML reports\n\u251c\u2500\u2500 json                     # JSON exports\n\u2514\u2500\u2500 csv                      # Spreadsheet exports\n\nexploit/\n\u2514\u2500\u2500 [Various data correlation and analysis modules]\n```\n\n**Workflow philosophy**: Recon-ng follows a \"pipeline\" model where modules feed data into subsequent modules:\n\n1. Start with domain \u2192 Find subdomains \u2192 Resolve IPs \u2192 Port scan \u2192 Service enumeration \u2192 Contact discovery\n2. Each module reads from database tables and writes results back\n3. This creates a \"snowball effect\" where intelligence compounds\n\n**Real-world usage**: \n- **Red teams**: Automated reconnaissance for penetration testing engagements\n- **Bug bounty hunters**: Subdomain discovery and asset mapping for programs\n- **Threat intelligence**: Infrastructure correlation for APT tracking\n- **Investigative journalists**: Corporate relationship mapping\n\n### Framework #2: SpiderFoot\n\n**SpiderFoot** is an automated OSINT reconnaissance tool that integrates 200+ data sources. Unlike Recon-ng's module-based approach, SpiderFoot uses a \"spider\" methodology\u2014it crawls relationships automatically.\n\n**Architecture**:\n\n- **Web UI**: Full-featured web interface (Flask-based) for managing scans\n- **CLI**: Command-line interface for automation and integration\n- **Modules**: 200+ plugins covering DNS, social media, certificates, breach databases, dark web, etc.\n- **Correlation engine**: Automatically discovers relationships between entities\n- **Data model**: 100+ entity types (IP, domain, email, social profile, phone, etc.)\n- **Export formats**: JSON, CSV, GEXF (graph visualization)\n\n**Key capabilities**:\n\n1. **Passive reconnaissance**: DNS, WHOIS, certificate transparency, search engines\n2. **Active reconnaissance**: Port scanning, web crawling, subdomain bruteforce\n3. **Breach intelligence**: HaveIBeenPwned, Dehashed, leak databases\n4. **Social media**: Twitter, LinkedIn, Instagram, Facebook\n5. **Threat intelligence**: Shodan, Censys, GreyNoise, AbuseIPDB\n6. **Dark web**: Tor hidden services, paste sites\n7. **Phone/email**: Carrier lookups, email validation, phone number intelligence\n\n**Correlation example**: You start a scan with a domain `example.com`. SpiderFoot automatically:\n\n```\nexample.com \u2192 Subdomains (DNS)\n\u2192 WHOIS (Registrant email)\n\u2192 Certificates (Alternative names)\n\u2192 IPs (A records)\n\u2192 Geolocation\n\u2192 Shodan (Open ports)\n\u2192 BGP (ASN/ISP)\n\u2192 Social media (Company profiles)\n\u2192 Breach databases (Leaked credentials)\n\u2192 Related domains (Certificate correlation)\n```\n\nAll of this happens automatically in a single scan.\n\n**Real-world usage**:\n- **Mandiant**: Uses SpiderFoot-like systems for initial APT infrastructure mapping\n- **SOC teams**: Automated threat intelligence enrichment for IOCs\n- **Compliance teams**: External attack surface monitoring\n- **MSSP providers**: Client security posture assessments\n\n### Framework #3: TheHarvester\n\n**TheHarvester** is a simple but powerful tool for gathering emails, names, subdomains, IPs, and URLs from public sources.\n\n**Data sources** (35+ supported):\n- Search engines: Google, Bing, Yahoo, DuckDuckGo, Baidu\n- Social media: LinkedIn, Twitter\n- Certificate transparency: CertSpotter, Censys\n- DNS: DNSdumpster, HackerTarget\n- Shodan, VirusTotal, Hunter.io (with API keys)\n\n**Use cases**:\n- Email enumeration for phishing assessments (red team)\n- Subdomain discovery for bug bounty\n- Employee intelligence for social engineering\n- Quick reconnaissance for penetration testing\n\n**Philosophy**: TheHarvester is designed for **speed and simplicity**. It's not a framework\u2014it's a Swiss Army knife for quick OSINT collection.\n\n### Choosing the Right Tool\n\n| Scenario | Recommended Tool | Why |\n|----------|------------------|-----|\n| Deep investigation requiring correlation | Recon-ng | Modular, database-driven, pipeline workflow |\n| Comprehensive scan across many sources | SpiderFoot | 200+ modules, automatic correlation |\n| Quick enumeration before pentest | TheHarvester | Fast, simple, command-line friendly |\n| Custom automation requiring code | Python scripts + APIs | Maximum flexibility and control |\n| Continuous monitoring | Python + Scheduler | Reusable, maintainable, alerting |\n\n**Professional approach**: Most organizations use **all three** in different contexts:\n- TheHarvester for quick recon\n- Recon-ng for deep investigations\n- SpiderFoot for comprehensive scans\n- Custom Python for production monitoring"
      }
    },
    {
      "type": "video",
      "content": {
        "text": "## Video: Recon-ng Complete Tutorial\n\n**Video URL**: https://www.youtube.com/watch?v=Sa5LbKqCmFI\n\n**Video**: \"Recon-ng Tutorial for Beginners\" by The Cyber Mentor (Heath Adams)\n\n**Duration**: 42 minutes\n\n**Key takeaways**:\n1. **Workspace creation**: Setting up isolated environments for different targets\n2. **Module usage**: Installing and running reconnaissance modules from the marketplace\n3. **API key configuration**: Adding keys for Shodan, VirusTotal, Hunter.io, etc.\n4. **Database queries**: Extracting intelligence from Recon-ng's built-in database\n5. **Reporting**: Generating HTML reports from reconnaissance data\n6. **Real-world workflow**: Complete reconnaissance of a target domain from start to finish\n\n**Why this video**: Heath Adams (TCM) is an experienced penetration tester and OSINT practitioner. This tutorial shows real-world Recon-ng usage in penetration testing engagements, demonstrating practical workflows used by professional red teams.\n\n**Companion resources**:\n- Recon-ng GitHub: https://github.com/lanmaster53/recon-ng\n- Recon-ng Wiki: https://github.com/lanmaster53/recon-ng/wiki\n- Module marketplace: https://github.com/lanmaster53/recon-ng-marketplace"
      }
    },
    {
      "type": "explanation",
      "content": {
        "text": "## API Integration for OSINT Automation\n\nThe real power of OSINT automation comes from integrating multiple data sources through APIs. Modern OSINT platforms combine 10-50 APIs to create comprehensive intelligence pictures.\n\n### Essential OSINT APIs\n\n#### 1. Shodan API\n\n**What it is**: Search engine for internet-connected devices and services\n\n**Use cases**:\n- Finding exposed services for a target organization\n- Identifying misconfigured assets (open databases, admin panels)\n- Historical data for infrastructure changes\n- Certificate and banner analysis\n\n**API capabilities**:\n```python\nimport shodan\n\napi = shodan.Shodan('YOUR_API_KEY')\n\n# Search for hosts\nresults = api.search('org:\"Example Corp\"')\nfor result in results['matches']:\nprint(f\"{result['ip_str']} - {result['port']} - {result.get('product', 'Unknown')}\")\n\n# Get host details\nhost = api.host('8.8.8.8')\nprint(f\"Organization: {host.get('org')}\")\nprint(f\"Operating System: {host.get('os')}\")\nfor item in host['data']:\nprint(f\"Port {item['port']}: {item['product']}\")\n```\n\n**Pricing**: $59/month (query credits), free tier available (limited queries)\n\n**Rate limits**: 1 query/second (paid), 1 query/minute (free)\n\n#### 2. VirusTotal API\n\n**What it is**: File/URL/domain/IP reputation and malware analysis aggregator\n\n**Use cases**:\n- Checking domain/IP reputation\n- Historical WHOIS and DNS data\n- Malware correlation\n- Subdomain discovery via passive DNS\n\n**API capabilities**:\n```python\nimport requests\n\nAPI_KEY = 'YOUR_API_KEY'\n\n# Domain report\nurl = f'https://www.virustotal.com/api/v3/domains/example.com'\nheaders = {'x-apikey': API_KEY}\nresponse = requests.get(url, headers=headers)\ndata = response.json()\n\n# Extract subdomains from passive DNS\nsubdomains_url = f'https://www.virustotal.com/api/v3/domains/example.com/subdomains'\nsubdomains = requests.get(subdomains_url, headers=headers).json()\nfor subdomain in subdomains['data']:\nprint(subdomain['id'])\n```\n\n**Pricing**: Free tier (500 requests/day), Premium ($180+/month)\n\n**Rate limits**: 4 requests/minute (free), 1000/day premium\n\n#### 3. Hunter.io API\n\n**What it is**: Email finding and verification service\n\n**Use cases**:\n- Discovering email addresses for a domain\n- Verifying email deliverability\n- Finding email patterns for organizations\n- Building target lists for phishing assessments\n\n**API capabilities**:\n```python\nimport requests\n\nAPI_KEY = 'YOUR_API_KEY'\n\n# Domain search\nurl = f'https://api.hunter.io/v2/domain-search?domain=example.com&api_key={API_KEY}'\nresponse = requests.get(url).json()\n\nfor email in response['data']['emails']:\nprint(f\"{email['value']} - {email['type']} - Confidence: {email['confidence']}%\")\n\n# Email verification\nverify_url = f'https://api.hunter.io/v2/email-verifier?email=john@example.com&api_key={API_KEY}'\nverify = requests.get(verify_url).json()\nprint(f\"Status: {verify['data']['status']} - Score: {verify['data']['score']}/100\")\n```\n\n**Pricing**: Free (25 searches/month), $49/month (500 searches)\n\n**Rate limits**: 10 requests/second (paid)\n\n#### 4. Censys API\n\n**What it is**: Internet-wide certificate and host scanning platform (similar to Shodan)\n\n**Use cases**:\n- Certificate transparency monitoring\n- Subdomain discovery via certificates\n- Service enumeration\n- Infrastructure attribution\n\n**API capabilities**:\n```python\nimport censys.search\n\napi = censys.search.CensysHosts(api_id='YOUR_ID', api_secret='YOUR_SECRET')\n\n# Search for hosts\nfor host in api.search('services.service_name: \"HTTP\" and autonomous_system.name: \"Example\"'):\nprint(f\"{host['ip']} - {host['services']}\")\n\n# Certificate search\ncerts_api = censys.search.CensysCerts(api_id='YOUR_ID', api_secret='YOUR_SECRET')\nfor cert in certs_api.search('parsed.names: example.com'):\nprint(cert['parsed.subject.common_name'])\n```\n\n**Pricing**: Free tier (250 queries/month), $99/month (10,000 queries)\n\n#### 5. URLScan.io API\n\n**What it is**: URL analysis and website screenshot service\n\n**Use cases**:\n- Phishing website analysis\n- Website change detection\n- Screenshot capture for evidence\n- Malicious URL identification\n\n**API capabilities**:\n```python\nimport requests\nimport time\n\nAPI_KEY = 'YOUR_API_KEY'\n\n# Submit URL for scanning\nheaders = {'API-Key': API_KEY}\ndata = {'url': 'https://example.com', 'visibility': 'public'}\nsubmit = requests.post('https://urlscan.io/api/v1/scan/', headers=headers, json=data)\nscan_id = submit.json()['uuid']\n\n# Wait for scan to complete\ntime.sleep(10)\n\n# Retrieve results\nresult_url = f'https://urlscan.io/api/v1/result/{scan_id}/'\nresult = requests.get(result_url).json()\n\nprint(f\"Page title: {result['page']['title']}\")\nprint(f\"Screenshot: {result['task']['screenshotURL']}\")\nprint(f\"IPs: {result['lists']['ips']}\")\n```\n\n**Pricing**: Free tier (unlimited public scans), $149/month (private scans)\n\n### Building a Multi-API OSINT Script\n\nThe power comes from **combining** these APIs. Here's a real-world example:\n\n**Goal**: Automated reconnaissance script that:\n1. Takes a domain as input\n2. Finds subdomains (VirusTotal, Censys)\n3. Discovers IPs (DNS resolution)\n4. Scans services (Shodan)\n5. Finds emails (Hunter.io)\n6. Generates a report\n\n**Architecture**:\n```python\nimport json\nfrom typing import Dict, List\nimport shodan\nimport requests\nimport dns.resolver\n\nclass OSINTAutomation:\ndef __init__(self, shodan_key: str, vt_key: str, hunter_key: str):\nself.shodan = shodan.Shodan(shodan_key)\nself.vt_key = vt_key\nself.hunter_key = hunter_key\nself.results = {\n'domain': '',\n'subdomains': [],\n'ips': [],\n'services': [],\n'emails': []\n}\n\ndef find_subdomains_vt(self, domain: str) -> List[str]:\n\"\"\"Find subdomains using VirusTotal passive DNS\"\"\"\nurl = f'https://www.virustotal.com/api/v3/domains/{domain}/subdomains'\nheaders = {'x-apikey': self.vt_key}\nresponse = requests.get(url, headers=headers)\n\nif response.status_code == 200:\ndata = response.json()\nreturn [item['id'] for item in data.get('data', [])]\nreturn []\n\ndef resolve_ips(self, domains: List[str]) -> Dict[str, List[str]]:\n\"\"\"Resolve domains to IPs\"\"\"\nip_map = {}\nfor domain in domains:\ntry:\nanswers = dns.resolver.resolve(domain, 'A')\nip_map[domain] = [str(rdata) for rdata in answers]\nexcept Exception as e:\nip_map[domain] = []\nreturn ip_map\n\ndef scan_services_shodan(self, ips: List[str]) -> List[Dict]:\n\"\"\"Scan services using Shodan\"\"\"\nservices = []\nfor ip in ips:\ntry:\nhost = self.shodan.host(ip)\nservices.append({\n'ip': ip,\n'org': host.get('org'),\n'os': host.get('os'),\n'ports': [item['port'] for item in host['data']]\n})\nexcept shodan.APIError:\npass\nreturn services\n\ndef find_emails_hunter(self, domain: str) -> List[Dict]:\n\"\"\"Find emails using Hunter.io\"\"\"\nurl = f'https://api.hunter.io/v2/domain-search?domain={domain}&api_key={self.hunter_key}'\nresponse = requests.get(url)\n\nif response.status_code == 200:\ndata = response.json()\nreturn [{\n'email': email['value'],\n'confidence': email['confidence']\n} for email in data.get('data', {}).get('emails', [])]\nreturn []\n\ndef run(self, domain: str) -> Dict:\n\"\"\"Run complete OSINT automation\"\"\"\nprint(f\"[*] Starting OSINT automation for {domain}\")\nself.results['domain'] = domain\n\n# Step 1: Subdomain discovery\nprint(\"[*] Finding subdomains...\")\nself.results['subdomains'] = self.find_subdomains_vt(domain)\nprint(f\"[+] Found {len(self.results['subdomains'])} subdomains\")\n\n# Step 2: DNS resolution\nprint(\"[*] Resolving IPs...\")\nip_map = self.resolve_ips([domain] + self.results['subdomains'])\nself.results['ips'] = list(set([ip for ips in ip_map.values() for ip in ips]))\nprint(f\"[+] Found {len(self.results['ips'])} unique IPs\")\n\n# Step 3: Service scanning\nprint(\"[*] Scanning services...\")\nself.results['services'] = self.scan_services_shodan(self.results['ips'])\nprint(f\"[+] Scanned {len(self.results['services'])} hosts\")\n\n# Step 4: Email discovery\nprint(\"[*] Finding emails...\")\nself.results['emails'] = self.find_emails_hunter(domain)\nprint(f\"[+] Found {len(self.results['emails'])} emails\")\n\nreturn self.results\n\n# Usage\nif __name__ == '__main__':\nosint = OSINTAutomation(\nshodan_key='YOUR_SHODAN_KEY',\nvt_key='YOUR_VT_KEY',\nhunter_key='YOUR_HUNTER_KEY'\n)\n\nresults = osint.run('example.com')\n\n# Save to JSON\nwith open('osint_results.json', 'w') as f:\njson.dump(results, f, indent=2)\n\nprint(\"\\n[+] Results saved to osint_results.json\")\n```\n\nThis script demonstrates the **compounding effect** of API integration: subdomains \u2192 IPs \u2192 services \u2192 emails. Each API builds on the previous one's output."
      }
    },
    {
      "type": "code_exercise",
      "content": {
        "text": "## Hands-On Lab: Build Your OSINT Automation Pipeline\n\nIn this lab, you'll build a complete OSINT automation pipeline using Recon-ng, SpiderFoot, and custom Python scripts. This represents a professional-grade workflow used by security operations centers and threat intelligence teams.\n\n### Lab Setup\n\n**Prerequisites**:\n- Kali Linux or Ubuntu VM\n- Python 3.8+\n- Internet connectivity\n- API keys (free tier): Shodan, VirusTotal, Hunter.io (optional but recommended)\n\n**Installation**:\n```bash\n# Install Recon-ng\nsudo apt update\nsudo apt install recon-ng -y\n\n# Install SpiderFoot\nsudo apt install python3-pip git -y\ngit clone https://github.com/smicallef/spiderfoot.git\ncd spiderfoot\npip3 install -r requirements.txt\n\n# Install TheHarvester\nsudo apt install theharvester -y\n\n# Install Python dependencies\npip3 install shodan dnspython requests\n```\n\n---\n\n### Exercise 1: Recon-ng Automated Reconnaissance\n\n**Goal**: Use Recon-ng to perform automated subdomain enumeration, contact discovery, and reporting for a target domain.\n\n**Target**: Choose one of:\n- `tesla.com` (large attack surface, good for testing)\n- `bugcrowd.com` (bug bounty platform, expects scanning)\n- Your own domain (if practicing professionally)\n\n**Step 1: Workspace Setup**\n```bash\n# Start Recon-ng\nrecon-ng\n\n# Create workspace\n[recon-ng][default] > workspaces create tesla_recon\n[recon-ng][tesla_recon] > \n```\n\n**Step 2: Install Required Modules**\n```bash\n# Search for modules\nmarketplace search\n\n# Install subdomain enumeration modules\nmarketplace install recon/domains-hosts/hackertarget\nmarketplace install recon/domains-hosts/threatcrowd\nmarketplace install recon/domains-hosts/certificate_transparency\n\n# Install contact discovery\nmarketplace install recon/domains-contacts/hunter_io\nmarketplace install recon/domains-contacts/whois_pocs\n\n# Install reporting\nmarketplace install reporting/html\n```\n\n**Step 3: Configure API Keys (Optional)**\n```bash\n# Add Shodan API key\nkeys add shodan_api YOUR_SHODAN_KEY\n\n# Add VirusTotal API key\nkeys add virustotal_api YOUR_VT_KEY\n\n# Add Hunter.io API key\nkeys add hunter_io_api YOUR_HUNTER_KEY\n\n# Verify keys\nkeys list\n```\n\n**Step 4: Add Target Domain**\n```bash\n# Add domain to database\ndb insert domains domain=tesla.com\n\n# Verify insertion\ndb query SELECT * FROM domains\n```\n\n**Step 5: Run Reconnaissance Modules**\n```bash\n# Load and run subdomain enumeration\nmodules load recon/domains-hosts/hackertarget\nrun\n\n# Run certificate transparency\nmodules load recon/domains-hosts/certificate_transparency\nrun\n\n# Run ThreatCrowd\nmodules load recon/domains-hosts/threatcrowd\nrun\n\n# View discovered hosts\nshow hosts\n```\n\n**Expected Output**:\n```\n+-----------+-------------------+\n| host      | ip_address        |\n+-----------+-------------------+\n| tesla.com | 199.66.11.62      |\n| www.tesla.com | 199.66.11.62  |\n| shop.tesla.com | 104.18.42.145 |\n| [... 50-200 more subdomains]  |\n+-----------+-------------------+\n```\n\n**Step 6: Contact Discovery**\n```bash\n# Find contacts from WHOIS\nmodules load recon/domains-contacts/whois_pocs\nrun\n\n# View contacts\nshow contacts\n```\n\n**Step 7: Generate Report**\n```bash\n# Generate HTML report\nmodules load reporting/html\nrun\n\n# Report saved to: ~/.recon-ng/workspaces/tesla_recon/results.html\n```\n\n**View the report**:\n```bash\nfirefox ~/.recon-ng/workspaces/tesla_recon/results.html\n```\n\n**Challenge**: Automate this entire workflow using Recon-ng's CLI mode:\n```bash\nrecon-ng -w tesla_recon -m recon/domains-hosts/hackertarget -c \"run\"\n```\n\n---\n\n### Exercise 2: SpiderFoot Comprehensive Scan\n\n**Goal**: Run a SpiderFoot scan correlating 100+ OSINT data points across multiple sources.\n\n**Step 1: Start SpiderFoot**\n```bash\ncd ~/spiderfoot\npython3 sf.py -l 127.0.0.1:5001\n```\n\n**Step 2: Access Web Interface**\n- Open browser: `http://127.0.0.1:5001`\n- Click \"New Scan\"\n\n**Step 3: Configure Scan**\n\n**Scan settings**:\n- **Scan Name**: `Tesla OSINT Scan`\n- **Target**: `tesla.com`\n- **Use Case**: Select \"Footprint\" (comprehensive)\n- **Modules**: Leave all enabled (200+ modules)\n\n**Step 4: Start Scan**\n- Click \"Run Scan\"\n- Monitor progress in real-time\n- Typical scan time: 10-30 minutes depending on target size\n\n**Step 5: Analyze Results**\n\nOnce complete, explore:\n- **Graph View**: Visual relationship map\n- **Browse**: All discovered entities\n- **Export**: Download JSON/CSV for analysis\n\n**Expected findings**:\n```\n- 150+ subdomains discovered\n- 50+ IP addresses\n- 20+ email addresses\n- SSL certificate details\n- Shodan open ports\n- Social media profiles\n- Breach database entries (if any)\n- Dark web mentions (if any)\n```\n\n**Step 6: Export Data**\n- Click \"Export\" \u2192 \"JSON\"\n- Save to `tesla_spiderfoot.json`\n\n**CLI Alternative** (for automation):\n```bash\npython3 sf.py -s tesla.com -t DOMAIN -o json > tesla_scan.json\n```\n\n---\n\n### Exercise 3: Build Custom Python OSINT Script\n\n**Goal**: Create a custom Python script integrating Shodan, VirusTotal, and Hunter.io APIs for automated reconnaissance.\n\n**Script**: `osint_recon.py`\n\n```python\n# !/usr/bin/env python3\n\"\"\"\nOSINT Automation Script\nIntegrates: Shodan, VirusTotal, Hunter.io\nAuthor: Your Name\n\"\"\"\n\nimport json\nimport sys\nimport time\nfrom typing import Dict, List\nimport requests\nimport shodan\n\nclass OSINTRecon:\ndef __init__(self, config_file: str):\n\"\"\"Initialize with API keys from config file\"\"\"\nwith open(config_file, 'r') as f:\nconfig = json.load(f)\n\nself.shodan_api = shodan.Shodan(config['shodan_key'])\nself.vt_key = config['virustotal_key']\nself.hunter_key = config['hunter_key']\n\nself.results = {\n'domain': '',\n'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n'subdomains': [],\n'ips': [],\n'shodan_results': [],\n'emails': [],\n'breaches': []\n}\n\ndef find_subdomains_vt(self, domain: str, limit: int = 100) -> List[str]:\n\"\"\"Find subdomains using VirusTotal passive DNS\"\"\"\nprint(f\"[*] Searching VirusTotal for subdomains of {domain}...\")\n\nurl = f'https://www.virustotal.com/api/v3/domains/{domain}/subdomains?limit={limit}'\nheaders = {'x-apikey': self.vt_key}\n\ntry:\nresponse = requests.get(url, headers=headers, timeout=30)\nif response.status_code == 200:\ndata = response.json()\nsubdomains = [item['id'] for item in data.get('data', [])]\nprint(f\"[+] Found {len(subdomains)} subdomains\")\nreturn subdomains\nelse:\nprint(f\"[-] VirusTotal API error: {response.status_code}\")\nreturn []\nexcept Exception as e:\nprint(f\"[-] Error: {e}\")\nreturn []\n\ndef get_domain_ips(self, domain: str) -> List[str]:\n\"\"\"Get IPs for domain using VirusTotal\"\"\"\nprint(f\"[*] Resolving IPs for {domain}...\")\n\nurl = f'https://www.virustotal.com/api/v3/domains/{domain}/resolutions'\nheaders = {'x-apikey': self.vt_key}\n\ntry:\nresponse = requests.get(url, headers=headers, timeout=30)\nif response.status_code == 200:\ndata = response.json()\nips = [item['attributes']['ip_address'] for item in data.get('data', [])[:10]]\nprint(f\"[+] Found {len(ips)} IPs\")\nreturn ips\nreturn []\nexcept Exception as e:\nprint(f\"[-] Error: {e}\")\nreturn []\n\ndef scan_shodan(self, ips: List[str]) -> List[Dict]:\n\"\"\"Scan IPs using Shodan\"\"\"\nprint(f\"[*] Scanning {len(ips)} IPs with Shodan...\")\n\nresults = []\nfor ip in ips:\ntry:\nhost = self.shodan_api.host(ip)\nresults.append({\n'ip': ip,\n'organization': host.get('org', 'Unknown'),\n'os': host.get('os', 'Unknown'),\n'ports': [item['port'] for item in host.get('data', [])],\n'vulns': host.get('vulns', [])\n})\nprint(f\"[+] {ip}: {len(host.get('data', []))} services found\")\ntime.sleep(1)  # Rate limiting\nexcept shodan.APIError as e:\nprint(f\"[-] Shodan error for {ip}: {e}\")\ncontinue\n\nreturn results\n\ndef find_emails_hunter(self, domain: str) -> List[Dict]:\n\"\"\"Find emails using Hunter.io\"\"\"\nprint(f\"[*] Searching for emails at {domain}...\")\n\nurl = f'https://api.hunter.io/v2/domain-search?domain={domain}&api_key={self.hunter_key}'\n\ntry:\nresponse = requests.get(url, timeout=30)\nif response.status_code == 200:\ndata = response.json()\nemails = [{\n'email': email['value'],\n'first_name': email.get('first_name', ''),\n'last_name': email.get('last_name', ''),\n'position': email.get('position', ''),\n'confidence': email.get('confidence', 0)\n} for email in data.get('data', {}).get('emails', [])]\nprint(f\"[+] Found {len(emails)} emails\")\nreturn emails\nelse:\nprint(f\"[-] Hunter.io API error: {response.status_code}\")\nreturn []\nexcept Exception as e:\nprint(f\"[-] Error: {e}\")\nreturn []\n\ndef check_breaches_hibp(self, domain: str) -> List[str]:\n\"\"\"Check for breaches using HaveIBeenPwned (v3 API requires payment now)\"\"\"\n# Note: HIBP API v3 requires paid API key\nprint(f\"[*] Checking breach databases for {domain}...\")\nprint(\"[!] Note: HIBP API requires paid subscription for domain search\")\nreturn []\n\ndef run(self, domain: str) -> Dict:\n\"\"\"Run complete OSINT reconnaissance\"\"\"\nprint(f\"\\n{'='*60}\")\nprint(f\"OSINT Reconnaissance: {domain}\")\nprint(f\"{'='*60}\\n\")\n\nself.results['domain'] = domain\n\n# Step 1: Subdomain enumeration\nself.results['subdomains'] = self.find_subdomains_vt(domain)\ntime.sleep(2)\n\n# Step 2: IP resolution\nself.results['ips'] = self.get_domain_ips(domain)\ntime.sleep(2)\n\n# Step 3: Shodan scanning\nif self.results['ips']:\nself.results['shodan_results'] = self.scan_shodan(self.results['ips'][:5])  # Limit to 5 IPs\ntime.sleep(2)\n\n# Step 4: Email discovery\nself.results['emails'] = self.find_emails_hunter(domain)\n\n# Step 5: Breach checking\nself.results['breaches'] = self.check_breaches_hibp(domain)\n\nreturn self.results\n\ndef save_report(self, filename: str):\n\"\"\"Save results to JSON file\"\"\"\nwith open(filename, 'w') as f:\njson.dump(self.results, f, indent=2)\nprint(f\"\\n[+] Report saved to: {filename}\")\n\ndef print_summary(self):\n\"\"\"Print summary of findings\"\"\"\nprint(f\"\\n{'='*60}\")\nprint(\"OSINT Reconnaissance Summary\")\nprint(f\"{'='*60}\")\nprint(f\"Domain: {self.results['domain']}\")\nprint(f\"Timestamp: {self.results['timestamp']}\")\nprint(f\"\\nFindings:\")\nprint(f\"  - Subdomains: {len(self.results['subdomains'])}\")\nprint(f\"  - IP Addresses: {len(self.results['ips'])}\")\nprint(f\"  - Shodan Results: {len(self.results['shodan_results'])}\")\nprint(f\"  - Email Addresses: {len(self.results['emails'])}\")\nprint(f\"\\nTop 5 Subdomains:\")\nfor subdomain in self.results['subdomains'][:5]:\nprint(f\"  - {subdomain}\")\nprint(f\"\\nTop 5 Emails:\")\nfor email in self.results['emails'][:5]:\nprint(f\"  - {email['email']} (Confidence: {email['confidence']}%)\")\nprint(f\"{'='*60}\\n\")\n\ndef main():\nif len(sys.argv) != 3:\nprint(\"Usage: python3 osint_recon.py <config.json> <domain>\")\nprint(\"Example: python3 osint_recon.py config.json example.com\")\nsys.exit(1)\n\nconfig_file = sys.argv[1]\ndomain = sys.argv[2]\n\n# Run reconnaissance\nosint = OSINTRecon(config_file)\nosint.run(domain)\n\n# Print summary\nosint.print_summary()\n\n# Save report\nreport_file = f\"{domain.replace('.', '_')}_osint_report.json\"\nosint.save_report(report_file)\n\nif __name__ == '__main__':\nmain()\n```\n\n**Configuration File**: `config.json`\n```json\n{\n\"shodan_key\": \"YOUR_SHODAN_API_KEY\",\n\"virustotal_key\": \"YOUR_VIRUSTOTAL_API_KEY\",\n\"hunter_key\": \"YOUR_HUNTER_API_KEY\"\n}\n```\n\n**Run the script**:\n```bash\nchmod +x osint_recon.py\npython3 osint_recon.py config.json tesla.com\n```\n\n**Expected output**:\n```\n============================================================\nOSINT Reconnaissance: tesla.com\n============================================================\n\n[*] Searching VirusTotal for subdomains of tesla.com...\n[+] Found 87 subdomains\n[*] Resolving IPs for tesla.com...\n[+] Found 8 IPs\n[*] Scanning 5 IPs with Shodan...\n[+] 199.66.11.62: 3 services found\n[+] 104.18.42.145: 2 services found\n[*] Searching for emails at tesla.com...\n[+] Found 12 emails\n\n============================================================\nOSINT Reconnaissance Summary\n============================================================\nDomain: tesla.com\nTimestamp: 2025-10-29 14:32:15\n\nFindings:\n- Subdomains: 87\n- IP Addresses: 8\n- Shodan Results: 5\n- Email Addresses: 12\n\nTop 5 Subdomains:\n- www.tesla.com\n- shop.tesla.com\n- service.tesla.com\n- my.tesla.com\n- auth.tesla.com\n\nTop 5 Emails:\n- press@tesla.com (Confidence: 92%)\n- ir@tesla.com (Confidence: 91%)\n- service@tesla.com (Confidence: 89%)\n[...]\n============================================================\n\n[+] Report saved to: tesla_com_osint_report.json\n```\n\n---\n\n### Exercise 4: Continuous Monitoring Pipeline\n\n**Goal**: Set up automated OSINT monitoring that runs daily and alerts on new findings.\n\n**Step 1: Create Monitoring Script**\n\n**File**: `osint_monitor.py`\n```python\n# !/usr/bin/env python3\n\"\"\"\nContinuous OSINT Monitoring\nRuns daily scans and alerts on changes\n\"\"\"\n\nimport json\nimport os\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom datetime import datetime\nimport subprocess\n\nclass OSINTMonitor:\ndef __init__(self, config_file: str):\nwith open(config_file, 'r') as f:\nself.config = json.load(f)\n\nself.history_dir = 'osint_history'\nos.makedirs(self.history_dir, exist_ok=True)\n\ndef run_scan(self, domain: str) -> dict:\n\"\"\"Run OSINT reconnaissance\"\"\"\nprint(f\"[*] Running scan for {domain}...\")\n\n# Call osint_recon.py\ncmd = ['python3', 'osint_recon.py', 'config.json', domain]\nsubprocess.run(cmd, check=True)\n\n# Load results\nreport_file = f\"{domain.replace('.', '_')}_osint_report.json\"\nwith open(report_file, 'r') as f:\nreturn json.load(f)\n\ndef load_previous_scan(self, domain: str) -> dict:\n\"\"\"Load previous scan results\"\"\"\nhistory_file = os.path.join(self.history_dir, f\"{domain}.json\")\nif os.path.exists(history_file):\nwith open(history_file, 'r') as f:\nreturn json.load(f)\nreturn None\n\ndef save_scan(self, domain: str, results: dict):\n\"\"\"Save scan to history\"\"\"\nhistory_file = os.path.join(self.history_dir, f\"{domain}.json\")\nwith open(history_file, 'w') as f:\njson.dump(results, f, indent=2)\n\ndef detect_changes(self, old: dict, new: dict) -> dict:\n\"\"\"Detect changes between scans\"\"\"\nchanges = {\n'new_subdomains': [],\n'new_ips': [],\n'new_emails': [],\n'removed_subdomains': [],\n'removed_ips': []\n}\n\nif old is None:\nreturn changes\n\n# New subdomains\nold_subs = set(old.get('subdomains', []))\nnew_subs = set(new.get('subdomains', []))\nchanges['new_subdomains'] = list(new_subs - old_subs)\nchanges['removed_subdomains'] = list(old_subs - new_subs)\n\n# New IPs\nold_ips = set(old.get('ips', []))\nnew_ips = set(new.get('ips', []))\nchanges['new_ips'] = list(new_ips - old_ips)\nchanges['removed_ips'] = list(old_ips - new_ips)\n\n# New emails\nold_emails = set([e['email'] for e in old.get('emails', [])])\nnew_emails = set([e['email'] for e in new.get('emails', [])])\nchanges['new_emails'] = list(new_emails - old_emails)\n\nreturn changes\n\ndef send_alert(self, domain: str, changes: dict):\n\"\"\"Send email alert for changes\"\"\"\nif not any(changes.values()):\nprint(\"[*] No changes detected\")\nreturn\n\nprint(f\"[!] Changes detected for {domain}:\")\nprint(f\"  - New subdomains: {len(changes['new_subdomains'])}\")\nprint(f\"  - New IPs: {len(changes['new_ips'])}\")\nprint(f\"  - New emails: {len(changes['new_emails'])}\")\n\n# Email alert (configure SMTP settings)\nsubject = f\"OSINT Alert: Changes detected for {domain}\"\nbody = f\"\"\"\nOSINT Monitoring Alert\nDomain: {domain}\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nChanges Detected:\n- New subdomains: {len(changes['new_subdomains'])}\n- New IPs: {len(changes['new_ips'])}\n- New emails: {len(changes['new_emails'])}\n\nNew Subdomains:\n{chr(10).join(changes['new_subdomains'][:10])}\n\nNew IPs:\n{chr(10).join(changes['new_ips'])}\n\nNew Emails:\n{chr(10).join(changes['new_emails'])}\n\"\"\"\n\nprint(body)\n# Uncomment to enable email alerts\n# self._send_email(subject, body)\n\ndef monitor(self, domain: str):\n\"\"\"Run monitoring for domain\"\"\"\n# Run new scan\nnew_results = self.run_scan(domain)\n\n# Load previous scan\nold_results = self.load_previous_scan(domain)\n\n# Detect changes\nchanges = self.detect_changes(old_results, new_results)\n\n# Alert if changes\nself.send_alert(domain, changes)\n\n# Save new results\nself.save_scan(domain, new_results)\n\nif __name__ == '__main__':\nmonitor = OSINTMonitor('config.json')\nmonitor.monitor('example.com')  # Replace with your target\n```\n\n**Step 2: Schedule with Cron**\n```bash\n# Edit crontab\ncrontab -e\n\n# Add daily scan at 2 AM\n0 2 * * * cd /path/to/osint && python3 osint_monitor.py >> osint_monitor.log 2>&1\n```\n\n**Expected workflow**:\n1. Script runs daily at 2 AM\n2. Performs full OSINT scan\n3. Compares with previous day's results\n4. Sends alert email if changes detected\n5. Logs all activity\n\n---\n\n### Lab Summary\n\nYou've now built:\n1. \u2705 Recon-ng automated reconnaissance workflow\n2. \u2705 SpiderFoot comprehensive scanning\n3. \u2705 Custom Python multi-API OSINT script\n4. \u2705 Continuous monitoring pipeline with change detection\n\nThese four components form a **complete OSINT automation stack** used by professional security teams."
      }
    },
    {
      "type": "real_world",
      "content": {
        "text": "## Real-World Case Studies: OSINT Automation in Action\n\n### Case Study 1: Mandiant APT Infrastructure Tracking\n\n**Organization**: Mandiant (acquired by Google Cloud for $5.4B in 2022)\n\n**Challenge**: Track evolving infrastructure of Advanced Persistent Threat (APT) groups across 100+ campaigns simultaneously. APT groups constantly rotate domains, IPs, and C2 servers, making manual tracking impossible.\n\n**OSINT Automation Solution**:\n\nMandiant developed an automated threat intelligence platform integrating:\n- **Certificate transparency monitoring**: Automatic detection of newly issued SSL certificates matching APT naming patterns\n- **Passive DNS correlation**: Historical DNS data from multiple providers (Farsight, VirusTotal, RiskIQ)\n- **WHOIS registration monitoring**: Automated alerts for registrar patterns used by APT groups\n- **Malware infrastructure pivoting**: Automated C2 server discovery from malware samples\n- **Social media monitoring**: Twitter/GitHub/Pastebin scraping for leaked IOCs\n\n**Technical Implementation**:\n```\nAutomation Pipeline:\n\n1. Certificate Transparency Feeds\n\u2193 (Pattern matching: APT29 naming conventions)\n2. Automated Pivoting\n\u2193 (Resolve IPs, check Shodan, correlate with known infrastructure)\n3. Machine Learning Classification\n\u2193 (Probability scoring: 85% match to APT29 infrastructure)\n4. Analyst Review Queue\n\u2193 (Human verification of high-confidence matches)\n5. Threat Intelligence Feed Update\n\u2193 (Push IOCs to customer SIEM/EDR platforms)\n```\n\n**Results**:\n- **3,200+ APT domains** tracked automatically per month\n- **92% reduction** in manual reconnaissance time\n- **Detection time improvement**: From 60 days (manual) to 4 days (automated)\n- **False positive rate**: 8% (acceptable for threat intelligence)\n\n**Key lesson**: Automation enables **continuous** rather than **point-in-time** intelligence. Mandiant's platform monitors 24/7, something impossible with manual OSINT.\n\n---\n\n### Case Study 2: Bug Bounty Hunter Automation (Jason Haddix)\n\n**Individual**: Jason Haddix, Director of Technical Operations at BugsPoC (formerly Head of Trust & Security at Bugcrowd)\n\n**Challenge**: Competing in bug bounty programs requires discovering assets faster than other researchers. Programs like Verizon Media (1,000+ domains) or Tesla (500+ subdomains) have massive attack surfaces.\n\n**OSINT Automation Solution**:\n\nJason developed a custom reconnaissance pipeline combining:\n- **Subdomain enumeration**: Amass, Subfinder, Assetfinder (running in parallel)\n- **DNS resolution**: MassDNS for high-speed validation\n- **Port scanning**: Masscan + Nmap for service enumeration\n- **Web probing**: EyeWitness for screenshots, httprobe for live hosts\n- **Content discovery**: ffuf, gobuster for hidden endpoints\n- **Continuous monitoring**: Daily scans with diff detection\n\n**Technical Stack**:\n```bash\n# !/bin/bash\n# Jason's reconnaissance pipeline (simplified version)\n\nDOMAIN=$1\n\n# Step 1: Subdomain enumeration (parallel)\namass enum -passive -d $DOMAIN -o amass.txt &\nsubfinder -d $DOMAIN -o subfinder.txt &\nassetfinder --subs-only $DOMAIN > assetfinder.txt &\nwait\n\n# Step 2: Merge and deduplicate\ncat amass.txt subfinder.txt assetfinder.txt | sort -u > all_subdomains.txt\n\n# Step 3: DNS resolution\nmassdns -r resolvers.txt -t A all_subdomains.txt -o S -w resolved.txt\n\n# Step 4: Port scanning\nmasscan -iL resolved.txt -p1-65535 --rate=10000 -oL masscan.txt\n\n# Step 5: Web probing\ncat resolved.txt | httprobe > live_hosts.txt\n\n# Step 6: Screenshots\npython3 EyeWitness.py -f live_hosts.txt --web\n\n# Step 7: Content discovery\ncat live_hosts.txt | ffuf -w wordlist.txt -u FUZZ\n\necho \"[+] Recon complete. Results in ./output/\"\n```\n\n**Automation Impact**:\n- **Subdomain discovery**: 500-2,000 subdomains per program (vs 50-100 manual)\n- **Time savings**: 15 minutes automated vs 8-12 hours manual\n- **Earnings increase**: $250K+ annually from bug bounties (partly attributed to automation speed advantage)\n- **Continuous monitoring**: Detects new assets within 24 hours of deployment\n\n**Key lesson**: Speed matters in competitive environments. Automation provides **first-mover advantage** in discovering new attack surface.\n\n---\n\n### Case Study 3: SpiderFoot for Corporate Attack Surface Management (Cisco)\n\n**Organization**: Cisco Systems (Fortune 100, 80,000+ employees)\n\n**Challenge**: Cisco's external attack surface spans 10,000+ domains (subsidiaries, acquisitions, legacy properties). Security team needs continuous visibility into exposed assets, misconfigurations, and third-party risks.\n\n**OSINT Automation Solution**:\n\nCisco deployed SpiderFoot Enterprise (commercial version) integrated with their SIEM:\n\n**Architecture**:\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SpiderFoot Scanning Cluster (5 nodes)       \u2502\n\u2502  - 10,000 domains monitored                   \u2502\n\u2502  - Daily scans (overnight)                    \u2502\n\u2502  - 200+ OSINT modules enabled                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Data Normalization & Enrichment              \u2502\n\u2502  - Asset tagging (business unit, criticality) \u2502\n\u2502  - Risk scoring (CVSS for exposed services)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Splunk SIEM Integration                      \u2502\n\u2502  - High-risk findings \u2192 SOC alerts            \u2502\n\u2502  - Dashboards for executives                  \u2502\n\u2502  - Compliance reporting (PCI-DSS, SOC 2)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Automated Detections**:\n1. **Exposed admin panels**: Detects /admin, /wp-admin, /login pages\n2. **Misconfigured cloud storage**: S3 buckets, Azure blobs with public access\n3. **SSL/TLS issues**: Expired certificates, weak ciphers, certificate mismatch\n4. **Data leaks**: Paste sites, GitHub repositories with credentials\n5. **Shadow IT**: Employee-created cloud services not tracked by IT\n6. **Third-party risks**: Vendors with poor security posture\n\n**Results** (first 12 months):\n- **487 critical findings** discovered and remediated\n- **62 data leaks** detected (employee credentials on paste sites)\n- **1,200+ SSL certificates** renewed before expiration\n- **94% reduction** in external attack surface assessment time\n- **$2.3M estimated savings** from prevented breaches (based on IBM breach cost data)\n\n**Key lesson**: Automation scales visibility. Cisco's security team (25 people) monitors 10,000 domains\u2014impossible without automation.\n\n---\n\n### Case Study 4: TheHarvester for Red Team Reconnaissance (US Department of Defense)\n\n**Organization**: US Department of Defense (Cybersecurity & Infrastructure Security Agency - CISA)\n\n**Challenge**: Before conducting authorized penetration testing of government networks, red teams must perform reconnaissance to understand attack surface. Traditional methods are time-consuming and inconsistent.\n\n**OSINT Automation Solution**:\n\nCISA red teams standardized on TheHarvester for initial reconnaissance:\n\n**Standard Operating Procedure**:\n```bash\n# Phase 1: Email enumeration\ntheHarvester -d target.mil -b all -l 500 -f emails_report\n\n# Phase 2: Subdomain discovery\ntheHarvester -d target.mil -b all -l 500 -f subdomains_report\n\n# Phase 3: Combine with Shodan\ncat subdomains_report.json | jq -r '.hosts[]' | while read host; do\nshodan host $host >> shodan_results.txt\ndone\n\n# Phase 4: Generate target list for exploitation\npython3 parse_results.py --output targets.txt\n```\n\n**Integration with Red Team Workflow**:\n1. **Week 1**: Automated OSINT (TheHarvester, Shodan, Recon-ng)\n2. **Week 2**: Manual analysis and target prioritization\n3. **Week 3-4**: Active exploitation based on OSINT findings\n4. **Week 5**: Reporting with full OSINT evidence\n\n**Results**:\n- **3,500+ federal networks** assessed using standardized OSINT automation\n- **Average 45 minutes** for complete reconnaissance (vs 4-6 hours manual)\n- **Consistency**: All red teams use same methodology, improving report comparability\n- **Evidence quality**: Automated tools provide timestamped, reproducible results\n\n**Key lesson**: Automation enables **standardization**. Critical for large organizations needing consistent, repeatable processes.\n\n---\n\n### Case Study 5: Continuous OSINT Monitoring (Recorded Future)\n\n**Organization**: Recorded Future (threat intelligence SaaS, 1,800+ customers)\n\n**Challenge**: Provide real-time threat intelligence to customers across 50+ industries. Manual OSINT cannot scale to monitor millions of entities (domains, IPs, email addresses, executives) continuously.\n\n**OSINT Automation Solution**:\n\nRecorded Future built a **fully automated** threat intelligence platform:\n\n**Data Collection**:\n- **1 billion+ events processed daily** from 300+ sources\n- Sources: Dark web forums, paste sites, code repositories, social media, blogs, technical sources (DNS, WHOIS, certificates)\n- **NLP and ML**: Automatic entity extraction, relationship mapping, risk scoring\n\n**Automation Architecture**:\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Data Collection Layer (300+ sources)           \u2502\n\u2502  - Web crawlers (100k pages/day)                \u2502\n\u2502  - API integrations (Shodan, VirusTotal, etc.)  \u2502\n\u2502  - Dark web monitoring (Tor hidden services)    \u2502\n\u2502  - Paste site monitors (Pastebin, Ghostbin)     \u2502\n\u2502  - Social media streams (Twitter, Telegram)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  NLP & Machine Learning Layer                   \u2502\n\u2502  - Entity extraction (IOCs, threat actors)      \u2502\n\u2502  - Sentiment analysis (threat severity)         \u2502\n\u2502  - Relationship mapping (infrastructure links)  \u2502\n\u2502  - Risk scoring (0-100 scale)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Customer Alerting & Integration                \u2502\n\u2502  - SIEM integration (Splunk, QRadar)            \u2502\n\u2502  - SOAR automation (Phantom, Demisto)           \u2502\n\u2502  - Email/Slack alerts                           \u2502\n\u2502  - API for custom integrations                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Customer Use Case** (Fortune 500 Financial Institution):\n\nBank monitors:\n- **500 executive names** for doxxing/threats\n- **2,000 domains** (company + subsidiaries)\n- **50,000 employee emails** for breach exposure\n- **200 brand keywords** for phishing/fraud\n\n**Automated Alerts** (examples from 1 month):\n- **42 executive mentions** in dark web forums (investigated, 3 credible threats identified)\n- **127 phishing domains** registered (takedown requests filed)\n- **1,847 employee credentials** found in breaches (password resets forced)\n- **5 data leaks** on paste sites (removed, source identified)\n\n**Results**:\n- **$50M+ prevented fraud** (2024, based on blocked phishing campaigns)\n- **Average 6-hour detection time** for new threats (vs industry average 207 days)\n- **98% automation rate** (only 2% of alerts require human analysis)\n\n**Key lesson**: Machine learning amplifies OSINT automation. Pattern recognition at scale (1B events/day) is impossible for humans.\n\n---\n\n### Common Patterns Across All Case Studies\n\n**What makes OSINT automation successful**:\n\n1. **Multi-source integration**: All cases use 10+ data sources\n2. **Continuous monitoring**: Not one-time scans, but 24/7 surveillance\n3. **Machine learning**: Automated classification and risk scoring\n4. **Human-in-the-loop**: High-confidence alerts go to analysts, not full automation\n5. **Standardization**: Repeatable processes with consistent output\n6. **API-first design**: Everything programmatically accessible\n7. **Actionable output**: Alerts lead to specific remediation steps\n\n**ROI Metrics**:\n- Time savings: **80-95%** reduction vs manual OSINT\n- Detection speed: **10-50x faster** than manual methods\n- Scale: Monitor **100-1000x more entities** than manual capacity\n- Cost: **$50-500K annual savings** per organization (based on prevented breaches + time savings)"
      }
    },
    {
      "type": "memory_aid",
      "content": {
        "text": "## Memory Aids: OSINT Automation Mastery\n\n### RAPID Framework (Automation Workflow)\n\nUse **RAPID** to remember the five phases of OSINT automation:\n\n**R** - **Reconnaissance** (Recon-ng, TheHarvester)\n- Initial data collection\n- Subdomain enumeration\n- Email discovery\n- Contact identification\n\n**A** - **APIs** (Shodan, VirusTotal, Hunter.io)\n- Integrate external data sources\n- Enrich findings with threat intelligence\n- Cross-reference multiple databases\n- Validate discovered assets\n\n**P** - **Processing** (Python scripts, data normalization)\n- Clean and deduplicate data\n- Correlate relationships\n- Score and prioritize findings\n- Filter false positives\n\n**I** - **Integration** (SIEM, SOAR, dashboards)\n- Connect to security operations\n- Automate alerting\n- Feed downstream systems\n- Enable analyst workflows\n\n**D** - **Detection** (Continuous monitoring, change detection)\n- Schedule recurring scans\n- Compare with baseline\n- Alert on anomalies\n- Track over time\n\n**Memory hook**: \"RAPID OSINT gets you intelligence fast and continuously.\"\n\n---\n\n### Tool Selection Matrix (ARMS)\n\nUse **ARMS** to choose the right OSINT automation tool:\n\n**A** - **Amass/Automation** \u2192 For comprehensive subdomain enumeration\n- Use when: Bug bounty, penetration testing, complete asset discovery\n- Strength: 50+ data sources, graph database, fastest subdomain finder\n\n**R** - **Recon-ng** \u2192 For modular, database-driven investigations\n- Use when: Deep investigation requiring multiple pivot points\n- Strength: 90+ modules, built-in database, pipeline workflow\n\n**M** - **Maltego** \u2192 For visual relationship mapping (from lesson 8)\n- Use when: Understanding complex infrastructure relationships\n- Strength: Graph visualization, transform marketplace, analyst-friendly\n\n**S** - **SpiderFoot** \u2192 For comprehensive, multi-source scanning\n- Use when: Need quick visibility across 200+ data sources\n- Strength: Web UI, automatic correlation, enterprise features\n\n**Memory hook**: \"ARMS yourself with the right OSINT tool for each mission.\"\n\n---\n\n### API Integration Checklist (KEY)\n\nUse **KEY** to remember essential API integration steps:\n\n**K** - **Keys** (Store securely)\n- Never hardcode in scripts\n- Use environment variables or config files\n- Rotate regularly\n- `.gitignore` config files\n\n**E** - **Errors** (Handle gracefully)\n- Try/except blocks for all API calls\n- Log failures for debugging\n- Implement retry logic with backoff\n- Provide fallback data sources\n\n**Y** - **Yield** (Respect rate limits)\n- Check API documentation for limits\n- Implement `time.sleep()` between calls\n- Use exponential backoff for retries\n- Consider paid tiers for higher limits\n\n**Example implementation**:\n```python\nimport os\nimport time\nimport requests\nfrom typing import Optional\n\nclass APIHelper:\ndef __init__(self):\n# K: Keys from environment\nself.api_key = os.getenv('SHODAN_API_KEY')\nself.rate_limit = 1  # requests per second\nself.last_request = 0\n\ndef call_api(self, url: str) -> Optional[dict]:\n# Y: Yield to rate limits\ntime_since_last = time.time() - self.last_request\nif time_since_last < self.rate_limit:\ntime.sleep(self.rate_limit - time_since_last)\n\n# E: Error handling\ntry:\nresponse = requests.get(url, timeout=30)\nresponse.raise_for_status()\nself.last_request = time.time()\nreturn response.json()\nexcept requests.exceptions.RequestException as e:\nprint(f\"API Error: {e}\")\nreturn None\n```\n\n---\n\n### Recon-ng Workflow (PRIME)\n\nUse **PRIME** to remember Recon-ng workflow:\n\n**P** - **Prepare** workspace\n```bash\nworkspaces create target_investigation\n```\n\n**R** - **Research** and install modules\n```bash\nmarketplace search domains-hosts\nmarketplace install recon/domains-hosts/hackertarget\n```\n\n**I** - **Input** target data\n```bash\ndb insert domains domain=example.com\n```\n\n**M** - **Module** execution\n```bash\nmodules load recon/domains-hosts/hackertarget\nrun\n```\n\n**E** - **Export** results\n```bash\nmodules load reporting/html\nrun\n```\n\n**Memory hook**: \"PRIME your reconnaissance with Recon-ng's systematic approach.\"\n\n---\n\n### SpiderFoot Configuration (SCAN)\n\nUse **SCAN** to configure SpiderFoot effectively:\n\n**S** - **Scope** (Define clearly)\n- Target: Single domain, IP, or person\n- Use case: Footprint (comprehensive) vs Investigate (focused)\n- Boundaries: Subdomains only vs everything\n\n**C** - **Configure** modules\n- Enable relevant modules only (faster scans)\n- Passive vs Active (active may alert targets)\n- API keys for premium sources\n\n**A** - **Analyze** results\n- Graph view for relationships\n- Filter by entity type\n- Sort by risk/severity\n\n**N** - **Notify** and export\n- Export to JSON/CSV for further analysis\n- Integrate with SIEM\n- Schedule recurring scans\n\n---\n\n### Common Pitfalls (FAIL)\n\nUse **FAIL** to avoid common automation mistakes:\n\n**F** - **Forgetting** rate limits\n- Problem: API bans, IP blacklisting\n- Solution: Implement `time.sleep()`, use paid tiers\n\n**A** - **Assuming** data accuracy\n- Problem: False positives, stale data\n- Solution: Cross-reference multiple sources, validate manually\n\n**I** - **Ignoring** legal boundaries\n- Problem: Unauthorized scanning, CFAA violations\n- Solution: Get written authorization, use passive methods\n\n**L** - **Leaking** API keys\n- Problem: Stolen keys, unauthorized usage\n- Solution: Use environment variables, `.gitignore`, key rotation\n\n**Memory hook**: \"Don't FAIL at OSINT automation\u2014avoid these common mistakes.\"\n\n---\n\n### Data Enrichment Pipeline (FLOW)\n\nUse **FLOW** to build data enrichment pipelines:\n\n**F** - **Find** initial data (subdomain, email, IP)\n**L** - **Lookup** in multiple sources (Shodan, VirusTotal, Hunter.io)\n**O** - **Output** to centralized database (SQLite, PostgreSQL, Elasticsearch)\n**W** - **Watch** for changes (continuous monitoring, diff detection)\n\n**Example workflow**:\n```\nInput: example.com\n\u2193\nF: TheHarvester \u2192 50 subdomains\n\u2193\nL: VirusTotal \u2192 Historical DNS (100 more subdomains)\n\u2193\nL: Shodan \u2192 30 IPs with open ports\n\u2193\nO: Save to SQLite database\n\u2193\nW: Cron job (daily) \u2192 Detect new subdomains \u2192 Alert\n```\n\n---\n\n### Quick Reference: Command Cheat Sheet\n\n**Recon-ng Essentials**:\n```bash\nworkspaces create <name>         # Create workspace\nmarketplace search <keyword>     # Find modules\nmarketplace install <module>     # Install module\nmodules load <module>            # Load module\nrun                              # Execute\nshow hosts/contacts/domains      # View results\ndb query SELECT * FROM hosts     # SQL query\n```\n\n**SpiderFoot CLI**:\n```bash\npython3 sf.py -s <target> -t DOMAIN -o json > output.json\npython3 sf.py -l 0.0.0.0:5001    # Start web UI\n```\n\n**TheHarvester**:\n```bash\ntheHarvester -d example.com -b all -l 500 -f report\n# -d: domain\n# -b: data sources (all, google, bing, shodan)\n# -l: limit results\n# -f: output filename\n```\n\n**Python API Calls** (generic structure):\n```python\nimport requests\nimport os\n\nAPI_KEY = os.getenv('API_KEY')\nurl = f'https://api.example.com/v1/search?query=example.com&key={API_KEY}'\nresponse = requests.get(url, timeout=30)\ndata = response.json()\n```\n\n---\n\n### Visual Memory Aid: Automation Stack\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Human Analyst      \u2502\n\u2502  (High-value only)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Visualization Layer                \u2502\n\u2502  Kibana, Grafana, Custom Dashboards        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Orchestration Layer                \u2502\n\u2502  Airflow, Cron, Custom Schedulers          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Collection Layer                   \u2502\n\u2502  Recon-ng, SpiderFoot, TheHarvester        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Data Sources                       \u2502\n\u2502  Shodan, VirusTotal, Hunter.io, Censys     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Memory technique**: Picture intelligence \"flowing upward\" from raw data sources to analyst insights, with each layer adding automation value.\n\n---\n\n### Final Memory Hook: The 3 C's of OSINT Automation\n\n1. **Continuous** (not one-time): Set up monitoring, not just scans\n2. **Comprehensive** (not narrow): Use multiple tools and sources\n3. **Correlated** (not isolated): Connect data points, find relationships\n\n**Remember**: \"3C OSINT: Continuous, Comprehensive, Correlated intelligence wins.\""
      }
    },
    {
      "type": "reflection",
      "content": {
        "text": "## Reflection: Your OSINT Automation Journey\n\n### What You've Accomplished\n\nCongratulations! You've completed the OSINT domain and mastered:\n\n1. **Social media reconnaissance** (Lesson 1) \u2192 Finding people and organizations\n2. **DNS and subdomain enumeration** (Lesson 2) \u2192 Mapping digital infrastructure\n3. **Search engine OSINT** (Lesson 3) \u2192 Google dorking and advanced search\n4. **WHOIS and domain intelligence** (Lesson 4) \u2192 Domain registration forensics\n5. **Shodan and IoT reconnaissance** (Lesson 5) \u2192 Finding exposed devices\n6. **Email and username intelligence** (Lesson 6) \u2192 Identity correlation\n7. **Image and geolocation** (Lesson 7) \u2192 Visual intelligence\n8. **Maltego and relationship mapping** (Lesson 8) \u2192 Graph analysis\n9. **Dark web monitoring** (Lesson 9) \u2192 Underground intelligence\n10. **OSINT automation** (Lesson 10) \u2192 Scaling intelligence operations\n\nYou've progressed from **manual investigator** to **automation architect**.\n\n---\n\n### Reflection Questions\n\nTake 10-15 minutes to reflect on your learning:\n\n#### 1. Automation Mindset\n\n**Question**: Think about a recent OSINT task you performed manually. How would you automate it now?\n\n**Prompts**:\n- What tools would you use? (Recon-ng, SpiderFoot, custom Python?)\n- What APIs would you integrate? (Shodan, VirusTotal, Hunter.io?)\n- How would you handle errors and rate limits?\n- What would trigger alerts? (New subdomains? Exposed services?)\n- How would you visualize the results?\n\n**Action**: Write a 1-page automation plan for this task.\n\n---\n\n#### 2. Real-World Application\n\n**Question**: If you were hired as an OSINT analyst at a Fortune 500 company tomorrow, what would your first 90 days look like?\n\n**Consider**:\n- **Days 1-30**: What would you assess? (Current OSINT maturity, tool inventory, data sources)\n- **Days 31-60**: What would you build? (Automation scripts, integrations, dashboards)\n- **Days 61-90**: What would you scale? (Continuous monitoring, team training, reporting)\n\n**Bonus**: Draft a \"90-Day OSINT Automation Roadmap\" you could present to a CISO.\n\n---\n\n#### 3. Ethical Boundaries\n\n**Question**: You've built an automated OSINT pipeline that monitors competitors' infrastructure 24/7. Where do you draw the line between competitive intelligence and unethical surveillance?\n\n**Discuss**:\n- Is subdomain enumeration ethical? (Generally yes\u2014public DNS data)\n- Is port scanning ethical? (Gray area\u2014may violate CFAA without authorization)\n- Is scraping employees' personal social media ethical? (Context-dependent)\n- Is monitoring dark web mentions of your company ethical? (Yes\u2014defensive intelligence)\n\n**Action**: Write your personal \"OSINT Ethics Code\" (5-10 principles you'll follow).\n\n---\n\n#### 4. Tool Mastery Assessment\n\n**Self-assessment**: Rate your confidence (1-10) in each tool:\n\n| Tool | Confidence (1-10) | Next Step to Improve |\n|------|-------------------|----------------------|\n| Recon-ng | __ | |\n| SpiderFoot | __ | |\n| TheHarvester | __ | |\n| Shodan API | __ | |\n| VirusTotal API | __ | |\n| Hunter.io API | __ | |\n| Custom Python scripts | __ | |\n| Continuous monitoring | __ | |\n\n**Action**: Choose your lowest-confidence tool and spend 2 hours this week building something with it.\n\n---\n\n#### 5. Career Pathway\n\n**Question**: OSINT automation skills open multiple career paths. Which resonates most with you?\n\n**Paths**:\n1. **Threat Intelligence Analyst** (Mandiant, CrowdStrike, Recorded Future)\n- Focus: APT tracking, infrastructure analysis, IOC generation\n- Skills: Maltego, SpiderFoot, custom Python, threat actor TTPs\n\n2. **Red Team Operator** (Professional penetration testing)\n- Focus: Reconnaissance automation, target enumeration, attack surface mapping\n- Skills: Recon-ng, TheHarvester, Amass, custom recon pipelines\n\n3. **Bug Bounty Hunter** (Independent security researcher)\n- Focus: Asset discovery, subdomain enumeration, continuous monitoring\n- Skills: Amass, Subfinder, automation scripts, speed optimization\n\n4. **Security Operations Center (SOC) Analyst** (Corporate security)\n- Focus: External attack surface monitoring, brand protection, executive monitoring\n- Skills: SpiderFoot, dark web monitoring, SIEM integration, dashboards\n\n5. **OSINT Tool Developer** (Building commercial OSINT platforms)\n- Focus: API integrations, scalability, machine learning, SaaS products\n- Skills: Python, APIs, databases, cloud infrastructure, web development\n\n**Action**: Research 3 job postings in your chosen path. What skills do they require that you still need to learn?\n\n---\n\n### Next Steps in Your OSINT Journey\n\n#### Short-term (Next 30 Days)\n\n1. **Build your portfolio**:\n- Create a GitHub repository with your OSINT automation scripts\n- Document your tools with README files\n- Share on Twitter/LinkedIn to build credibility\n\n2. **Practice continuously**:\n- Join bug bounty programs (HackerOne, Bugcrowd) to practice legally\n- Automate reconnaissance for your own domains\n- Participate in OSINT CTF challenges (TraceLabs, OSINT Exercises)\n\n3. **Expand your toolkit**:\n- Learn **Amass** for subdomain enumeration (used by top bug bounty hunters)\n- Master **Elasticsearch + Kibana** for OSINT dashboards\n- Explore **Apache Airflow** for workflow orchestration\n\n#### Medium-term (Next 90 Days)\n\n1. **Advanced integrations**:\n- Build a complete OSINT platform integrating 10+ APIs\n- Create Slack/Discord bots for automated alerts\n- Develop custom Maltego transforms\n\n2. **Machine learning**:\n- Learn basic ML for OSINT (entity extraction, classification)\n- Use **spaCy** for named entity recognition in text\n- Implement **anomaly detection** for continuous monitoring\n\n3. **Certifications** (if career-focused):\n- **GIAC Open Source Intelligence (GOSI)** by SANS\n- **Certified OSINT Professional (COP)** by McAfee Institute\n- **Certified Threat Intelligence Analyst (CTIA)** by EC-Council\n\n#### Long-term (Next 12 Months)\n\n1. **Specialization**:\n- Choose your path: Threat intelligence vs Red team vs Bug bounty vs SOC\n- Deep-dive into domain-specific techniques\n- Build reputation through blog posts, conference talks, tool releases\n\n2. **Community contribution**:\n- Contribute to open-source OSINT tools (Recon-ng, SpiderFoot, Amass)\n- Teach others (blog, YouTube, local meetups)\n- Participate in OSINT conferences (OSINT Summit, SANS CTI Summit)\n\n3. **Professional growth**:\n- Land your first OSINT-focused role\n- Build a team or scale to enterprise-level operations\n- Mentor junior analysts\n\n---\n\n### Community and Resources\n\n**Stay connected with the OSINT community**:\n\n- **Twitter**: Follow @IntelTechniques (Michael Bazzell), @Sector035, @OSINTCurious, @TraceLabs\n- **Discord**: Join OSINT Curious Discord server\n- **Podcasts**: \"The OSINT Curious Project\", \"The Privacy, Security, & OSINT Show\"\n- **Blogs**: Bellingcat, IntelTechniques, Sector035, OSINT Combine\n- **Books**: \n- \"Open Source Intelligence Techniques\" by Michael Bazzell\n- \"OSINT Techniques: Resources for Uncovering Online Information\" by Nihad Hassan\n- \"Operator Handbook: OSINT\" by Joshua Picolet\n\n**Events**:\n- **OSINT Summit** (Annual, Washington DC)\n- **SANS CTI Summit** (Cyber Threat Intelligence)\n- **TraceLabs Global CTF** (Missing persons OSINT competition\u2014humanitarian focus)\n\n---\n\n### Your OSINT Automation Manifesto\n\nAs you conclude this lesson, write your personal OSINT manifesto:\n\n**Template**:\n\n```\nI believe OSINT automation should:\n1. [Your principle, e.g., \"Always respect privacy boundaries\"]\n2. [Your principle, e.g., \"Prioritize open-source tools over proprietary\"]\n3. [Your principle, e.g., \"Provide actionable intelligence, not just data\"]\n\nI will use OSINT automation to:\n1. [Your goal, e.g., \"Improve organizational security posture\"]\n2. [Your goal, e.g., \"Advance my career in threat intelligence\"]\n3. [Your goal, e.g., \"Help find missing persons through TraceLabs\"]\n\nI commit to:\n1. [Your commitment, e.g., \"Never conduct OSINT without authorization\"]\n2. [Your commitment, e.g., \"Share knowledge with the community\"]\n3. [Your commitment, e.g., \"Continuously learn and adapt\"]\n```\n\n---\n\n### Final Thought\n\n**You are now an OSINT automation professional.** The techniques you've learned power:\n- Mandiant's APT investigations\n- Bellingcat's war crime documentation\n- Fortune 500 security operations centers\n- Top bug bounty hunters earning $250K+/year\n- Threat intelligence platforms protecting millions\n\nThe difference between you and these professionals is **practice and scale**. Start building, automate relentlessly, and share your work.\n\n**Welcome to the OSINT community. Your intelligence journey has just begun.**\n\n---\n\n**Final action item**: Share one thing you built in this lesson on Twitter/LinkedIn with hashtag #OSINTAutomation. Tag the instructor or CyberLearn platform to showcase your work!"
      }
    }
  ],
  "post_assessment": [
    {
      "question_id": "a1b2c3d4-e5f6-4a7b-8c9d-0e1f2a3b4c5d",
      "type": "multiple_choice",
      "question": "You're building an automated OSINT pipeline for a red team engagement. The pipeline needs to discover subdomains, resolve IPs, scan services with Shodan, and find emails\u2014all for 50 target organizations. Your Shodan API key has a rate limit of 1 query/second, and you have 2,000 IPs to scan across all targets. Which approach is most efficient while respecting rate limits?",
      "options": [
        "Scan all 2,000 IPs sequentially with time.sleep(1) between each Shodan query, completing all scans before moving to email discovery (~33 minutes for Shodan alone)",
        "Use threading to scan 10 IPs concurrently with Shodan, implementing a semaphore to enforce 1 query/second rate limit globally (~3 minutes for Shodan)",
        "Split the 2,000 IPs across 5 Shodan API keys (400 IPs each), run parallel scans with separate Python processes, aggregate results at the end (~7 minutes total)",
        "Cache Shodan results in a local database from previous scans, only query new/changed IPs (reducing 2,000 queries to ~200 new IPs, ~3 minutes), then proceed with email discovery immediately"
      ],
      "correct_answer": 3,
      "explanation": "**Correct answer: Option 4 (Caching with incremental updates)**\n\n**Why this is best**:\n\n1. **Efficiency**: Reduces 2,000 API calls to ~200 (90% reduction) by caching previous results. Most infrastructure doesn't change daily, so re-scanning the same 1,800 IPs is wasteful.\n\n2. **Rate limit compliance**: Even with caching, you still respect Shodan's 1 query/second limit for the 200 new IPs (~3 minutes vs 33 minutes).\n\n3. **Cost savings**: Shodan charges per query (or has monthly limits). Caching reduces API costs by 90%.\n\n4. **Scalability**: As you scan more organizations over time, cache hit rate increases (more historical data = fewer new queries).\n\n5. **Real-world practice**: Professional OSINT platforms (Mandiant, Recorded Future) all use caching/databases to avoid redundant API calls.\n\n**Why other options are suboptimal**:\n\n- **Option 1 (Sequential with sleep)**: Technically correct but extremely slow (33 minutes just for Shodan). Doesn't leverage concurrency.\n\n- **Option 2 (Threading with semaphore)**: Improves speed (3 minutes) but violates rate limits if not implemented perfectly. Risk of API ban if semaphore logic has bugs. Also, Shodan's rate limit is per API key, not per connection\u2014threading doesn't help much.\n\n- **Option 3 (Multiple API keys)**: Clever but expensive (5 Shodan accounts = $295/month vs $59/month). Also violates Shodan's Terms of Service (account sharing/circumventing rate limits can result in bans).\n\n**Implementation example (Option 4)**:\n```python\nimport sqlite3\nimport time\nimport shodan\n\nclass CachedShodanScanner:\n    def __init__(self, api_key: str, cache_db: str):\n        self.shodan_api = shodan.Shodan(api_key)\n        self.conn = sqlite3.connect(cache_db)\n        self.cursor = self.conn.cursor()\n        self._init_cache()\n    \n    def _init_cache(self):\n        self.cursor.execute('''\n            CREATE TABLE IF NOT EXISTS shodan_cache (\n                ip TEXT PRIMARY KEY,\n                data TEXT,\n                timestamp INTEGER\n            )\n        ''')\n        self.conn.commit()\n    \n    def scan_ip(self, ip: str, cache_ttl: int = 86400) -> dict:\n        # Check cache (86400 = 24 hours)\n        self.cursor.execute(\n            'SELECT data, timestamp FROM shodan_cache WHERE ip = ? AND timestamp > ?',\n            (ip, int(time.time()) - cache_ttl)\n        )\n        cached = self.cursor.fetchone()\n        \n        if cached:\n            print(f\"[CACHE HIT] {ip}\")\n            return eval(cached[0])  # JSON stored as string\n        \n        # Cache miss: Query Shodan\n        print(f\"[CACHE MISS] Querying Shodan for {ip}...\")\n        try:\n            data = self.shodan_api.host(ip)\n            # Store in cache\n            self.cursor.execute(\n                'INSERT OR REPLACE INTO shodan_cache (ip, data, timestamp) VALUES (?, ?, ?)',\n                (ip, str(data), int(time.time()))\n            )\n            self.conn.commit()\n            time.sleep(1)  # Rate limiting\n            return data\n        except shodan.APIError as e:\n            print(f\"Shodan error: {e}\")\n            return {}\n    \n    def scan_ips(self, ips: list) -> list:\n        results = []\n        for ip in ips:\n            results.append(self.scan_ip(ip))\n        return results\n\n# Usage\nscanner = CachedShodanScanner('YOUR_API_KEY', 'shodan_cache.db')\nips = ['8.8.8.8', '1.1.1.1', ...]\nresults = scanner.scan_ips(ips)\n\n# First run: 2,000 queries (~33 min)\n# Second run (same IPs): 0 queries (~instant)\n# Third run (50 new IPs): 50 queries (~50 sec)\n```\n\n**Key lesson**: Professional OSINT automation prioritizes **efficiency** (caching) over **speed** (concurrency). Respecting rate limits through intelligent data management, not just `time.sleep()`, is the mark of experienced OSINT engineers.",
      "difficulty": 3
    },
    {
      "question_id": "b2c3d4e5-f6a7-4b8c-9d0e-1f2a3b4c5d6e",
      "type": "multiple_choice",
      "question": "Your organization has deployed a SpiderFoot-based OSINT monitoring system that scans 1,000 domains daily, alerting the SOC team to high-risk findings. After 6 months, the SOC complains about 'alert fatigue'\u2014they receive 500+ alerts per day, but only 5-10 are actionable. Which solution would MOST effectively reduce false positives while maintaining detection of genuine threats?",
      "options": [
        "Reduce SpiderFoot module count from 200 to 50, disabling low-value modules like 'social media mentions' and 'email format enumeration' to cut alert volume by 75%",
        "Implement a machine learning classifier trained on 6 months of historical alerts (labeled as 'actionable' vs 'noise') to score and filter alerts before sending to SOC analysts",
        "Increase alert thresholds: Only alert on 'critical' severity findings (e.g., exposed databases, leaked credentials) and suppress 'low' and 'medium' findings entirely",
        "Create a 'baseline' for each domain during the first 30 days, then only alert on deviations from baseline (new subdomains, new exposed services, new data leaks)"
      ],
      "correct_answer": 3,
      "explanation": "**Correct answer: Option 4 (Baseline + deviation detection)**\n\n**Why this is most effective**:\n\n1. **Context-aware**: Each organization has different 'normal' states. A SaaS company might legitimately have 500 subdomains and 100 S3 buckets, while a small business has 5 subdomains and 0 cloud storage. Baseline detection adapts to each target's unique profile.\n\n2. **Reduces static noise**: Many alerts are for **existing** conditions that have been present for months (e.g., 'WHOIS privacy enabled' or 'subdomain discovered'). These aren't actionable\u2014they were already known. Baseline filtering eliminates this noise.\n\n3. **Prioritizes change**: Security teams care about **changes** (new attack surface, new vulnerabilities, new data leaks). Deviation detection focuses on exactly this.\n\n4. **Maintains detection coverage**: Unlike Option 1 (disabling modules) or Option 3 (increasing thresholds), baseline detection doesn't sacrifice visibility\u2014it just filters known-good states.\n\n5. **Real-world validation**: This is how enterprise OSINT platforms work (Recorded Future, RiskIQ, Mandiant Advantage). They all use 'first-seen' and 'change detection' to reduce noise.\n\n**Implementation example**:\n```python\nimport json\nimport sqlite3\nfrom datetime import datetime, timedelta\n\nclass BaselineOSINT:\n    def __init__(self, db_path: str):\n        self.conn = sqlite3.connect(db_path)\n        self.cursor = self.conn.cursor()\n        self._init_db()\n    \n    def _init_db(self):\n        self.cursor.execute('''\n            CREATE TABLE IF NOT EXISTS baselines (\n                domain TEXT,\n                entity_type TEXT,\n                entity_value TEXT,\n                first_seen INTEGER,\n                last_seen INTEGER,\n                PRIMARY KEY (domain, entity_type, entity_value)\n            )\n        ''')\n        self.conn.commit()\n    \n    def update_baseline(self, domain: str, scan_results: dict):\n        \"\"\"Update baseline with current scan results\"\"\"\n        timestamp = int(datetime.now().timestamp())\n        \n        for entity_type, entities in scan_results.items():\n            for entity in entities:\n                self.cursor.execute('''\n                    INSERT INTO baselines (domain, entity_type, entity_value, first_seen, last_seen)\n                    VALUES (?, ?, ?, ?, ?)\n                    ON CONFLICT(domain, entity_type, entity_value)\n                    DO UPDATE SET last_seen = ?\n                ''', (domain, entity_type, entity, timestamp, timestamp, timestamp))\n        \n        self.conn.commit()\n    \n    def detect_changes(self, domain: str, scan_results: dict, baseline_days: int = 30) -> dict:\n        \"\"\"Detect new findings vs baseline\"\"\"\n        changes = {\n            'new_entities': {},\n            'removed_entities': {}\n        }\n        \n        baseline_cutoff = int((datetime.now() - timedelta(days=baseline_days)).timestamp())\n        \n        for entity_type, entities in scan_results.items():\n            # Check for new entities\n            for entity in entities:\n                self.cursor.execute('''\n                    SELECT first_seen FROM baselines\n                    WHERE domain = ? AND entity_type = ? AND entity_value = ?\n                ''', (domain, entity_type, entity))\n                \n                result = self.cursor.fetchone()\n                \n                if result is None:\n                    # Never seen before\n                    if entity_type not in changes['new_entities']:\n                        changes['new_entities'][entity_type] = []\n                    changes['new_entities'][entity_type].append(entity)\n                elif result[0] > baseline_cutoff:\n                    # First seen within baseline period (still consider 'new')\n                    if entity_type not in changes['new_entities']:\n                        changes['new_entities'][entity_type] = []\n                    changes['new_entities'][entity_type].append(entity)\n            \n            # Check for removed entities (seen in baseline, not in current scan)\n            self.cursor.execute('''\n                SELECT entity_value FROM baselines\n                WHERE domain = ? AND entity_type = ? AND first_seen < ?\n            ''', (domain, entity_type, baseline_cutoff))\n            \n            baseline_entities = set([row[0] for row in self.cursor.fetchall()])\n            current_entities = set(entities)\n            removed = baseline_entities - current_entities\n            \n            if removed:\n                changes['removed_entities'][entity_type] = list(removed)\n        \n        return changes\n    \n    def alert_on_changes(self, domain: str, changes: dict, severity_rules: dict):\n        \"\"\"Generate alerts based on changes and severity rules\"\"\"\n        alerts = []\n        \n        for entity_type, entities in changes['new_entities'].items():\n            severity = severity_rules.get(entity_type, 'medium')\n            \n            if severity in ['high', 'critical']:\n                alerts.append({\n                    'type': 'new_entity',\n                    'domain': domain,\n                    'entity_type': entity_type,\n                    'entities': entities,\n                    'severity': severity,\n                    'timestamp': datetime.now().isoformat()\n                })\n        \n        return alerts\n\n# Usage\nbaseline = BaselineOSINT('osint_baseline.db')\n\n# First scan (builds baseline)\nscan_results = {\n    'subdomains': ['www.example.com', 'mail.example.com'],\n    'ips': ['192.0.2.1'],\n    'open_ports': ['192.0.2.1:80', '192.0.2.1:443']\n}\nbaseline.update_baseline('example.com', scan_results)\n\n# Second scan (30 days later)\nnew_scan = {\n    'subdomains': ['www.example.com', 'mail.example.com', 'admin.example.com'],  # NEW!\n    'ips': ['192.0.2.1', '192.0.2.2'],  # NEW!\n    'open_ports': ['192.0.2.1:80', '192.0.2.1:443', '192.0.2.2:3389']  # NEW RDP!\n}\n\n# Detect changes\nchanges = baseline.detect_changes('example.com', new_scan)\n\n# Alert only on high-severity changes\nseverity_rules = {\n    'subdomains': 'medium',\n    'ips': 'medium',\n    'open_ports': 'high'  # Open ports are high severity\n}\n\nalerts = baseline.alert_on_changes('example.com', changes, severity_rules)\n\n# Result: Only 1 alert (new RDP port 3389) instead of 3 alerts (subdomain, IP, port)\nprint(f\"Alerts: {len(alerts)}\")\n# Output: Alerts: 1\n```\n\n**Why other options are suboptimal**:\n\n- **Option 1 (Disable modules)**: Reduces visibility. You might miss critical findings (e.g., a leaked credential on Twitter). Better to collect data and filter intelligently.\n\n- **Option 2 (Machine learning)**: Sounds sophisticated, but requires 6 months of labeled training data, ongoing model maintenance, and expertise in ML. Overkill for this problem\u2014baseline detection is simpler and equally effective.\n\n- **Option 3 (Increase thresholds)**: Loses medium-severity findings that might be important (e.g., a new subdomain hosting a phishing page). Also, 'critical' findings are rare\u2014you might get only 1-2 alerts per day, missing genuine threats.\n\n**Key lesson**: Effective OSINT automation isn't about collecting **more** data\u2014it's about filtering **intelligently**. Baseline + deviation detection is the industry-standard approach for reducing false positives while maintaining security coverage.\n\n**Metrics** (from real-world deployments):\n- Baseline detection reduces alerts by **80-95%** (500 alerts \u2192 25-100 alerts)\n- False positive rate: **10-20%** (vs 95% without baseline)\n- Analyst efficiency: **5-10x improvement** (more time for investigation vs alert triage)",
      "difficulty": 3
    },
    {
      "question": "What is the most important takeaway from this lesson?",
      "options": [
        "Understanding the core concepts and their practical applications",
        "Memorizing all technical details",
        "Only knowing the theory without practice",
        "Focusing on a single aspect"
      ],
      "correct_answer": 0,
      "explanation": "The key takeaway is understanding how to apply the concepts learned in real-world scenarios, combining both theoretical knowledge and practical skills.",
      "question_id": "aa929ad5-18cb-43e4-87ef-a8f7b6a7bcea",
      "type": "multiple_choice",
      "difficulty": 1
    }
  ],
  "jim_kwik_principles": [
    "active_learning",
    "meta_learning",
    "memory_hooks",
    "teach_like_im_10",
    "connect_to_what_i_know",
    "minimum_effective_dose",
    "reframe_limiting_beliefs",
    "gamify_it",
    "learning_sprint",
    "multiple_memory_pathways"
  ]
}
